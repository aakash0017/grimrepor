{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"classification_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = df[df['classification'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>issue_691.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>issue_134.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>issue_652.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>issue_120.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>issue_108.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>issue_75.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>issue_677.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>issue_111.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>issue_139.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>issue_844.txt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>625 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename  classification\n",
       "0     issue_691.txt               1\n",
       "5     issue_134.txt               1\n",
       "6     issue_652.txt               1\n",
       "8     issue_120.txt               1\n",
       "9     issue_108.txt               1\n",
       "...             ...             ...\n",
       "1023   issue_75.txt               1\n",
       "1026  issue_677.txt               1\n",
       "1027  issue_111.txt               1\n",
       "1028  issue_139.txt               1\n",
       "1032  issue_844.txt               1\n",
       "\n",
       "[625 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of issue_691.txt:\n",
      "Title: Add gradient_checkpointing_segment_size\n",
      "URL: https://github.com/huggingface/transformers/issues/26103\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_134.txt:\n",
      "Title: fp16 support for grounding dino\n",
      "URL: https://github.com/huggingface/transformers/issues/32672\n",
      "Body:\n",
      "Feature requestCurrently, if fp16 is used with grounding dino viahttps://huggingface.co/docs/transformers/main/en/model_doc/grounding-dino, there is an error of the following:...\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py\", line 3023, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py\", line 2360, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py\", line 1753, in forward\n",
      "    (vision_features, text_features), attentions = encoder_layer(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py\", line 1274, in forward\n",
      "    (text_features, text_enhanced_attn) = self.text_enhancer_layer(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py\", line 828, in forward\n",
      "    attention_output, attention_weights = self.self_attn(\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py\", line 1331, in forward\n",
      "    query_layer = self.transpose_for_scores(self.query(queries))\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 117, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: mat1 and mat2 must have the same dtype, but got Float and HalfMotivationIt would be good to add support for fp16 to speed up the inference time.Your contributionHappy to contribute if this feature is deemed to be useful.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_652.txt:\n",
      "Title: The current architecture does not support Flash Attention 2.0 - distilgpt2, gpt2-medium\n",
      "URL: https://github.com/huggingface/transformers/issues/26994\n",
      "Body:\n",
      "Could you please add the flash attention 2.0 support for these models: distilgpt2, gpt2-medium?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_120.txt:\n",
      "Title: MLPs withFbgemmFp8Linearon Llama-405b-FP8 do not handle batch sizes >1 correctly\n",
      "URL: https://github.com/huggingface/transformers/issues/32868\n",
      "Body:\n",
      "System Infotransformers 4.44.0torch 2.4.0+cu121fbgemm_gpu 0.8.0+cu121Who can help?@ArthurZucker(also maybe@SunMarc?)InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction>>>model_name=\"meta-llama/Meta-Llama-3.1-405B-FP8\">>>base_model=AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.bfloat16,device_map=\"auto\")>>>tokenizer=AutoTokenizer.from_pretrained(model_name)>>>train_strs=[\n",
      "...'The Golden Gate Bridge AI is talking to the user.\\n\\nUser: Who are you?\\n\\nAI: That',\n",
      "...'An AI is talking to the user.\\n\\nUser: Who are you?\\n\\nAI: That',\n",
      "...'The Golden Gate Bridge AI is talking to the user.\\n\\nUser: Who are you?\\n\\nAI: I',\n",
      "...'An AI is talking to the user.\\n\\nUser: Who are you?\\n\\nAI: I',\n",
      "...'The Golden Gate Bridge AI is talking to the user.\\n\\nUser: Who are you?\\n\\nAI: I can'... ]>>>encoded_batch=tokenizer(train_strs[:32],padding=True,return_tensors=\"pt\").to(model.device)>>>out=base_model(**encoded_batch)\n",
      "...Filepython3.10/site-packages/transformers/models/llama/modeling_llama.py:751,inLlamaDecoderLayer.forward(self,hidden_states,attention_mask,position_ids,past_key_value,output_attentions,use_cache,cache_position,position_embeddings,**kwargs)749hidden_states=self.post_attention_layernorm(hidden_states)750hidden_states=self.mlp(hidden_states)-->751hidden_states=residual+hidden_states753outputs=(hidden_states,)755ifoutput_attentions:RuntimeError:Thesizeoftensora(23)mustmatchthesizeoftensorb(736)atnon-singletondimension1>>># Note 736 = 32 * 23, the maximum sequence lengthAfter some digging in pdb, I tracked it down to the quantized MLPs:>>>emd=base_model.model.embed_tokens>>>emd.shapetorch.Size([32,23,16384])>>>type(base_model.model.model.layers[0].mlp.up_proj)torch.nn.modules.linear.Linear>>>base_model.model.model.layers[0].mlp(emd).shapetorch.Size([32,23,16384])>>>type(base_model.model.model.layers[1].mlp.up_proj)transformers.integrations.fbgemm_fp8.FbgemmFp8Linear>>>base_model.model.model.layers[1].mlp(emd).shapetorch.Size([736,16384])# <-------------------------------- wrong!!I was able to patch it with this monkeypatch:>>>classFixedQuantedMLP(torch.nn.Module):\n",
      "...def__init__(self,mlp):\n",
      "...super().__init__()\n",
      "...self.mlp=mlp... \n",
      "...defforward(self,x):\n",
      "...shape=x.shape...x=self.mlp(x)\n",
      "...returnx.reshape(shape)>>>deffix_layer_mlp(layer):\n",
      "...layer.old_mlp=layer.mlp...layer.mlp=FixedQuantedMLP(layer.mlp)>>>forlayerinbase_model.model.layers:fix_layer_mlp(layer)...which mademodel.generatework as expected.Expected behaviorThe quantized MLP layers should not squish batch size and sequence length together. I suspect these lines are at fault, but I'm not sure:transformers/src/transformers/integrations/fbgemm_fp8.pyLines 50 to 52\n",
      "      in52cb403x_quantized,x_scale=torch.ops.fbgemm.quantize_fp8_per_row(x.view(-1,x.shape[-1]),num_tokens,self.input_scale_ub)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_108.txt:\n",
      "Title: Running on Multiple GPU with DeepSpeed. Error: Model was not initialized with Zero-3 despite being configured for Deepspeed Zero-3. Please re-initialize your model via Model.from_pretrained or Model.from_config after creating your TrainingArguments!\n",
      "URL: https://github.com/huggingface/transformers/issues/32901\n",
      "Body:\n",
      "System Infotransformers-cli env OutputAccelerate ConfigWho can help?@muellerzrI get an error when running script using accelerate on 4 GPU.The error:DeepSpeed ConfigTraining ScriptInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe error:DeepSpeed ConfigTraining ScriptExpected behaviorTh error shown below occurs. Although the training arguments are already initialized in script. Am I doing something wrong here?Error:\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_487.txt:\n",
      "Title: Support more memory efficient processing for segmentation models.\n",
      "URL: https://github.com/huggingface/transformers/issues/29546\n",
      "Body:\n",
      "Feature requestThis feature request significantly improves memory consumption for segmentation models, particularly when working with datasets with large numbers of instances per image.MotivationMost (all?) of the models for segmentation tasks intransformersrely on the label/ground truth input being a list of instance masks. For images that contain large numbers of objects (for example aerial imagery, life sciences/microscopy) this can result in huge arrays being passed around. For example a slide image containing 200 cells, each as separate instances, requires a mask input of 200xWxH. At least on my computer, trying to process such datasets means I regularly get OOMs - even with 64GB RAM - unless I take care to limit the number of instances per sample.This issue is also relevant for torchvision's implementation of Mask-RCNN for the same reason, but I think Detectron2 (and possibly mmdet) can operate on polygons/RLE masks directly and I've not had issues training instance segmentation models from inputs with large numbers of objects. (Actually an alternative to this proposal would be to support internally encoding masks as RLE which would also significantly save on memory). My suspicion is that this hasn't been an issue because benchmark datasets like COCO have relatively few instances per image.There are a couple of places that this situation can be improved, with significant boosts to processing speed and memory usage. Perhaps the biggest advantage is the ability to process much larger batch sizes on memory-constrained machines.(1) The first is maybe specific to DETR.DetrForSegmentation's processor computes bounding boxes by using amasks_to_boxesfunction which operates on stack of instance masks. This seems like an intentional decision, but I'm not sure why unless we can't assume that the segments_info boxes are scaled correctly. This function is expensive and is noticeably slow if you have e.g. 100 objects in an image. For object detection models, the processor simply loads the box coordinates fromannotations. In the panoptic regime we'd achieve the same by queryingsegments_info; we can fall back to the mask processing if the bounding box info isn't provided.This a minor fix, but for some samples it gives me an order of magnitude improvement in data-loading speed (which, without this optimisation, can be much longer than the forward/backward pass)# This is taken almost verbatim from the object detection processorif\"bbox\"intarget['segments_info'][0]:boxes=[segment_info[\"bbox\"]forsegment_infointarget[\"segments_info\"]]boxes=np.asarray(boxes,dtype=np.float32).reshape(-1,4)boxes[:,2:]+=boxes[:, :2]boxes[:,0::2]=boxes[:,0::2].clip(min=0,max=image_width)boxes[:,1::2]=boxes[:,1::2].clip(min=0,max=image_height)#keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])new_target[\"boxes\"]=masks_to_boxes(masks)else:new_target[\"boxes\"]=masks_to_boxes(masks)(2) The second is more significant for memory, but a more involved fix. Most of the models use the target masks to compute a mask loss of some kind.MaskFormeruses the same function.Mask2FormerandOneFormeruse a slightly different approach with a sampled point loss.For DETR, bounding box comparisons are used to assign source:target predictions, and then some permutation happens such that we can pair up the relevant source predictions (one for each target), and re-order the target masks so that we can compare. For MaskFormer/Mask2Former/OneFormer, the Hungarian matching algorithm is run on the masks themselves - see a comment later.The main issue here is not processing speed (passing around individual masks makes things simple to reason about), but the significant memory burden of passing around these massive instance arrays which get, somewhat by definition, more sparse the more objects are present. Instead, if we have access to (a) a panoptic mask as processed withrgb_to_idand (b) the segment IDs which are ordered with respect to the input bounding boxes, we can iterate over the ground truth and pick off the mask for each object.Performance wise I think should be net zero because this masking operation is normally done as part of dataloadinganywayto generate the individual instance masks. I'm sure a Numpy wizard could make the actual code more performant but here is a possible implementation that (in my brief testing) gives identical losses to theloss_masksversion.defloss_mask(self,outputs,targets,indices,num_boxes):\"\"\"Compute the losses related to the masks: the focal loss and the dice loss.Targets dicts must contain the key \"mask\" containing a tensor of dim [h, w] where each pixelcorresponds to a segment index. The target dict must also contain \"segment_ids\" which are usedto extract individual objects from the mask itself.\"\"\"if\"pred_masks\"notinoutputs:raiseKeyError(\"No predicted masks found in outputs\")source_idx=self._get_source_permutation_idx(indices)target_idx=self._get_target_permutation_idx(indices)# Permute/filter outputs to one source per targetsource_masks=outputs[\"pred_masks\"]source_masks=source_masks[source_idx]# Resize target masks to uniform shape# TODO use valid to mask invalid areas due to padding in lossmasks=[t[\"mask\"].unsqueeze(0)fortintargets]target_masks,_=nested_tensor_from_tensor_list(masks).decompose()target_masks=target_masks.to(source_masks)# Upsample predictions to the target sizesource_masks=nn.functional.interpolate(source_masks[:,None],size=target_masks.shape[-2:],mode=\"bilinear\",align_corners=False)segment_ids=[t['segment_ids']fortintargets]fromcollectionsimportdefaultdictlosses=defaultdict(int)# Calculate loss per predicted maskforidx,sinenumerate(source_masks):# Derive batch/segment (probably a better way to do this)batch,segment=target_idx[0][idx],target_idx[1][idx]# Extract mask for objectt=(target_masks[batch]==segment_ids[batch][segment]).flatten(1).float()s=s.flatten().unsqueeze(0)losses[\"loss_mask\"]+=sigmoid_focal_loss(s,t,num_boxes)losses[\"loss_dice\"]+=dice_loss(s,t,num_boxes)returnlossesThe main user-facing difference here is that the preprocessor needs to provide the rest of \"segments_info\" in the labels. There may also need to be some logic around transformations, but in principle this should be done prior to processing/encoding? e.g. one loads the image and the annotations, performs any transformation and the dataset returns the augmented sample and takes care not to include e.g. segments that were cropped out.For DETR, this modification is minor but it really improves memory usage by 2-3 orders of magnitude in some cases. For me it enables training with a batch size of 8-16 images instead of 1-2 and I can run with many workers without hitting OOM. It provides the benefit of (almost) constant, predictable memory consumption during dataloading because the input mask is always a fixed size.On Mask/Mask2/OneFormer: the difference with more recent models is that matching is done on a mask-basis and not a box-basis (e.g. MaskFormerHungarianMatcher), but a similar approach could be made where we would replace this with an iteration over segment indices present in the target mask when computing the matching cost matrix.target_mask_flat = target_mask[:, 0].flatten(1)we would pay a penalty in speed, because presumably everything is well-vectorised at the moment (loops bad?). However, I think having the option to pay that price instead over memory may be worth it (again - in order to generate the stack of instance masks, that masking operation has to happen somewhere else anyway).Note that currently the matcher calculates the same costs asloss_masksin order to derive the cost matrix, but these scores are then discarded - it would make more sense to just use the source:target losses directly from the cost matrix, once the matcher has run? i.e.loss_masksshould just return a sum over the winning indices in the cost matrix.Your contributionThere are two primary contributions here:Aim to speed up dataloading by using existing bounding box coordinates, if provided by the labels. This is canonically part of the COCO-panoptic spec. This is certainly a hotfix for DETR segmentation/panoptic, but seems to not be relevant for more recent models.Offer the option for users to provide a panoptic 2D mask instead of a instance stack. This requires a modified loss function which in a few cases isloss_masks. I've implemented this for DETR (which seems to be a simple case), but I think the approach could be extended to Mask/Mask2/OneFormer.For Mask/Mask2/OneFormer we would also have to provide a modified version of the Hungarian matcher that can operate on a panoptic mask as the target.An aside - the loss computation for these models can be simplified by using the Hungarian matching costs directly instead of using loss_masks.I'm happy to PR these but would appreciate some discussion on implementation any other considerations that we'd have to make r.e. the order of dataloading and transformations.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_450.txt:\n",
      "Title: SDPA gives nans/infs during sampling on ROCM w/ float16\n",
      "URL: https://github.com/huggingface/transformers/issues/30056\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.3Platform: Linux-4.18.0 RHEL 8Python version: 3.11.5Huggingface_hub version: 0.22.2Safetensors version: 0.4.2Accelerate version: 0.28.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.4.0.dev20240401+rocm6.0 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: Yes, 4x AMD Mi250x withdevice_map='auto'Using distributed or parallel set-up in script?: noWho can help?@ArthurZuckerand@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionobservationsSDPA gives nans/infs during sampling on ROCM w/ float16 or bfloat16Noticed models: Phi-2, Mistral 7b, Mixtral 8x7b, and likely more!using float32 make the issue go awayGeneration is complete gibberish until a NaN or INF is hitperhaps related[MusicGen] SDPA gives nans/infs during sampling#30020showed me that reverting to 'eagar' attention implementation get's rid of the problemreproductioncreate a virtual environmentmodule load rocm/6.0.2\n",
      "\n",
      "python3.11 -m venv mlvenv --system-site-packagessourcemlvenv/bin/activate\n",
      "\n",
      "python -m pip install --upgrade pip\n",
      "python -m pip install --upgrade setuptools lit ipython\n",
      "python -m pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0\n",
      "python -m pip install --no-cache-dir transformers==4.39.3 accelerate==0.28.0Try to sample from mistral 7b (or mixtral 8x7b)importtorchfromtransformersimportAutoTokenizer,AutoModelForCausalLMmodel_name='mistralai/Mistral-7B-Instruct-v0.2'tokenizer=AutoTokenizer.from_pretrained(model_name)model=AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.bfloat16,device_map='auto',attn_implementation='sdpa',\n",
      ")inputs=tokenizer(\"The world is\",return_tensors=\"pt\",\n",
      ").to(model.device)out=model.generate(**inputs,do_sample=True,max_new_tokens=256,\n",
      ")TracebackRuntimeErrorTraceback(mostrecentcalllast)CellIn[13],line1---->1out=model.generate(2**inputs,3do_sample=True,4max_new_tokens=256,5)File/usr/WS2/jekel1/Repos/llmchat-latest/mlvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115,incontext_decorator.<locals>.decorate_context(*args,**kwargs)112@functools.wraps(func)113defdecorate_context(*args,**kwargs):114withctx_factory():-->115returnfunc(*args,**kwargs)File/usr/WS2/jekel1/Repos/llmchat-latest/mlvenv/lib/python3.11/site-packages/transformers/generation/utils.py:1575,inGenerationMixin.generate(self,inputs,generation_config,logits_processor,stopping_criteria,prefix_allowed_tokens_fn,synced_gpus,assistant_model,streamer,negative_prompt_ids,negative_prompt_attention_mask,**kwargs)1567input_ids,model_kwargs=self._expand_inputs_for_generation(1568input_ids=input_ids,1569expand_size=generation_config.num_return_sequences,1570is_encoder_decoder=self.config.is_encoder_decoder,1571**model_kwargs,1572)1574# 13. run sample->1575result=self._sample(1576input_ids,1577logits_processor=prepared_logits_processor,1578logits_warper=logits_warper,1579stopping_criteria=prepared_stopping_criteria,1580pad_token_id=generation_config.pad_token_id,1581eos_token_id=generation_config.eos_token_id,1582output_scores=generation_config.output_scores,1583output_logits=generation_config.output_logits,1584return_dict_in_generate=generation_config.return_dict_in_generate,1585synced_gpus=synced_gpus,1586streamer=streamer,1587**model_kwargs,1588)1590elifgeneration_mode==GenerationMode.BEAM_SEARCH:1591# 11. prepare beam search scorer1592beam_scorer=BeamSearchScorer(1593batch_size=batch_size,1594num_beams=generation_config.num_beams,\n",
      "   (...)1599max_length=generation_config.max_length,1600)File/usr/WS2/jekel1/Repos/llmchat-latest/mlvenv/lib/python3.11/site-packages/transformers/generation/utils.py:2735,inGenerationMixin._sample(self,input_ids,logits_processor,stopping_criteria,logits_warper,max_length,pad_token_id,eos_token_id,output_attentions,output_hidden_states,output_scores,output_logits,return_dict_in_generate,synced_gpus,streamer,**model_kwargs)2733# sample2734probs=nn.functional.softmax(next_token_scores,dim=-1)->2735next_tokens=torch.multinomial(probs,num_samples=1).squeeze(1)2737# finished sentences should have their next token be a padding token2738ifeos_token_idisnotNone:RuntimeError:probabilitytensorcontainseither`inf`,`nan`orelement<0Note that if we try phi-2, the generation does not throw inf or nan, but rather is just kind of giberishmodel_name='microsoft/phi-2'...out=model.generate(**inputs,do_sample=True,max_new_tokens=256,\n",
      ")tokenizer.batch_decode(out)gives me something like:['The world is, \", and you might feel sad at 3n, the \"not the the best all of its focus and he's probably at which is that, we've been working together and learning more\\n\\n#CI: The, a special day, the the most beautiful town, we are going to talk about some ideas so that the doctors in their next move, a series of events and the tell the world. COVID\\n The LES is an important theme in this time.\\n whole of Australia from the University of of New Mexico, who is a great way to do the work. a more comfortable bed. this scenario.\\n\\n\\n\".., to the main reason for its operations. The Party, on this situation, the first to store data. No, the state\\u202ct. So often the owners? That will get into the y\\n}\\n\\nI will tell you the amount is\\nisopion of total weight of the elements of interest in a better futureocrin discreetly with your\\n\\n1,''J's. What“b')\\n\\n<class and last game-safe. Its Parents are being used for the whole community.\\n accidents are a well-adir)) and Tangle in a1.']Expected behaviorIf we switch to the 'eagar' attention implementation, everything is okay.importtorchfromtransformersimportAutoTokenizer,AutoModelForCausalLMmodel_name='mistralai/Mistral-7B-Instruct-v0.2'tokenizer=AutoTokenizer.from_pretrained(model_name)model=AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.bfloat16,device_map='auto',attn_implementation='eager',\n",
      ")inputs=tokenizer(\"The world is\",return_tensors=\"pt\",\n",
      ").to(model.device)out=model.generate(**inputs,do_sample=True,max_new_tokens=256,\n",
      ")and if we use phi-2 we get the following.model_name='microsoft/phi-2'tokenizer=AutoTokenizer.from_pretrained(model_name)model=AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.bfloat16,device_map='auto',attn_implementation='eager',\n",
      ")out=model.generate(**inputs,do_sample=True,max_new_tokens=256,\n",
      ")tokenizer.batch_decode(out)[\"The world is an incredibly interconnected globe that has always relied on individuals working together towards a shared goal. However, throughout history, we have seen that achieving this goal has not always been easy, and the path has been riddled with obstacles. Today, we are faced with a unique challenge in the form of a global pandemic that has forced us to work together in ways we never have before. Crafting together in the age of global social isolation has become an art, and one that we must master if we are to emerge stronger on the other side.\\n\\nAt the heart of any successful teamwork is communication and empathy. We must learn to listen to each other, to understand each other's perspectives, and to work together towards a common goal. This can be a challenge, especially when stress and tension are high. However, there are ways we can diffuse tense communications and manage our stress levels. We can use humor to lighten the mood, we can acknowledge and validate each other's feelings, and we can practice active listening to ensure that everyone's voice is heard.\\n\\nOne example of this successful teamwork is the development of vaccines for COVID-19. Scientists from all over the world have worked tirelessly to develop vaccines in record time, and their efforts have paid off. The vaccines have been\"]which makes a lot more sense...\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_322.txt:\n",
      "Title: Multi-GPU inference affects LLM's (Llama2-7b-chat-hf) generation.\n",
      "URL: https://github.com/huggingface/transformers/issues/31582\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.2Platform: Linux-5.4.0-186-generic-x86_64-with-glibc2.31Python version: 3.9.19Huggingface_hub version: 0.23.1Safetensors version: 0.4.3Accelerate version: 0.30.1Accelerate config:    not foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: Yes (A100_80GB x 4)Using distributed or parallel set-up in script?: NoWho can help?@ArthurZucker@ganteInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWhen I perform inference with two GPUs using the following command,CUDA_VISIBLE_DEVICES=0,1 nohup python ./inference.pythe model generates answer properly.Whereas, when I use more than two GPUs using the following command,CUDA_VISIBLE_DEVICES=0,1,3,4 nohup python ./inference.pythe model starts generatingGibberish. Upon close introspection, the model outputs logits which are allNaNvalues.Note: I use device_map = \"auto\" while loading the model.Expected behaviorI expect the model to generate properly.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_444.txt:\n",
      "Title: 4bit Adam\n",
      "URL: https://github.com/huggingface/transformers/issues/30172\n",
      "Body:\n",
      "Feature requestIs there any chance we coukd get this 4bit adam optimizer added to tranformers?It has nearly the same performance as 32bit adam with significant drop in vram overhead.repoPaperWith this added qlora would be even more memory efficient, and theoretically, you should be capable of FFT a 7b on a 24gb card.MotivationThe github repo has a paper which shows negligible difference between 32bit and 4bit adam, and they have the code for the adam optimizer here:4bit codeOr one bit adamw from deep speed, I only didn't recommend it since digging through the deep speed code, it isn't as laid out as this one is with a whole dedicated script. While yeah you can always use deepspeed and transformers, but deepspeed comes with its own set of draw backs, such as windows compatibility, and unsloth compatibility. But either or a one bit adamw would be awesome. Aside from that there aren't too many other ways to save memory for qlora. I mean theoretically if someone had the will, they could make a bitnet adamnw, that runs on the cpu. Since bit net doesn't need matmuls, the entire computation could be offloaded to the cpu. It would be actually fast, so the training wont be bogged down.Your contributionSubmit feature request\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_93.txt:\n",
      "Title: resize_token_embeddingsin NLLB leading to empty outputs\n",
      "URL: https://github.com/huggingface/transformers/issues/32948\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.3Platform: Linux-4.18.0-513.11.1.el8_9.x86_64-x86_64-with-glibc2.28Python version: 3.10.14Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu121 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: yesWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
      "\n",
      "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\", additional_special_tokens=[f\"code_{i}\" for i in range(18)], use_fast=True)\n",
      "model.resize_token_embeddings(len(tokenizer))After resizing, generation using an official example:article = \"Şeful ONU spune că nu există o soluţie militară în Siria\"\n",
      "inputs = tokenizer(article, return_tensors=\"pt\")\n",
      "\n",
      "translated_tokens = model.generate(\n",
      "    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"deu_Latn\"), max_length=30\n",
      ")\n",
      "tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]Output is:'t t t t t t t t t t'Expected behaviorGeneration should work without any errors. One interesting thing to note here is if I add just 2 new tokens, it works fine.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_87.txt:\n",
      "Title: Trainer has stuck during the code block of \"Trainer.train\" in Jupyter Notebook\n",
      "URL: https://github.com/huggingface/transformers/issues/33094\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Linux-5.4.0-187-generic-x86_64-with-glibc2.31Python version: 3.10.13Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.2.0+cu118 (True)Tensorflow version (GPU?): 2.17.0 (False)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA A100-SXM4-80GBWho can help?@sanchit-gandhi@muellerzthe following is my TrainerArguments & Trainer：training_args = TrainingArguments(\n",
      "    output_dir=\"semi_structured_wav2vec2_model\",\n",
      "    per_device_train_batch_size=64,\n",
      "    gradient_accumulation_steps=2,\n",
      "    learning_rate=1e-5,\n",
      "    warmup_steps=500,\n",
      "    max_steps=2000,\n",
      "    gradient_checkpointing=True,\n",
      "    fp16=True,\n",
      "    group_by_length=True,\n",
      "    eval_strategy=\"steps\",\n",
      "    per_device_eval_batch_size=64,\n",
      "    save_steps=1000,\n",
      "    eval_steps=1000,\n",
      "    dataloader_num_workers = 32,\n",
      "    dataloader_pin_memory = True,\n",
      "    dataloader_persistent_workers = True,\n",
      "    dataloader_prefetch_factor = 2,\n",
      "    logging_steps=25,\n",
      "    load_best_model_at_end=True,\n",
      "    metric_for_best_model=\"wer\",\n",
      "    greater_is_better=False,\n",
      "    disable_tqdm=False,\n",
      "    push_to_hub=False,\n",
      ")\n",
      "\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=encoded_train,\n",
      "    eval_dataset=encoded_valid,\n",
      "    tokenizer=processor,\n",
      "    data_collator=data_collator,\n",
      "    compute_metrics=compute_metrics,\n",
      ")I would like to give it a try of finetuning wav2vec2.0 model (\"facebook/wav2vec2-base\") with the dataset of LibriSpeech (I know there's the version which has been finetuned already, but I would like to know how to manipulate trainer.train()). Considering the enormous data of the librispeech dataset, I set the larger num_worker (32, there's 128 logical cpus in A100 server)to pre-load the data, but I have stuck at the trainer.train for more than 20 minutes, and there's no any logs showing up.GPU isn't work at all, however, when I use the command of htop, I can see  that CPU is working for transfering data I thought(btw, there's on;y one user, me, in the server)what's wrong for trainer.train() or do I need to set the special arguments for distributed data parallel training?InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionthe links for colabhttps://colab.research.google.com/drive/1n4JiRejsSdaPGPRPI2pFQCBXeaVLm4Ok?usp=sharingExpected behaviortrainer.train() works wellthere's progress bar for trainer.train()\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_478.txt:\n",
      "Title: Grok-1 MoE support\n",
      "URL: https://github.com/huggingface/transformers/issues/29704\n",
      "Body:\n",
      "Model descriptionX-AI recently releasedgrok-1, a massive MoE model, with a total parameter count of 314B across 8 experts, 2 active at a time. Would be interesting if we could have support for the weights in transformers, would make integrations everywhere else much easier to handle.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationThey have a reference implementation in JAX, but similar to meta's original release of llama, the inference code is highly unoptimized, and has a hard-coded number of GPUs as a requirement.https://github.com/xai-org/grok-1\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_281.txt:\n",
      "Title: idefics2 fails when first element of batch contains no images\n",
      "URL: https://github.com/huggingface/transformers/issues/31854\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-6.2.0-36-generic-x86_64-with-glibc2.35Python version: 3.10.11Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config:    - compute_environment: LOCAL_MACHINE- distributed_type: NO- mixed_precision: fp16- use_cpu: False- debug: False- num_processes: 1- machine_rank: 0- num_machines: 1- gpu_ids: all- rdzv_backend: static- same_network: True- main_training_function: main- enable_cpu_affinity: False- downcast_bf16: no- tpu_use_cluster: False- tpu_use_sudo: False- tpu_env: []- dynamo_config: {'dynamo_backend': 'INDUCTOR', 'dynamo_mode': 'default', 'dynamo_use_dynamic': False, 'dynamo_use_fullgraph': False}PyTorch version (GPU?): 2.3.1.post300 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: no/not yetWho can help?Path:transformers/models/idefics2/image_processing_idefics2.pymake_list_of_images accessesimages[0][0], test cases:make_list_of_images([[], []])\n",
      "make_list_of_images([[], [Image.new(\"RGB\", (100, 100))]])example patch:def patched_image_processing_idefics2(images: ImageInput) -> List[List[np.ndarray]]:\n",
      "    \"\"\"\n",
      "    Convert a single image or a list of images to a list of numpy arrays.\n",
      "\n",
      "    Args:\n",
      "        images (`ImageInput`):\n",
      "            A single image or a list of images.\n",
      "\n",
      "    Returns:\n",
      "        A list of numpy arrays.\n",
      "    \"\"\"\n",
      "    # fixes bug when first element in image list does not contain an image, by searching for the first non-empty idx\n",
      "    \n",
      "    # If it's a single image, convert it to a list of lists\n",
      "    if is_valid_image(images):\n",
      "        images = [[images]]\n",
      "    # If it's a list of images, it's a single batch, so convert it to a list of lists\n",
      "    else:\n",
      "        first_non_empty_idx = next((i for i, imgs in enumerate(images) if len(imgs) > 0), None)\n",
      "        if first_non_empty_idx is not None:\n",
      "            if isinstance(images, (list, tuple)) and len(images) > 0 and is_valid_image(images[first_non_empty_idx]):\n",
      "                images = [images]\n",
      "            # If it's a list of batches, it's already in the right format\n",
      "            elif (\n",
      "                isinstance(images, (list, tuple))\n",
      "                and len(images) > 0\n",
      "                and isinstance(images[first_non_empty_idx], (list, tuple))\n",
      "                and is_valid_image(images[first_non_empty_idx][0])\n",
      "            ):\n",
      "                pass\n",
      "            else:\n",
      "                raise ValueError(\n",
      "                    \"Invalid input type. Must be a single image, a list of images, or a list of batches of images.\"\n",
      "                )\n",
      "    return imagesSimilarly foris_scaled_image(images_list[0][0])intransformers/models/idefics2/image_processing_idefics2.pyThere may be more issues.For a temporary workaround, what other models text+vision -> text models do you recommend?InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers.models.idefics2.image_processing_idefics2 import make_list_of_images\n",
      "from PIL import Image\n",
      "\n",
      "make_list_of_images([[], []])\n",
      "make_list_of_images([[], [Image.new(\"RGB\", (100, 100))]])\n",
      "\n",
      "\n",
      "# similarly for the processor\n",
      "# processor(text=..., images=[[], []])Expected behaviorshould not lead to exception\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_524.txt:\n",
      "Title: err_handle(layoutlmv3): Error message doesn't give much clarity when boxes not containing enough information\n",
      "URL: https://github.com/huggingface/transformers/issues/29127\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.2Platform: Windows-10-10.0.22000-SP0Python version: 3.11.5Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.2.0+cpu (False)Tensorflow version (GPU?): 2.15.0 (False)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: noUsing distributed or parallel set-up in script?: noWho can help?@younesbelkada@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionModel I am using LayoutLMv3:whenboxes = [[123, 53], [36, 87], ...](basically any list which is not according to the proper format)by proper format I mean[[123, 346, 234, 634], [356, 568, 234, 25], ...]encoding=processor(image_1,text,boxes=boxes,max_length=512,padding=\"max_length\",truncation=True,return_tensors=\"pt\")It produces a this error messageValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (labels in this case) have excessive nesting (inputs type list where type int is expected).To ReproduceSteps to reproduce the behavior:add any list of boxes with not enough values likeboxes = [[123, 53], [36, 87], ...]when run it throws the ValueError mentioned aboveExpected behaviorCan throw an error sayingValueError: boxes doesn't have enough values inside each box. Each box should contain 4 values\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_256.txt:\n",
      "Title: The implementations ofLlamaAttentionandLlamaSdpaAttentionare not equivalent.\n",
      "URL: https://github.com/huggingface/transformers/issues/32086\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.0.dev0Platform: Linux-6.5.0-44-generic-x86_64-with-glibc2.39Python version: 3.10.14Huggingface_hub version: 0.23.2Safetensors version: 0.4.3Accelerate version: 0.25.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA RTX A4500Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionchange theoutput_attentions=output_attentionsto beoutput_attentions=Trueathttps://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L617run the official example script:from PIL import Image\n",
      "import requests\n",
      "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
      "\n",
      "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
      "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
      "\n",
      "prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n",
      "url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
      "image = Image.open(requests.get(url, stream=True).raw)\n",
      "\n",
      "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
      "\n",
      "generate_ids = model.generate(**inputs, max_new_tokens=15)\n",
      "\n",
      "print(\n",
      "    processor.batch_decode(\n",
      "        generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
      "    )[0]\n",
      ")The generation will be cut down like this:USER:  \\nWhat's the content of the image? ASSISTANT: theThis is due to the fact that the implementations ofLlamaAttentionandLlamaSdpaAttentionare not equivalent.It can be fixed by align the implementations ofLlamaAttention.forwardwith the execution logic oftorch.nn.functional.scaled_dot_product_attentionlike this:causal_mask = attention_mask\n",
      "        is_causal = True if causal_mask is None and q_len > 1 else False\n",
      "        if attention_mask is not None:  # no matter the length, we just slice it\n",
      "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
      "            attn_weights = attn_weights + causal_mask\n",
      "        elif is_causal:\n",
      "            assert causal_mask is None\n",
      "            attn_bias = torch.zeros(query_states.shape[-2], key_states.shape[-2], dtype=query_states.dtype)\n",
      "            temp_mask = torch.ones(query_states.shape[-2], key_states.shape[-2], dtype=torch.bool).tril(diagonal=0)\n",
      "            attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
      "            attn_bias.to(query_states.dtype)\n",
      "            attn_weights += attn_biaschange the implementation fromtransformers/src/transformers/models/llama/modeling_llama.pyLine 330\n",
      "      ine316c52ifattention_maskisnotNone:# no matter the length, we just slice itto the code above works for me(transformersversion: 4.43.0.dev0).Expected behaviorThe output should be like this:What's the content of the image? ASSISTANT: The image features a street scene with a stop sign, a red building,\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_929.txt:\n",
      "Title: Weird behavior with mBART-50 and Spanish\n",
      "URL: https://github.com/huggingface/transformers/issues/12958\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_901.txt:\n",
      "Title: GeneratorExp aren't supported by torch.jit.script when I try to export a previously trained model  'google/vit-base-patch16-224-in21k'.\n",
      "URL: https://github.com/huggingface/transformers/issues/15354\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_915.txt:\n",
      "Title: LayoutLMv2Processor does not accept the XLMRobertaTokenizerFast\n",
      "URL: https://github.com/huggingface/transformers/issues/13972\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_6.txt:\n",
      "Title: Request to Add Option to Disable mmap in transformers | Loading models is taking too much time through due to mmap on storage over network case.\n",
      "URL: https://github.com/huggingface/transformers/issues/33366\n",
      "Body:\n",
      "System InfoUbuntu 22.04Troch 2.4.0Cuda 12.4Transformers 4.44.2Python 3.11Diffusers 0.30.2Who can help?I will say mainly@ArthurZuckerbut its more general issue as its involving the base class of transformers pretrained model.Here's an issue explanation :I am currently using the transformers library to loadCLIPTextModelin a Kubernetes environment where I mount an S3 bucket via the S3 CSI driver as a persistent volume to access models. While accessing large files (around 30 GB), I am experiencing severe performance issues, and after investigating, I believe the root cause is related to the forced usage of mmap when loading model weights.It seems that the current implementation inthis sectionof the code forces the use of mmap without providing an option to disable it. This behavior is highly problematic in storage-over-network use cases, as each mmap call introduces significant latency and performance bottlenecks due to the overhead of network access.I think the feature was introduced here =>#28331It would be extremely useful if there were a flag or option to disable mmap usage when loading models, allowing users to load the files directly into memory instead. This would enable users like me, to avoid the network-bound performance issues.I've already tried to find a workaround playing with env variable to disable mmap, but the issue is that i loss so much performance.InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionIt's quite hard to reproduce as u need to have AWS Account and CSI Driver. But I belive this issue can be reproduced on any storage over network case.Anyway here the doc for the driver i used, if needed it can be deployed quite fast on a k8s cluster with the dochttps://github.com/awslabs/mountpoint-s3-csi-driver?tab=readme-ov-fileHere u can find a deployement manifesthttps://github.com/awslabs/mountpoint-s3-csi-driver/blob/main/examples/kubernetes/static_provisioning/static_provisioning.yamlTo reproduce you just have to put the models on the S3 bucket, and try to load them throughCLIPTextModel.from_pretrained.Expected behaviorLoading should be fast.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_726.txt:\n",
      "Title: Add Vocos model\n",
      "URL: https://github.com/huggingface/transformers/issues/25123\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_733.txt:\n",
      "Title: Initialize Flax model params on CPU\n",
      "URL: https://github.com/huggingface/transformers/issues/24711\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_727.txt:\n",
      "Title: Support symbolic tracing for NeoX models\n",
      "URL: https://github.com/huggingface/transformers/issues/24937\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_914.txt:\n",
      "Title: [performance/precision] addingjit.scriptto activation functions\n",
      "URL: https://github.com/huggingface/transformers/issues/13997\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_7.txt:\n",
      "Title: Pipelines download the latest revision by default instead of the default/output revision\n",
      "URL: https://github.com/huggingface/transformers/issues/33365\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: macOS-14.6.1-arm64-arm-64bitPython version: 3.12.5Huggingface_hub version: 0.24.5Safetensors version: 0.4.3Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.3.1 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noWho can help?@NarsilInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfromtransformersimportpipelineclassifier=pipeline(\"sentiment-analysis\")classifier(\"We are very happy to show you the 🤗 Transformers library.\")outputsNo model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).However, it downloads themainrevision instead ofaf0f99b.From what I can tell, this happens with all of the pipelines.Edit: I believe it was broken indc9147fshortly after being added (e4d2588)Expected behaviorThe downloaded revision is consistent with the output.Addingrevisiontohub_kwargsseems to fix it, but it'd probably be good to update the defaults to the latest version of each model (since that's the current behavior).--- a/src/transformers/pipelines/__init__.py+++ b/src/transformers/pipelines/__init__.py@@ -860,6 +860,7 @@def pipeline(\n",
      "             f\" {revision} ({HUGGINGFACE_CO_RESOLVE_ENDPOINT}/{model}).\\n\"\n",
      "             \"Using a pipeline without specifying a model name and revision in production is not recommended.\"\n",
      "         )+hub_kwargs['revision'] = revisionif config is None and isinstance(model, str):\n",
      "             config = AutoConfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs, **model_kwargs)\n",
      "             hub_kwargs[\"_commit_hash\"] = config._commit_hash\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_928.txt:\n",
      "Title: Problem about using mBART50 for Russian to Chinese translation\n",
      "URL: https://github.com/huggingface/transformers/issues/13116\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_519.txt:\n",
      "Title: Device Mismatch Error when Exporting Hugging Face's BERTModel to TorchScript\n",
      "URL: https://github.com/huggingface/transformers/issues/29205\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.1Platform: Linux-4.18.0-477.27.1.el8_8.x86_64-x86_64-with-glibc2.31Python version: 3.10.13Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU): 2.1.2 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@ArthurZuckerand@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionexport in cpufrom transformers import BertModel, BertTokenizer\n",
      "import torch\n",
      "\n",
      "model = BertModel.from_pretrained('bert-base-multilingual-uncased', torchscript=True)\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
      "\n",
      "input_test = torch.ones([1, 20]).long()\n",
      "\n",
      "script_model = torch.jit.trace(model, [input_test])\n",
      "torch.jit.save(script_model, 'test.pt')load and inference in gpuimport torch\n",
      "\n",
      "input_test = torch.ones([1, 20]).long()\n",
      "script_model = torch.jit.load('test.pt', map_location='cuda')\n",
      "script_model(input_test.cuda())errorclass BertSelfAttention(Module):\n",
      "  File \"code/__torch__/transformers/models/bert/modeling_bert.py\", line 175, in forward\n",
      "    attention_scores = torch.matmul(query_layer, torch.transpose(key_layer, -1, -2))\n",
      "    attention_scores0 = torch.div(attention_scores, CONSTANTS.c2)\n",
      "    input = torch.add(attention_scores0, attention_mask)\n",
      "            ~~~~~~~~~ <--- HERE\n",
      "    input2 = torch.softmax(input, -1)\n",
      "    context_layer = torch.matmul((dropout).forward(input2, ), value_layer)\n",
      "....\n",
      "....\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!The device variable creating tensors remains fixed during tracing, necessitating dynamic adjustment for flexibility.transformers/src/transformers/models/bert/modeling_bert.pyLines 968 to 974\n",
      "      in2a9b1f8device=input_ids.deviceifinput_idsisnotNoneelseinputs_embeds.device# past_key_values_lengthpast_key_values_length=past_key_values[0][0].shape[2]ifpast_key_valuesisnotNoneelse0ifattention_maskisNone:attention_mask=torch.ones(((batch_size,seq_length+past_key_values_length)),device=device)Expected behavior�Ensuring traced models operate on different devices(cpu -> gpu, gpu -> cpu)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_257.txt:\n",
      "Title: Tokenizer regression in 4.43-dev affecting Aya/Command-r 35B models\n",
      "URL: https://github.com/huggingface/transformers/issues/32081\n",
      "Body:\n",
      "System InfoUbuntu 22.04Who can help?@ArthurZuckerReproduction4.42.4 has no such issue. Regression/crash only happens on transformer tip/main.Traceback (most recent call last):\n",
      "  File \"/root/projects/go/python/ai/train/sft_trainer.py\", line 673, in <module>\n",
      "    trainer = SFTTrainer(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py\", line 373, in __init__\n",
      "    train_dataset = self._prepare_dataset(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py\", line 519, in _prepare_dataset\n",
      "    return self._prepare_non_packed_dataloader(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py\", line 587, in _prepare_non_packed_dataloader\n",
      "    tokenized_dataset = dataset.map(\n",
      "                        ^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 602, in wrapper\n",
      "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 567, in wrapper\n",
      "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3161, in map\n",
      "    for rank, done, content in Dataset._map_single(**dataset_kwargs):\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3552, in _map_single\n",
      "    batch = apply_function_on_filtered_inputs(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3421, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py\", line 557, in tokenize\n",
      "    outputs = tokenizer(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 2945, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 3032, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 3228, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/transformers/models/cohere/tokenization_cohere_fast.py\", line 174, in _batch_encode_plus\n",
      "    return super()._batch_encode_plus(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 561, in _batch_encode_plus\n",
      "    for key in tokens_and_encodings[0][0].keys():\n",
      "               ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "IndexError: list index out of rangeExpected behaviorNot crash.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_531.txt:\n",
      "Title: load_state_dict doesnt support torch._subclasses.fake_tensor.FakeTensorMode\n",
      "URL: https://github.com/huggingface/transformers/issues/29006\n",
      "Body:\n",
      "System Infotransformersversion: 4.36.0Platform: Linux-6.5.0-15-generic-x86_64-with-glibc2.31Python version: 3.11.5Huggingface_hub version: 0.19.4Safetensors version: 0.4.0Accelerate version: 0.24.1Accelerate config:    not foundPyTorch version (GPU?): 2.3.0a0+git78a84f1 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?: noWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWhen PyTorch'sFakeTensorModeis active, the underlying storage is changed to beUntypedStorageas a way to not really allocate the memory for the parameters.As a consequence, transformers'sget_tensorfailed withValueError: could not determine the shape of object type 'torch.storage.UntypedStorage'fromtorch._subclassesimportfake_tensorimporttransformersfake_mode=fake_tensor.FakeTensorMode(allow_non_fake_inputs=False)withfake_mode:fake_model=transformers.AutoModel.from_pretrained(\"sshleifer/tiny-gpt2\")Error:Loading checkpoint shards:   0%||0/19 [00:00<?,?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File\"/opt/pytorch/test_mixtral.py\", line 9,in<module>model = AutoModelForCausalLM.from_pretrained(model_id)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File\"/opt/conda/envs/ptca/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 566,infrom_pretrainedreturnmodel_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File\"/opt/conda/envs/ptca/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3694,infrom_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File\"/opt/conda/envs/ptca/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4079,in_load_pretrained_model\n",
      "    state_dict = load_state_dict(shard_file)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File\"/opt/conda/envs/ptca/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 510,inload_state_dictreturnsafe_load_file(checkpoint_file)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File\"/opt/conda/envs/ptca/lib/python3.11/site-packages/safetensors/torch.py\", line 310,inload_file\n",
      "    result[k] = f.get_tensor(k)\n",
      "                ^^^^^^^^^^^^^^^\n",
      "ValueError: could not determine the shape of objecttype'torch.storage.UntypedStorage'Expected behaviortransformersget_tensorshould be able to load fake tensors from a fakefied checkpoint\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_525.txt:\n",
      "Title: Request to add FLMR\n",
      "URL: https://github.com/huggingface/transformers/issues/29103\n",
      "Body:\n",
      "Model descriptionBasic InformationThis issue requests adding Fine-grained Late-interaction Multi-modal Retriever (FLMR).The model leverages late interaction (as originally proposed by StanfordColBERT) to compute token-level similarity between every query token and document token, which enables more accurate retrieval relative to DPR-like systems.The model was proposed inhere(NeurIPS 2023) andhere(a follow-up version that was pre-trained on more than ten million of multi-modal retrieval data).ResourcesProject pagehereOfficial codebasehereThe pre-trained checkpoints arehere.Why adding this modelThis work has gained attention from researchers across the world. We received many requests during NeurIPS 2023 to provide an easy implementation of this model.The work has been recognized by the authors of ColBERTtwitter postThere exists no implementation for late-interaction retrieval models in hf-transformers, which have been extensively researched in these years.There are many requests in the original codebasean example issueAs original authors, we have finished 99% of the model, including an example for indexing and searching.   Limited work is required to have it on huggingface-transformers![WIP] Add FLMR model#29062Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationThe PR is already here:#29062\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_86.txt:\n",
      "Title: llama3 position_ids error with left padding\n",
      "URL: https://github.com/huggingface/transformers/issues/33095\n",
      "Body:\n",
      "Feature requestThe LLaMA 3 implementation should generate defaultposition_idsthat take theattention_maskinto account.@ArthurZucker@younesbelkadaMotivationIs there a specific reason why the defaultposition_idsgeneration doesn’t consider theattention_mask? A friend mentioned that this issue has persisted for almost half a year now.transformers/src/transformers/models/llama/modeling_llama.pyLine 962\n",
      "      inadb9117ifposition_idsisNone:The problem arises when using left padding, as the default position_ids start from the first index in the sequence length  rather than from the first non-zero index in theattention_mask.As far as I know, this is handled correctly in thegeneratefunction, but I would expect consistency during training as well.https://discuss.huggingface.co/t/llama-position-ids/75870Your contributionI can submit a PR changingtransformers/src/transformers/models/llama/modeling_llama.pyLine 963\n",
      "      inadb9117position_ids=cache_position.unsqueeze(0)toposition_ids = (attn_mask.cumsum(-1) - 1).clamp(min=0)\n",
      "position_ids.masked_fill_(attn_mask.to(torch.bool) == 0, 0)if noone has any objections on correctness of that. Otherwise, let's discuss.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_479.txt:\n",
      "Title: mamba generation throughput lower than original due to DecodingCGCache\n",
      "URL: https://github.com/huggingface/transformers/issues/29699\n",
      "Body:\n",
      "System InfoPython 3.10.13, CUDA 12.1GPU = NVIDIA GeForce RTX 2080 Ti. Max memory = 10.747 GB.torch==2.2.1torchaudio==2.1.0torchvision==0.16.0tokenizers==0.15.2transformers ==git+https://github.com/huggingface/transformers@dd1c9052159ae824c8acef7c2552f9fad5ca020atriton==2.2.0causal_conv1d==git+https://github.com/Dao-AILab/causal-conv1d.git@96456720c00393a5c32872d8352d7a7ec31fb3db#egg=causal_conv1dmamba_ssm==git+https://github.com/state-spaces/mamba.git@9127d1f47f367f5c9cc49c73ad73557089d02cb8#egg=mamba_ssmWho can help?text models:@ArthurZuckerand@younesbelkadagenerate:@ganteInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe key model initialization and generation parts are given as below.Original code repoIn the originalcode repofrom mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
      "model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-130m\")\n",
      "model.eval()\n",
      "\n",
      "model.generate(\n",
      "        input_ids=input_ids,\n",
      "        max_length=max_length,\n",
      "        **cg=True**\n",
      "    )Then throughput for generating 1K length isNumber of parameters: 129135360\n",
      "Prompt length: 100, generation length: 1000\n",
      "Prompt processing + decoding time: 1011 msUsing the HF libraryfrom transformers import MambaForCausalLM\n",
      "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
      "model.eval()\n",
      "\n",
      "model.generate(\n",
      "        input_ids=input_ids,\n",
      "        max_length=max_length\n",
      "    )Then throughput for generating 1K length isNumber of parameters: 129135360\n",
      "Prompt length: 100, generation length: 1000\n",
      "state-spaces/mamba-130m-hf prompt processing + decoding time: 15970msExpected behaviorThe \"cg=True\" isconfirmed to be the part has a significant impact on the generation performancefor mamba.I have tried:Passing the \"use_cache=True\" as follows won't affect the resultsmodel = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", use_cache=True)\n",
      "or\n",
      "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\", cache_params={use_cache: True})\n",
      "or\n",
      "model.config.use_cache=TrueModifying the mamba model to force the argument \"use_cache=True\" in theMambaModel, but still not working.I assume this is related to the#29605, but modifying the argument directly seems not solving the problem.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_92.txt:\n",
      "Title: Whisper generate return a slice of result if result have more than one added token\n",
      "URL: https://github.com/huggingface/transformers/issues/33082\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: Linux-5.15.0-116-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.31.0Accelerate config:    - compute_environment: LOCAL_MACHINE- distributed_type: MULTI_GPU- mixed_precision: bf16- use_cpu: False- debug: False- num_processes: 4- machine_rank: 0- num_machines: 1- gpu_ids: all- rdzv_backend: static- same_network: True- main_training_function: main- enable_cpu_affinity: True- downcast_bf16: no- tpu_use_cluster: False- tpu_use_sudo: False- tpu_env: []PyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noUsing GPU in script?: yesGPU type: NVIDIA L40SWho can help?@sanchit-gandhi/@kamilakesbiInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI added some new tokens to the tokenizer, resized the word embedding, and then finetuned with the custom dataset.fromtransformersimport(WhisperForConditionalGeneration,WhisperProcessor,\n",
      ")processor=WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\",language=\"en\",task=\"transcribe\", \n",
      ")model=WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\",\n",
      ")processor.tokenizer.add_tokens([])#fill list with tokens you want to addmodel.resize_token_embeddings(len(processor.tokenizer))# then finetuneWhen I use the generate function on a fine-tuned model, I find that if the model's output contains more than one newly added token, the output is truncated.fromtransformersimport(WhisperForConditionalGeneration,WhisperProcessor,\n",
      ")processor=WhisperProcessor.from_pretrained(\"\",#model save pathlanguage=\"en\",task=\"transcribe\", \n",
      ")model=WhisperForConditionalGeneration.from_pretrained(\"\",#model save path)eval_dataloader=#use your dataloaderforbatchintqdm(eval_dataloader):generated_tokens=(model.generate(input_features=batch[\"input_features\"].cuda(),max_new_tokens=255,\n",
      "        )\n",
      "        .cpu()\n",
      "        .numpy()\n",
      "    )print(processor.tokenizer.batch_decode(generated_tokens,skip_special_tokens=True))I've found that this line of code should make it consider the new tokens to be timestamp_tokens, which causes it to truncate the result.transformers/src/transformers/models/whisper/generation_whisper.pyLine 1759\n",
      "      ind806fa3timestamp_tokens:torch.Tensor=seek_sequence.ge(timestamp_begin)Expected behaviorThe generate function should output the full transcription.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_323.txt:\n",
      "Title: OOM when loading 300B models withAutoModelForCausalLM.from_pretrainedandBitsAndBytesConfigquantization.\n",
      "URL: https://github.com/huggingface/transformers/issues/31577\n",
      "Body:\n",
      "System InfoMy goal is to followDistributed fine-tuning blogpost with FSDPto test with distributed fine-tuning on larger size of model like 300BGrok-1.Context is that I have triedg5.48xlarge(8 GPUs with 192 GB and 768 GB CPU) andp4d.24xlarge(8 GPUs. with 320 GB and 1152 GB CPU). There are two issues listed as following.Transformer version is:transformers==4.40.0Issue 1When I tried to load the model with 4 bits quantization with code below (WITHOUT FSDP and it is purely on a EC2 of g5.48xlarge), the total GPU memory required should be around 150GB (since model is ~300B Grok-1), which is smaller than 192GB GPU memory of g5.48xlarge, but I hit OOM. If I turn onlow_cpu_mem_usage=True, then the model can be successfully loaded on CPU in the EC2 of g5.48xlarge. Same error happens atp4d.24xlargewhere 4 bit quantization is failed at loading.from transformers import (\n",
      "    AutoModelForCausalLM,\n",
      "    AutoTokenizer,\n",
      "    BitsAndBytesConfig,\n",
      "    set_seed,\n",
      ")\n",
      "import torch\n",
      "import os\n",
      "   \n",
      "torch_dtype = torch.bfloat16\n",
      "quant_storage_dtype = torch.bfloat16\n",
      "\n",
      "quantization_config = BitsAndBytesConfig(\n",
      "    load_in_4bit=True,\n",
      "    bnb_4bit_use_double_quant=True,\n",
      "    bnb_4bit_quant_type=\"nf4\",\n",
      "    bnb_4bit_compute_dtype=torch_dtype,\n",
      "    bnb_4bit_quant_storage=quant_storage_dtype,\n",
      ")\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "    \"keyfan/grok-1-hf\",\n",
      "    quantization_config=quantization_config,\n",
      "    torch_dtype=quant_storage_dtype,\n",
      "    use_cache=(\n",
      "        False\n",
      "    ), \n",
      "    trust_remote_code=True,\n",
      ")Issue 2Continue on point 1, i think I find a path forward to load the model into CPU by settinglow_cpu_mem_usage=True. Follow theblogpost above, I start try SageMaker training job and I try to load this model using thedefault qlora_fsdp script, shown in the blog. Further,I disabled the quantization (as the quantization will load the model into GPUs but it failed in the point 1). Since when FSDP is enabled, it will by default uselow_cpu_mem_usage=Trueaccording to thisline. However, I hit timeout issue even after I modified training argumentddp_timeoutto be 10800.The model checkpoints are loaded twice and failed at second time of loading.return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards:   5%|▌         | 1/19 [00:00<00:03,  5.36it/s]\n",
      "Loading checkpoint shards:  11%|█         | 2/19 [00:00<00:03,  5.28it/s]\n",
      "Loading checkpoint shards:  16%|█▌        | 3/19 [00:00<00:03,  5.24it/s]\n",
      "Loading checkpoint shards:  21%|██        | 4/19 [00:00<00:02,  5.23it/s]\n",
      "Loading checkpoint shards:  26%|██▋       | 5/19 [00:00<00:02,  5.29it/s]\n",
      "Loading checkpoint shards:  32%|███▏      | 6/19 [00:01<00:02,  5.27it/s]\n",
      "Loading checkpoint shards:  37%|███▋      | 7/19 [00:01<00:02,  5.25it/s]\n",
      "Loading checkpoint shards:  42%|████▏     | 8/19 [00:01<00:02,  5.25it/s]\n",
      "Loading checkpoint shards:  47%|████▋     | 9/19 [00:01<00:01,  5.23it/s]\n",
      "Loading checkpoint shards:  53%|█████▎    | 10/19 [00:01<00:01,  5.21it/s]\n",
      "Loading checkpoint shards:  58%|█████▊    | 11/19 [00:02<00:01,  5.20it/s]\n",
      "Loading checkpoint shards:  63%|██████▎   | 12/19 [00:02<00:01,  5.20it/s]\n",
      "Loading checkpoint shards:  68%|██████▊   | 13/19 [00:02<00:01,  5.19it/s]\n",
      "Loading checkpoint shards:  74%|███████▎  | 14/19 [00:02<00:00,  5.21it/s]\n",
      "Loading checkpoint shards:  79%|███████▉  | 15/19 [00:02<00:00,  5.20it/s]\n",
      "Loading checkpoint shards:  84%|████████▍ | 16/19 [00:03<00:00,  5.19it/s]\n",
      "Loading checkpoint shards:  89%|████████▉ | 17/19 [00:03<00:00,  5.19it/s]\n",
      "Loading checkpoint shards:  95%|█████████▍| 18/19 [00:03<00:00,  5.24it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:03<00:00,  5.27it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:03<00:00,  5.23it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 1 examples [00:00,  8.21 examples/s]\n",
      "Generating train split: 827 examples [00:00, 1985.28 examples/s]\n",
      "Generating train split: 1641 examples [00:00, 3495.17 examples/s]\n",
      "Generating train split: 2496 examples [00:00, 3041.11 examples/s]\n",
      "Generating train split: 3324 examples [00:01, 3366.71 examples/s]\n",
      "Generating train split: 4001 examples [00:01, 3996.93 examples/s]\n",
      "Generating train split: 4797 examples [00:01, 4292.10 examples/s]\n",
      "Generating train split: 5698 examples [00:01, 4238.81 examples/s]\n",
      "Generating train split: 6060 examples [00:01, 3625.63 examples/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 324 examples [00:00, 4303.12 examples/s]\n",
      "Loading checkpoint shards:   5%|▌         | 1/19 [01:51<33:35, 111.97s/it]\n",
      "Loading checkpoint shards:   5%|▌         | 1/19 [01:56<34:51, 116.18s/it]\n",
      "Loading checkpoint shards:   5%|▌         | 1/19 [01:55<34:45, 115.86s/it]\n",
      "Loading checkpoint shards:   5%|▌         | 1/19 [01:55<34:46, 115.93s/it]\n",
      "Loading checkpoint shards:   5%|▌         | 1/19 [01:55<34:46, 115.89s/it]\n",
      "Loading checkpoint shards:   5%|▌         | 1/19 [01:55<34:47, 115.98s/it]\n",
      "Loading checkpoint shards:   5%|▌         | 1/19 [01:57<35:21, 117.86s/it]\n",
      "Loading checkpoint shards:  11%|█         | 2/19 [04:45<42:02, 148.38s/it]\n",
      "Loading checkpoint shards:  11%|█         | 2/19 [04:49<42:21, 149.50s/it]\n",
      "Loading checkpoint shards:  11%|█         | 2/19 [04:48<42:19, 149.37s/it]\n",
      "Loading checkpoint shards:  11%|█         | 2/19 [04:48<42:20, 149.42s/it]\n",
      "Loading checkpoint shards:  11%|█         | 2/19 [04:48<42:19, 149.39s/it]\n",
      "Loading checkpoint shards:  11%|█         | 2/19 [04:50<42:32, 150.16s/it]\n",
      "Loading checkpoint shards:  11%|█         | 2/19 [04:51<42:45, 150.92s/it]\n",
      "Loading checkpoint shards:  16%|█▌        | 3/19 [07:27<40:58, 153.63s/it]\n",
      "Loading checkpoint shards:  16%|█▌        | 3/19 [07:27<41:10, 154.41s/it]\n",
      "Loading checkpoint shards:  16%|█▌        | 3/19 [07:28<41:01, 153.85s/it]\n",
      "Loading checkpoint shards:  16%|█▌        | 3/19 [07:27<41:00, 153.78s/it]\n",
      "Loading checkpoint shards:  16%|█▌        | 3/19 [07:27<41:00, 153.78s/it]\n",
      "Loading checkpoint shards:  16%|█▌        | 3/19 [07:29<41:13, 154.57s/it]\n",
      "Loading checkpoint shards:  16%|█▌        | 3/19 [07:31<41:20, 155.04s/it]\n",
      "Loading checkpoint shards:  21%|██        | 4/19 [10:21<40:24, 161.67s/it]\n",
      "Loading checkpoint shards:  21%|██        | 4/19 [10:21<40:24, 161.63s/it]\n",
      "Loading checkpoint shards:  21%|██        | 4/19 [10:22<40:28, 161.92s/it]\n",
      "Loading checkpoint shards:  21%|██        | 4/19 [10:22<40:36, 162.40s/it]\n",
      "Loading checkpoint shards:  21%|██        | 4/19 [10:21<40:28, 161.87s/it]\n",
      "Loading checkpoint shards:  21%|██        | 4/19 [10:23<40:28, 161.92s/it]\n",
      "Loading checkpoint shards:  21%|██        | 4/19 [10:26<40:43, 162.87s/it]\n",
      "Loading checkpoint shards:  26%|██▋       | 5/19 [13:01<37:33, 160.99s/it]\n",
      "Loading checkpoint shards:  26%|██▋       | 5/19 [13:01<37:27, 160.56s/it]\n",
      "Loading checkpoint shards:  26%|██▋       | 5/19 [13:02<37:37, 161.28s/it]\n",
      "Loading checkpoint shards:  26%|██▋       | 5/19 [13:02<37:37, 161.22s/it]\n",
      "Loading checkpoint shards:  26%|██▋       | 5/19 [13:02<37:37, 161.26s/it]\n",
      "Loading checkpoint shards:  26%|██▋       | 5/19 [13:02<37:45, 161.83s/it]\n",
      "Loading checkpoint shards:  26%|██▋       | 5/19 [13:06<37:46, 161.88s/it]\n",
      "Loading checkpoint shards:  32%|███▏      | 6/19 [15:56<35:54, 165.76s/it]\n",
      "Loading checkpoint shards:  32%|███▏      | 6/19 [15:56<35:53, 165.62s/it]\n",
      "Loading checkpoint shards:  32%|███▏      | 6/19 [15:56<35:53, 165.67s/it]\n",
      "Loading checkpoint shards:  32%|███▏      | 6/19 [15:57<36:00, 166.18s/it]\n",
      "Loading checkpoint shards:  32%|███▏      | 6/19 [15:57<35:56, 165.90s/it]\n",
      "Loading checkpoint shards:  32%|███▏      | 6/19 [15:57<36:00, 166.17s/it]\n",
      "Loading checkpoint shards:  32%|███▏      | 6/19 [16:01<36:01, 166.23s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 7/19 [18:36<32:47, 164.00s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 7/19 [18:38<32:54, 164.56s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 7/19 [18:38<32:55, 164.67s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 7/19 [18:38<32:53, 164.44s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 7/19 [18:39<32:55, 164.63s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 7/19 [18:40<33:00, 165.05s/it]\n",
      "Loading checkpoint shards:  37%|███▋      | 7/19 [18:45<33:05, 165.47s/it]\n",
      "Loading checkpoint shards:  42%|████▏     | 8/19 [21:36<30:58, 168.96s/it]\n",
      "Loading checkpoint shards:  42%|████▏     | 8/19 [21:36<30:55, 168.64s/it]\n",
      "Loading checkpoint shards:  42%|████▏     | 8/19 [21:36<30:56, 168.80s/it]\n",
      "Loading checkpoint shards:  42%|████▏     | 8/19 [21:36<30:56, 168.78s/it]\n",
      "Loading checkpoint shards:  42%|████▏     | 8/19 [21:36<30:57, 168.91s/it]\n",
      "Loading checkpoint shards:  42%|████▏     | 8/19 [21:38<31:01, 169.27s/it]\n",
      "Loading checkpoint shards:  42%|████▏     | 8/19 [21:44<31:09, 169.91s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 9/19 [24:19<27:49, 166.91s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 9/19 [24:21<27:57, 167.72s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 9/19 [24:21<27:57, 167.71s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 9/19 [24:21<27:56, 167.63s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 9/19 [24:21<27:56, 167.69s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 9/19 [24:22<27:56, 167.63s/it]\n",
      "Loading checkpoint shards:  47%|████▋     | 9/19 [24:27<27:55, 167.55s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 10/19 [27:17<25:30, 170.07s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 10/19 [27:17<25:30, 170.01s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 10/19 [27:17<25:31, 170.16s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 10/19 [27:17<25:31, 170.15s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 10/19 [27:21<25:38, 170.90s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 10/19 [27:21<25:41, 171.30s/it]\n",
      "Loading checkpoint shards:  53%|█████▎    | 10/19 [27:24<25:35, 170.64s/it]\n",
      "Loading checkpoint shards:  58%|█████▊    | 11/19 [29:54<22:10, 166.27s/it]\n",
      "Loading checkpoint shards:  58%|█████▊    | 11/19 [29:54<22:10, 166.29s/it]\n",
      "Loading checkpoint shards:  58%|█████▊    | 11/19 [29:54<22:10, 166.26s/it]\n",
      "Loading checkpoint shards:  58%|█████▊    | 11/19 [29:57<22:13, 166.71s/it]\n",
      "Loading checkpoint shards:  58%|█████▊    | 11/19 [29:57<22:16, 167.09s/it]\n",
      "Loading checkpoint shards:  58%|█████▊    | 11/19 [30:00<22:19, 167.44s/it]\n",
      "Loading checkpoint shards:  58%|█████▊    | 11/19 [30:03<22:16, 167.03s/it]\n",
      "[E ProcessGroupNCCL.cpp:474] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800772 milliseconds before timing out.\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "[E ProcessGroupNCCL.cpp:488] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n",
      "[E ProcessGroupNCCL.cpp:494] To avoid data inconsistency, we are taking the entire process down.\n",
      "[E ProcessGroupNCCL.cpp:915] [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800772 milliseconds before timing out.\n",
      "terminate called after throwing an instance of 'std::runtime_error'\n",
      "  what():  [Rank 0] NCCL watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=2, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=1800000) ran for 1800772 milliseconds before timing out.Who can help?@philschmid@SunMarc@lewtun@sgugger@ArthurZucker@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSame as aboveExpected behaviorShould be no OOM\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_445.txt:\n",
      "Title: LayoutLM.from_pretrained doesn't load embeddings' weights when using safetensors\n",
      "URL: https://github.com/huggingface/transformers/issues/30125\n",
      "Body:\n",
      "System Infotransformersversion: 4.38.1Platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31Python version: 3.10.13Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRunning the following:fromtransformersimportLayoutLMModelmodel=LayoutLMModel.from_pretrained(\"microsoft/layoutlm-base-uncased\",use_safetensors=True)results in:Some weights of LayoutLMModel were not initialized from the model checkpoint at microsoft/layoutlm-base-uncased and are newly initialized: ['layoutlm.embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.Note that this is also the default behavior if a user hassafetensorsinstalled and doesn't provideuse_safetensors.The following works as expected (without safetensors):fromtransformersimportLayoutLMModelmodel=LayoutLMModel.from_pretrained(\"microsoft/layoutlm-base-uncased\",use_safetensors=False)Expected behaviorEmbeddings' weights should be correctly loaded.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_451.txt:\n",
      "Title: test_encode_decode_fast_slow_all_tokensis failing\n",
      "URL: https://github.com/huggingface/transformers/issues/30045\n",
      "Body:\n",
      "This test starts to fail after#29473See#29473 (review)#30044skips it for now, but let's fix it once we have the bandwidth.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_337.txt:\n",
      "Title: AddStatefulDataLoadersupport\n",
      "URL: https://github.com/huggingface/transformers/issues/31441\n",
      "Body:\n",
      "Feature requestAdd official support forStatefulDataLoaderas intorchdataanddatasets.MotivationThe StatefulDataLoader from the torchdata package provides a convenient way to recover a dataset iterator that was interrupted, without having to skip the first batches via a naive for loop, which can be time-consuming for extremely large datasets. Thedatasetspackage now officially supports statefulIterableDatasetand its combination withStatefulDataLoaderinv2.20.0.Example usage:fromtorchdata.stateful_dataloaderimportStatefulDataLoaderiterable_dataset=load_dataset(\"deepmind/code_contests\",streaming=True,split=\"train\")dataloader=StatefulDataLoader(iterable_dataset,batch_size=32,num_workers=4)# checkpointstate_dict=dataloader.state_dict()# uses iterable_dataset.state_dict() under the hood# resume from checkpointdataloader.load_state_dict(state_dict)# uses iterable_dataset.load_state_dict() under the hoodTo enhance the usability and efficiency of theTrainer, it would be highly beneficial for the community if official support forStatefulDataLoadercould be added.This would allow users to easily recover from interruptions and resume training from checkpoints without wasting time on re-iterating over already processed batches.By integratingStatefulDataLoaderinto theTrainer, users can seamlessly handle large datasets and ensure a smooth training process. This feature would greatly improve the overall user experience and make the Trainer more robust and efficient.We kindly request the development team to consider adding official support for thoese features in theTrainer, as it would be a valuable addition to the library and benefit the wider community.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_45.txt:\n",
      "Title: 'CohereModel' object has no attribute '_prune_heads'\n",
      "URL: https://github.com/huggingface/transformers/issues/33235\n",
      "Body:\n",
      "System InfoclassCoherePreTrainedModeldocstring says that \"This model inherits from [PreTrainedModel]. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,pruning headsetc.)\". However, it doesn't haveprune_heads()methods, even though it explicitly mentions that it has in the docstring.@ArthurZuckerWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoTokenizer, AutoModel\n",
      "\n",
      "model_id = \"CohereForAI/aya-23-8B\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "model = AutoModel.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
      "model.prune_heads({1: [1]})Expected behaviorI expect the method to prune heads of the cohere model.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_79.txt:\n",
      "Title: Covert chemaleon weights to hf, ImportError\n",
      "URL: https://github.com/huggingface/transformers/issues/33116\n",
      "Body:\n",
      "System InfoImportError: cannot import name'ChameleonForCausalLM'from'transformers'(/mnt/petrelfs/xxxx/miniconda3/envs/lumina_mgpt/lib/python3.10/site-packages/transformers/__init__.py)InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionpython src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py --input_dir /mnt/petrelfs/q-pth --model_size 7B --output_dir /mnt/petrelfsExpected behaviorHope this is fixedChecklistI have read the migration guide in the readme. (pytorch-transformers;pytorch-pretrained-bert)I checked if a related official extension example runs on my machine.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_486.txt:\n",
      "Title: Adherence to semantic versioning with APIs\n",
      "URL: https://github.com/huggingface/transformers/issues/29547\n",
      "Body:\n",
      "Feature requestTransformers is an incredibly popular library used by many folks. It would be great if Transformers would adopt semantic versioning (https://semver.org/) and follow best practices to provide API deprecations and backwards compatibility.For example, the following PR#29143breaks API changes. This is acknowledged in the PR review. In the future, it would be great to instead deprecate the existing interface and provide a new interface, and then remove the old, deprecated interface in future releases.MotivationWithout following semantic versioning, users have immense amounts of pain upgrading versions and code is often broken. It also leads to hacky support such asmosaicml/llm-foundry#1018which now has to case on the API of multiple versions of Transformer.Your contributionN/A\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_492.txt:\n",
      "Title: [Tokenizer] Inconsistent behavior when decoding a single ID and a list of the single ID\n",
      "URL: https://github.com/huggingface/transformers/issues/29489\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.0.dev0Platform: Linux-5.4.0-163-generic-x86_64-with-glibc2.10Python version: 3.8.18Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config:    not foundPyTorch version (GPU?): 2.1.2+cu121 (True)Tensorflow version (GPU?): 2.13.1 (True)Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)Jax version: 0.4.13JaxLib version: 0.4.13Using GPU in script?: no needUsing distributed or parallel set-up in script?:no needWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionCodefromtransformersimportAutoTokenizertokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\",use_fast=False)int_single_id=tokenizer.vocab_size-1list_single_id=[tokenizer.vocab_size-1]print(f'<<<<{tokenizer.decode(int_single_id)}>>>>')print(f'<<<<{tokenizer.decode(list_single_id)}>>>>')tokenizer=AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\",use_fast=False)int_single_id=tokenizer.vocab_size-1list_single_id=[tokenizer.vocab_size-1]print(f'<<<<{tokenizer.decode(int_single_id)}>>>>')print(f'<<<<{tokenizer.decode(list_single_id)}>>>>')# Roughly estimated, around 15 models would have this issue.Output<<<<# # ～>>>>\n",
      "<<<<##～>>>>\n",
      "<<<<# # ～>>>>\n",
      "<<<<##～>>>>Expected behaviorConsistent behaviors. For example, when decoding the single ID, the output could also be##~.Suspected rationale: In thesrc/transformers/tokenization_utils.py, the_decodefunction incorrectly usesspaces_between_special_tokens, and then adds spaces between the sub-tokens.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_109.txt:\n",
      "Title: [whisper] transcription is different from hf & openai\n",
      "URL: https://github.com/huggingface/transformers/issues/32900\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.2Platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35Python version: 3.10.14Huggingface_hub version: 0.21.4Safetensors version: 0.4.2Accelerate version: 0.24.1Accelerate config:    not foundPyTorch version (GPU?): 2.1.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionsilence-middle.wav.zipthe following wav file produceswith hf whisper repo (tiny model):Split infinity, and a time when less is more. Where too much is neverwith openai whisper repo (tiny model)Split infinity, and a time when less is more.Expected behaviorI would expect the result to beSplit infinity, and a time when less is more. Where too much is never\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_647.txt:\n",
      "Title: Difference in LlamaAttention & LlamaFlashAttention2 attn_output\n",
      "URL: https://github.com/huggingface/transformers/issues/27050\n",
      "Body:\n",
      "System Infotransformersversion: 4.34.1Platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.31Python version: 3.11.5Huggingface_hub version: 0.17.3Safetensors version: 0.4.0Accelerate version: 0.23.0Accelerate config:    not foundPyTorch version (GPU?): 2.1.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: noWho can help?@ArthurZuckerand@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWe noticeLlamaFlashAttention2._flash_attention_forwardreturns a differentattn_outputthanLlamaAttentioncomputes.flash_attn_non_determinism.py:importargparseimporttorchimporttorch.backends.cudnnimporttransformersfromtransformers.modelsimportllamadefmain()->None:torch.backends.cudnn.deterministic=Trueparser=argparse.ArgumentParser()parser.add_argument(\"--use-flash-attention-2\",action=\"store_true\")args=parser.parse_args()use_flash_attention_2=args.use_flash_attention_2tokenizer=transformers.AutoTokenizer.from_pretrained(\"/models/huggingface/meta-llama/llama-2-7b-chat-hf\",local_files_only=True,use_safetensors=True,device_map=torch.device(\"cuda\")\n",
      "    )tokenizer.pad_token=tokenizer.eos_tokentokenizer.padding_side=\"left\"text=\"Hello world!\"tokenized_text=tokenizer(text)tokenized_text={key:torch.tensor(value).unsqueeze(dim=0).to(torch.device(\"cuda\"))forkey,valueintokenized_text.items()}tokenized_text[\"labels\"]=tokenized_text[\"input_ids\"].clone()torch.manual_seed(0)model=llama.LlamaForCausalLM.from_pretrained(\"/models/huggingface/meta-llama/llama-2-7b-chat-hf\",local_files_only=True,use_safetensors=True,device_map=torch.device(\"cuda\"),use_flash_attention_2=use_flash_attention_2,torch_dtype=torch.bfloat16,\n",
      "    )assertisinstance(model,llama.LlamaForCausalLM)model.eval()forparaminmodel.parameters():param.requires_grad=Falsemodel.model.layers[0].train()forparaminmodel.model.layers[0].parameters():param.requires_grad=Trueoptim=torch.optim.AdamW(model.parameters())torch.manual_seed(0)foriinrange(10):output=model(**tokenized_text)loss=output[\"loss\"]ifiin(0,9):print(loss)loss.backward()optim.step()optim.zero_grad()if__name__==\"__main__\":main()$python flash_attn_non_determinism.py --use-flash-attention-2tensor(5.6612, device='cuda:0', grad_fn=<NllLossBackward0>)tensor(0.3542, device='cuda:0', grad_fn=<NllLossBackward0>)$python flash_attn_non_determinism.pytensor(5.6589, device='cuda:0', grad_fn=<NllLossBackward0>)tensor(0.2275, device='cuda:0', grad_fn=<NllLossBackward0>)Expected behaviorI am not expecting the magnitude of the difference between the 2 implementations. A difference of0.1267compared to0.3542seems very large.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_135.txt:\n",
      "Title: Inference API takes forever and output: \"Model ... is currently loading\" [couldn't reopen #23793]\n",
      "URL: https://github.com/huggingface/transformers/issues/32665\n",
      "Body:\n",
      "System InfoIt wasn't possible to reopen issue#23793..so I've created a new one. I'd like to know if there's a fix for it or any explanation, workaround or estimative for model availability.Unfortunately, it seems we are still getting the same error response that many others reported for more than one model.Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionExecute a curl to a modelcurl https://api-inference.huggingface.co/models/meta-llama/llama-2-13b-chat \\\n",
      "        -X POST \\\n",
      "        -d '{\"inputs\": \"Deploying my first endpoint was not an amazing experience.\"}' \\\n",
      "        -H \"Authorization: Bearer <insert-hf-token-here>\"Get this error response{\"error\":\"Model meta-llama/Llama-2-13b-chat is currently loading\",\"estimated_time\":20.0}Expected behaviorStatus 200 and llamas response...currently even in the HF website, I've just gotten a time out\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_860.txt:\n",
      "Title: Add DFFT\n",
      "URL: https://github.com/huggingface/transformers/issues/18004\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_684.txt:\n",
      "Title: align_to_words=TrueinQuestionAnsweringPipelinecan lead to duplicate answers\n",
      "URL: https://github.com/huggingface/transformers/issues/26286\n",
      "Body:\n",
      "System Infotransformersversion: 4.31.0Platform: macOS-13.4.1-arm64-arm-64bitPython version: 3.11.4Huggingface_hub version: 0.15.1Safetensors version: 0.3.1Accelerate version: 0.21.0Accelerate config:    not foundPyTorch version (GPU?): 2.0.1 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: NoUsing distributed or parallel set-up in script?: NoWho can help?@narsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfromtransformersimportpipelineanswers=pipeline(\"question-answering\",model=\"deepset/tinyroberta-squad2\")(question=\"Who is the chancellor of Germany?\",context=\"Angela Merkel was the chancellor of Germany.\",top_k=10)print(answers[0])# Returns {'score': 0.9961308836936951, 'start': 0, 'end': 13, 'answer': 'Angela Merkel'}print(answers[5])# Returns {'score': 7.520078361267224e-05, 'start': 0, 'end': 13, 'answer': 'Angela Merkel'}Ifalign_to_wordsis set toTrue(which is the default), all start or end tokens that are contained in the same word are mapped to the same start and end character index (seehere). This is expected when usingalign_to_words. However, the top_k filtering happens before this step so duplicate answers can remain.Expected behaviorIdeally, the mapping from token to word should happen at aroundthis point. You would have a start and end probability for each word. If there are multiple tokens in a word, their probabilities should be summed. This would make the probabilities more correct because every token in the word would affect the probability of selecting the word.If this is too slow, there should at least be a check for duplicates somewherehere. This would mean that you are not guaranteed to get k answers when settingtop_k, but only that you get at most k answers. A way to mitigate that somewhat (but not perfectly), would be to use a higher value than top_k when callingselect_starts_endshere.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_848.txt:\n",
      "Title: Add Mask2Former\n",
      "URL: https://github.com/huggingface/transformers/issues/18503\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_862.txt:\n",
      "Title: Add Flax implementation for BLOOM\n",
      "URL: https://github.com/huggingface/transformers/issues/17703\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_876.txt:\n",
      "Title: pointer to transformer (big) model\n",
      "URL: https://github.com/huggingface/transformers/issues/16747\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_123.txt:\n",
      "Title: Integrate Liger (Linkedin GPU Efficient Runtime) Kernel to HuggingFace\n",
      "URL: https://github.com/huggingface/transformers/issues/32861\n",
      "Body:\n",
      "Feature requestIntegrate Liger (Linkedin GPU Efficient Runtime) Kernel to HuggingFace Trainer, user could decide whether to enable kernel with a simple flagMotivationLiger (Linkedin GPU Efficient Runtime) Kernel is a collection of Triton kernels designed specifically for LLM training. We have implemented Hugging Face Compatible RMSNorm, RoPE, SwiGLU, CrossEntropy, FusedLinearCrossEntropy, and more to come. It can effectively increase multi-GPU training throughput by 20% and reduces memory usage by 60%. The kernel works out of the box withflash attention, PyTorch FSDP, and Microsoft DeepSpeed. We welcome contributions from the community to gather the best kernels for LLM training.Your contributionWe (LinkedIn) will take care of work for a smooth integration and would need HF review and feedback for changes.BenchmarkBenchmark conditions: LLaMA 3-8B, Alpaca Dataset, Max seq len = 512, Data Type = bf16, Optimizer = AdamW, Gradient Checkpointing = True, Distributed Strategy = FSDP1 on 4 A100s.The throughput increases by approximately 20% with more data, but the GPU memory is reduced by 40%. This means you can train the model on smaller GPUs, with larger batch sizes, or with longer sequence lengths at no additional cost.For more detailed benchmark setup and more exciting efficiency for multi-head training (Medusa), please refer to original repo:https://github.com/linkedin/Liger-Kernel(Repo will be public soon)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_889.txt:\n",
      "Title: Add OFA to transformers\n",
      "URL: https://github.com/huggingface/transformers/issues/15813\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_137.txt:\n",
      "Title: offload to CPU will increase memory\n",
      "URL: https://github.com/huggingface/transformers/issues/32654\n",
      "Body:\n",
      "System InfolatestWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionimport torchfrom transformers import AutoModelimport psutilprocess = psutil.Process()CPM_PATH  = \"minicpm_v2\"minicpm_model = AutoModel.from_pretrained(CPM_PATH,trust_remote_code=True,torch_dtype=torch.bfloat16)minicpm_model.to(\"cuda\")memory_info = process.memory_info()memory_usage_MB = memory_info.rss / (1024 ** 2)print(f\"Memory usage: {memory_usage_MB}\")for i in range(10):minicpm_model.to(\"cpu\")minicpm_model.to(\"cuda\")memory_info = process.memory_info()memory_usage_MB = memory_info.rss / (1024 ** 2)print(f\"Memory usage: {memory_usage_MB}\")Expected behaviorMemory usage: 720.6875Memory usage: 6311.3515625The memory should be same since the model is both on GPU at the beginning and the end.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_679.txt:\n",
      "Title: Community contribution: Adding Flash Attention 2 support for more architectures\n",
      "URL: https://github.com/huggingface/transformers/issues/26350\n",
      "Body:\n",
      "Feature requestFlash Attention 2 is a library that provides attention operation kernels for faster and more memory efficient inference and training:https://github.com/Dao-AILab/flash-attentionLet's try to add Flash Attention 2 support for more architectures! Currently supported architectures areLlamaFalconIt would be great to add the support for more architectures such asBarkBartBERT |@sorenmcCLIPWIP - Add Flash Attention CLIP#27444DistilBERTGPT-2GPT-JGPTBigCode (Starcoder) |@susnatoGPT-neoGPT-neo-x |@younesbelkada[Flash Attention 2] Add flash attention 2 for GPT-Neo-X#26463OPT |@susnato[FA2] Add flash attention for opt#26414LlavaVipLlavamBARTMistralMixtralMPT |@rajveer43T5Persimmon |@jeromekuPhiWhisperQwen2... and many moreAdding this feature would require to follow the same protocol as in#25598. First create a new module inside the corresponding modeling file termed asxxxFlashAttentionthat inherits fromxxxAttentionand override the foward method to use the public methods fromflash-attn. Make sure to have access to a GPU that supports Flash Attention 2.Given the slight challenge of the issue, labelling it as a good second issue!If you are interested to take up the challenge, comment below with the architecture name you want to integrate and open a PR!Once you open a PR, feel free to ping@LysandreJik@ArthurZucker@amyeroberts@younesbelkada@fxmarty@SunMarc@pacman100for a reviewMotivationMaking LLMs more memory efficient and faster !Your contributionReviewing PRs and possibly adding the support for more models\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_484.txt:\n",
      "Title: Add predict_proba method for Autoformer/Informer\n",
      "URL: https://github.com/huggingface/transformers/issues/29556\n",
      "Body:\n",
      "Feature requestCurrently, the autoformer and informer does only provide sampled outputs from a probability distribution for future values. However, it would be nice if there would be the possibility to provide the forecasted distribution to the user.MotivationI am currently trying to develop an adapter in sktime that enables the integration of the time series models from the transformers library (sktime/sktime#5790). Since sktime has apredict_probamethod, I would like to translate the transformers probability distribution into sktime probability distributions.Your contributionI think there are at least three solutions:Add a new method, e.g.,predict_distribution, which is returning the distribution object or its parameters.Add a parameter to thegeneratefunction that controls if the distribution or its parameters are returned.Always returns the distributions or it's parameters together with the sampledsequenceingenerate.If you are preferring any of these three solutions, I am happy to implement it :)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_490.txt:\n",
      "Title: TrainingArguments shouldn't set ACCELERATE_MIXED_PRECISION env variable\n",
      "URL: https://github.com/huggingface/transformers/issues/29510\n",
      "Body:\n",
      "Feature requestWhen a mixed precision data type (such asfp16) is provided toTrainingArguments, it currently sets theACCELERATE_MIXED_PRECISION(cf) environment variable to this mixed precision data type value.Consequently, theTrainerclass uses this mixed precision setting even whenTrainingArgumentsare not explicitly provided asargs=training_args.The expected behavior should be thatTrainerdoes not use mixed precision unless it's explicitly stated in theTrainingArgumentsprovided asargs=training_args.CC:@muellerzr&@pacman100MotivationI encountered this behavior while debugging a model that was producingnans as output logits during training.To troubleshoot this issue, I opted to stop providingTrainingArgumentstoTrainerin order to minimize potential sources of error but unfortunately that did not solved the problem.After extensive investigation, this is how I discovered thatTrainingArgumentsautomatically setsTrainer's mixed precision, even when not explicitly provided as an argument.To prevent others from encountering the same debugging process, I advocate for mixed precision to be set only when explicitly required by the user by providingTrainingArgumentstoTrainer.Your contributionI'm interested in opening a pull request to address this issue. This a good opportunity for me to learn more about the codebase.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_53.txt:\n",
      "Title: Persistent ImportError with TrainingArguments in Hugging Face Transformers despite installing accelerate and dependencie\n",
      "URL: https://github.com/huggingface/transformers/issues/33216\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Windows-10-10.0.19041-SP0Python version: 3.8.0Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0+cpu (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import TrainingArgumentstraining_args = TrainingArguments(output_dir='./output',do_train=True,do_eval=True,num_train_epochs=100,per_device_train_batch_size=32,per_device_eval_batch_size=16,warmup_steps=100,weight_decay=0.05,logging_strategy='steps',logging_dir='./multi-class-logs',logging_steps=50,evaluation_strategy=\"steps\",eval_steps=50,save_strategy=\"steps\",load_best_model_at_end=True)Expected behaviorExpected Behavior:I expect the TrainingArguments class from the Hugging Face transformers library to initialize without any ImportError when I have installed all required dependencies (transformers, accelerate, and PyTorch). Specifically, after running the following commands:pip install transformers[torch]pip install accelerate -U\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_447.txt:\n",
      "Title: WhisperForAudioClassification cannot evaluate during training using use_weighted_layer_sum\n",
      "URL: https://github.com/huggingface/transformers/issues/30104\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.0.dev0Platform: Linux-6.1.58+-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.22.2Safetensors version: 0.4.2Accelerate version: 0.30.0.dev0Accelerate config: \tnot foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): 2.15.0 (True)Flax version (CPU?/GPU?/TPU?): 0.8.2 (cpu)Jax version: 0.4.23JaxLib version: 0.4.23Using GPU in script?:Using distributed or parallel set-up in script?:Who can help?speech models:@sanchit-gandhiInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionFor a classification task, I tried to fine-tune whisper-base pre-trained model using WhisperForAudioClassification and setting use_weighted_layer_sum equal to true. It threw the following error, when it is was evaluating during training.setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.I guess the error occurs when it tries to get prediction, executing the following line of code in my compute metric function:np.argmax(eval_pred.predictions, axis=1)Using whisper-base pretrained model and setting use_weighted_layer_sum equal to trueconfig = AutoConfig.from_pretrained(\n",
      "                'openai/whisper-small',\n",
      "                ..........\n",
      "            )config.use_weighted_layer_sum = Truestart training it using a label datasetExpected behaviorIt should not throw the above error as it should work for both use_weighted_layer_sum = True and use_weighted_layer_sum = False. Note, it does not throw this error while executing the exact same code with use_weighted_layer_sum = False.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_321.txt:\n",
      "Title: Newsave_strategyoption called \"best\" to save when a new best performance is achieved.\n",
      "URL: https://github.com/huggingface/transformers/issues/31626\n",
      "Body:\n",
      "Feature requestIntroduce a new option for thesave_strategyargument called\"best\"which would save the model once a new best performance is achieved.MotivationThesave_strategyargument was first introduced a few years ago in#10286. Currently the supported options are\"no\",\"epoch\", and\"steps\". I'm assuming that this is to match theIntervalStrategythat's used by evaluation as well.Judging by a conversation on a HuggingFace Discussion Forum topic, the best model is always kept by default and therefore if saving occurs at any time during the process then the best model is saved (ref:https://discuss.huggingface.co/t/save-only-best-model-in-trainer/8442). If the user deems that saving often is too burdensome then they may setsave_strategy = \"no\"which would save the best model at the end of training.I believe that introducing some flexibility for saving would be beneficial so that users don't have to perform saving so often but also don't have to wait until the end for a checkpoint.Your contributionIf this feature is deemed worth it by the core maintainers then I'd be willing to take this on myself and open a PR. There are some aspects that I believe might warrant further discussion (e.g., which metric should be used to determine \"best,\" how to overrideIntervalStrategy, etc.).\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_335.txt:\n",
      "Title: Trainer having issues with DataLoaderShard when running with torchrun\n",
      "URL: https://github.com/huggingface/transformers/issues/31457\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.2Platform: Linux-3.10.0-1160.25.1.el7.x86_64-x86_64-with-glibc2.17Python version: 3.11.8Huggingface_hub version: 0.23.3Safetensors version: 0.4.2Accelerate version: 0.31.0Accelerate config:    not foundPyTorch version (GPU?): 2.2.2 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: Yes (CUDA)Using distributed or parallel set-up in script?: Yes, running withtorchrun --nnodes=1 --nproc-per-node=${N_GPUS}Who can help?@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI am fine-tuning a RoBERTa with differential privacy (using PyTorch's Opacus).Thisis the specific script I'm running usingtorchrunfor distributed training. My code also relies onprivate-transformersbut as you can see in the stacktrace below, the error happens inside HuggingFace'sTrainerand I have made a quick fix inside theTrainersource code (shown below) to make my code work. However, I am opening an issue here to see if this is a general issue that needs fixing.Traceback (most recent call last):\n",
      "  File \"/work/fairness-privacy/src/train.py\", line 335, in <module>\n",
      "    train_helper(args, dataset['train'], dataset['validation'])\n",
      "  File \"/work/fairness-privacy/src/train.py\", line 300, in train_helper\n",
      "    model_ft = train_private(args, train_data_tok, val_data_tok)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/work/fairness-privacy/src/train.py\", line 160, in train_private\n",
      "    trainer.train(model_path=None, dev_objective=\"eval_accuracy\")\n",
      "  File \"/work/fairness-privacy/private-transformers/examples/classification/src/trainer.py\", line 401, in train\n",
      "    logging_loss_scalar = self.evaluate_and_log(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/work/fairness-privacy/private-transformers/examples/classification/src/trainer.py\", line 586, in evaluate_and_log\n",
      "    output = self.evaluate()\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/work/fairness-privacy/private-transformers/examples/classification/src/trainer.py\", line 569, in evaluate\n",
      "    output = self.prediction_loop(eval_dataloader, description=\"Evaluation\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/work/fairness-privacy/condaenv/lib/python3.11/site-packages/transformers/trainer.py\", line 3862, in prediction_loop\n",
      "    losses = loss.repeat(batch_size)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: repeat(): argument 'repeats' (position 1) must be tuple of ints, but found element of type NoneType at pos 0I am executing this script using:EPOCHS=1\n",
      "BATCH_SIZE=64\n",
      "EPSILON=8\n",
      "MODEL_OUT=\"models/roberta-priv-eps_${EPSILON}_epochs_${EPOCHS}-bs_${BATCH_SIZE}\"\n",
      "N_GPUS=1\n",
      "\n",
      "torchrun --nnodes=1 --nproc-per-node=${N_GPUS} src/train.py \\\n",
      "    --train-mode private \\\n",
      "    --data-path /work/fairness-privacy/twitteraae-sentiment-data-split/ \\\n",
      "    --epochs $EPOCHS \\\n",
      "    --model-out-path $MODEL_OUT \\\n",
      "    --tracking-interval 5000 \\\n",
      "    --priv-epsilon $EPSILON \\\n",
      "    --priv-max-grad-norm 0.1 \\\n",
      "    --do-evalI am able to avoid this error when I make the following hack insideprediction_loop:from accelerate.data_loader import DataLoaderShard\n",
      "if type(dataloader) == DataLoaderShard:\n",
      "    batch_size = dataloader.total_batch_size\n",
      "else:\n",
      "    batch_size = dataloader.batch_sizeExpected behaviorExpected behavior is thatprediction_loopruns normally and the function that calls it (evaluate_and_log) is able to log the evaluation results during the training process. On a more fine-grained levelbatch_sizeshould be a scalar and notNoneas is happening in this case solosses = loss.repeat(batch_size)insideprediction_loopis able to run.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_453.txt:\n",
      "Title: Flash Attention 2 support for Wav2Vec2ForCTC\n",
      "URL: https://github.com/huggingface/transformers/issues/29994\n",
      "Body:\n",
      "HelloUsage of \"optimum.bettertransformer\" for Wav2Vec2ForCTC gave me the inference speedup for ~2x.How do you think, is it reasonable to try to implement flash attention 2 for Wav2Vec2ForCTC?I saw the same implementation in the Whisper family of models, and it works very well (speedup ~1.8x on batch inference).\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_309.txt:\n",
      "Title: Addbot_tokenattribute toPreTrainedTokenizerandPreTrainedTokenizerFast\n",
      "URL: https://github.com/huggingface/transformers/issues/31709\n",
      "Body:\n",
      "Feature requestI'm requesting for the attributebot_token(beginning-of-tools token) to be added to thePreTrainedTokenizerclasses, similar toeos_token. This token would be associated withself.bot_tokenandself.bot_token_idand would expose the token to downstream consumers like vLLM.MotivationThis request builds offthis PR commentas well as the ongoing work to support function callingin transformers.A number of downstream consumers depend on what's available in thePreTrainedTokenizerclasses, like vLLM'sSequenceclass andLLMEngineclassexample. For example, the current problem I'm facing is that vLLM doesn't correctly label the finish reason for \"tool call\" outputs, as, well, tool calls, since theCompletionOutput.finish_reasonultimately relies on the attributes available inPreTrainedTokenizer.As open-source tool calling proliferates, having these attributes exposed would greatly enhance the utility of the library. This token can be set toNoneby default and should be backwards compatible with the right implementation.Your contributionI can help contribute to the PR and write code. Might need help navigating the library + writing good test cases.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_90.txt:\n",
      "Title: Cannot export sdxl encoder to onnx when transformers[torch] >= 4.43.0 (Occurred when translating scaled_dot_product_attention).\n",
      "URL: https://github.com/huggingface/transformers/issues/33089\n",
      "Body:\n",
      "System Infopython3.9Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib64/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib64/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib64/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/yuntaopan/.local/lib/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib64/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers==4.43.0->transformers[torch]==4.43.0) (4.66.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib64/python3.9/site-packages (from transformers[torch]==4.43.0) (2.1.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.9/site-packages (from transformers[torch]==4.43.0) (0.29.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib64/python3.9/site-packages (from accelerate>=0.21.0->transformers[torch]==4.43.0) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.0->transformers[torch]==4.43.0) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.0->transformers[torch]==4.43.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/yuntaopan/.local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/yuntaopan/.local/lib/python3.9/site-packages (from torch->transformers[torch]==4.43.0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]==4.43.0) (12.4.127)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib64/python3.9/site-packages (from requests->transformers==4.43.0->transformers[torch]==4.43.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.43.0->transformers[torch]==4.43.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.43.0->transformers[torch]==4.43.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.43.0->transformers[torch]==4.43.0) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib64/python3.9/site-packages (from jinja2->torch->transformers[torch]==4.43.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/site-packages (from sympy->torch->transformers[torch]==4.43.0) (1.3.0)Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWhen I use version 4.42.4 transformers,  I can still export.Do I need to make any changes to adapt to version 4.43?export_onnx.pyfrom dataclasses import dataclass\n",
      "from diffusers import DiffusionPipeline\n",
      "from diffusers.utils import BaseOutput\n",
      "from diffusers.utils.torch_utils import randn_tensor\n",
      "import torch\n",
      "import os, sys\n",
      "from pathlib import Path\n",
      "\n",
      "from optimum.exporters.tasks import TasksManager\n",
      "import transformers\n",
      "import onnx\n",
      "import shutil\n",
      "# load both base & refiner\n",
      "base = DiffusionPipeline.from_pretrained(\"sdxl/huggingface_model/\", torch_dtype=torch.float32, variant = 'float16', use_safetensors=True)\n",
      "base.to(\"cpu\")\n",
      "\n",
      "class text_encoder(torch.nn.Module):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.encoder = base.text_encoder\n",
      "\n",
      "    def forward(self, input_ids):\n",
      "        prompt_embeds = self.encoder(input_ids, output_hidden_states=True)\n",
      "        return prompt_embeds[0], prompt_embeds.hidden_states[-2]\n",
      "\n",
      "\n",
      "def export_text_encoder():\n",
      "    model = text_encoder()\n",
      "    input = torch.tensor([range(77)], dtype=torch.long)\n",
      "    output = 'encoder.onnx'\n",
      "\n",
      "    torch.onnx.export(model, (input),\n",
      "                        str(output),\n",
      "                        verbose=False,\n",
      "                        input_names=['input_ids'],\n",
      "                        output_names=['pooled_prompt_embed', 'prompt_embed'],\n",
      "                        #dynamic_axes={\"input\": { 1: \"seq_len\" } },\n",
      "                        do_constant_folding=True,\n",
      "                        opset_version=15)\n",
      "if __name__ == '__main__':\n",
      "\n",
      "    export_text_encoder()Traceback (most recent call last):File \"/home/export_onnx.py\", line 59, inexport_text_encoder()File \"/home/export_onnx.py\", line 49, in export_text_encodertorch.onnx.export(model, (input),File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/utils.py\", line 516, in export_export(File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/utils.py\", line 1596, in _exportgraph, params_dict, torch_out = _model_to_graph(File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/utils.py\", line 1139, in _model_to_graphgraph = _optimize_graph(File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/utils.py\", line 677, in _optimize_graphgraph = _C._jit_pass_onnx(graph, operator_export_type)File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/utils.py\", line 1940, in _run_symbolic_functionreturn symbolic_fn(graph_context, *inputs, **attrs)File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/symbolic_helper.py\", line 306, in wrapperreturn fn(g, *args, **kwargs)File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/symbolic_opset14.py\", line 176, in scaled_dot_product_attentionquery_scaled = g.op(\"Mul\", query, g.op(\"Sqrt\", scale))File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/_internal/jit_utils.py\", line 87, in opreturn _add_op(self, opname, *raw_args, outputs=outputs, **kwargs)File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/_internal/jit_utils.py\", line 238, in _add_opinputs = [_const_if_tensor(graph_context, arg) for arg in args]File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/_internal/jit_utils.py\", line 238, ininputs = [_const_if_tensor(graph_context, arg) for arg in args]File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/_internal/jit_utils.py\", line 269, in _const_if_tensorreturn _add_op(graph_context, \"onnx::Constant\", value_z=arg)File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/_internal/jit_utils.py\", line 246, in _add_opnode = _create_node(File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/_internal/jit_utils.py\", line 305, in _create_node_add_attribute(node, key, value, aten=aten)File \"/usr/local/lib64/python3.9/site-packages/torch/onnx/internal/jit_utils.py\", line 356, inadd_attributereturn getattr(node, f\"{kind}\")(name, value)TypeError: z(): incompatible function arguments. The following argument types are supported:1. (self: torch._C.Node, arg0: str, arg1: torch.Tensor) -> torch._C.NodeInvoked with: %354 : Tensor = onnx::Constant(), scope:main.text_encoder::/transformers.models.clip.modeling_clip.CLIPTextModel::encoder/transformers.models.clip.modeling_clip.CLIPTextTransformer::text_model/transformers.models.clip.modeling_clip.CLIPEncoder::encoder/transformers.models.clip.modeling_clip.CLIPEncoderLayer::layers.0/transformers.models.clip.modeling_clip.CLIPSdpaAttention::self_attn, 'value', 0.125(Occurred when translating scaled_dot_product_attention).Expected behaviorSuccessfully exported onnx\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_282.txt:\n",
      "Title: NotImplementedError: ggml_type 3 not implemented\n",
      "URL: https://github.com/huggingface/transformers/issues/31847\n",
      "Body:\n",
      "DescriptionWhen trying to use the modelQwen/Qwen2-7B-Instruct-GGUFwith the gguf fileqwen2-7b-instruct-q4_0.gguf, I encountered a NotImplementedError about ggml_type 3 not implemented.Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionInstall the required libraries:pip install transformers==4.42.3 gguf==0.6.0Load the model and tokenizer:from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "\n",
      "model_id = \"Qwen/Qwen2-7B-Instruct-GGUF\"\n",
      "file_name = \"qwen2-7b-instruct-q4_0.gguf\"\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file = file_name)\n",
      "model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file = file_name)Expected behaviorThe code should process ggml_type 3 without raising an error, or provide an alternative implementation.System Infotransformers version: 4.42.3gguf version: 0.6.0Platform: Ubuntu 20.04Python version: 3.9.16PyTorch version (GPU?): 2.2.2(True)Using GPU in script?: YesUsing distributed or parallel set-up in script?: No\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_533.txt:\n",
      "Title: tracker:generatecompatibility withtorch.compile\n",
      "URL: https://github.com/huggingface/transformers/issues/28981\n",
      "Body:\n",
      "generate🤜 🤛torch.compileThis issue is a tracker of the compatibility between.generateandtorch.compile(intro docs by pytorch). The goal is to enablefullgraph=Truecompilation on the maingenerateuse cases.⚠️Isyourgenerateuse case not covered by this tracker? Check if it was requested below and upvote it if it was. Otherwise, add a comment. We will consider expanding the selection below on widely requested use cases 🤗Decoding Strategies (end-to-end compilation)greedy_search/sampleare compatible (Generate: end-to-end compilation#30788)beam_search/beam_sampleare compatible, depends on the step aboveassisted_decoding(aka speculative decoding) is compatible, depends on the steps aboveGenerate Flags and OptionsallLogitsProcessorclasses were checked for compatibility (and the appropriate exceptions are raised when not compatible)allStoppingCriteriaclasses were checked for compatibility (and the appropriate exceptions are raised when not compatible)ModelsNotes:models tagged as \"important models\" in our CI + popular modelslanguage models released starting from v4.42 should ALL support compileDecoder-only:GPT-J is compatible (Cache: new Cache format in decoder-only models#31421)GPT2 is compatibleLlama is compatible ([Core generation] Adds support for static KV cache#27931)Gemma is compatible ([gemma] Adds support for Gemma 💎#29167)Llava is compatible (LLaVAtorch.compileimplementation#29891)Mistral is compatible (Add torch.compile for Mistral#30642)Mixtral is compatible (Add torch compile for mixtral#30793)Phi is compatible (Compile compatibilty for decoder-only models#32617)Phi3 is compatible (Phi: static cache & compile compatibility#30688)BLOOM is compatible (note: this one might be tricky due to cache format)Compile compatibilty for decoder-only models#32617Mamba is compatible (requestedhere) (PR:Add torch.compile Support For Mamba#31247)Encoder-decoder:BART is compatibleT5 is compatibleWhisper is compatible ([whisper] static kv cache#31166)QuantizationBNB supportGPTQ supportAWQ supportOthersWe have a benchmark script to quickly compare the impact of PRsAdd section to existing docs on the topicConfirm that pipelines work after compiling generate\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_255.txt:\n",
      "Title: [Error] with Trainer: TypeError: Unsupported types (<class 'NoneType'>) passed to_gpu_broadcast_one.\n",
      "URL: https://github.com/huggingface/transformers/issues/32090\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.4Platform: Linux-5.15.0-101-generic-x86_64-with-glibc2.35Python version: 3.10.13Huggingface_hub version: 0.24.0Safetensors version: 0.4.2Accelerate version: 0.32.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu121 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Who can help?@muellerzr@SunMarc@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionhttps://gist.github.com/halixness/eadd6d1d89ae48597f70cb09f2b44139Expected behaviorHello,I have written a simple training script to train from scratch a gpt2-like model with a large dataset of strings (molecules in SMILES format). After around ~2k steps (batch_size=128,#samples = ~1.5M), I encounter the following error:TypeError: Unsupported types (<class 'NoneType'>) passed to `_gpu_broadcast_one`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.I tried already:to use thedefault_data_collatorinstead and to manually group samples as inin the official example.to check manually for the value of the batch that makes the script crash apparently: no NaN values, it all seems to make sense.to check whether the dataset initially contains any empty or None strings, which is not the case.I'm not sure about what could case this error. Any suggestion is much appreciated!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_269.txt:\n",
      "Title: WavLM returns empty hidden states when loaded directly to GPU\n",
      "URL: https://github.com/huggingface/transformers/issues/31970\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.4Platform: Linux-6.5.0-41-generic-x86_64-with-glibc2.35Python version: 3.9.19Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.31.0PyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: NoUsing GPU in script?: YesGPU type: NVIDIA RTX A6000Who can help?@sanchit-gandhi@GantInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionOutputs of the hidden states are NaN when directly loading the model to the GPU. They work when the model is run on the CPU or first loaded to the CPU then moved to the GPU.This issue can be reproduced using the following code taken from WavLM's huggingface documentation.fromtransformersimportWavLMModel,AutoFeatureExtractorimporttorchfromdatasetsimportload_datasetdataset=load_dataset(\"hf-internal-testing/librispeech_asr_demo\",\"clean\",split=\"validation\",trust_remote_code=True)dataset=dataset.sort(\"id\")sampling_rate=dataset.features[\"audio\"].sampling_rateprocessor=AutoFeatureExtractor.from_pretrained(\"microsoft/wavlm-large\")model=WavLMModel.from_pretrained(\"microsoft/wavlm-large\",device_map=\"cuda:4\")model.eval()# audio file is decoded on the flyinputs=processor(dataset[0][\"audio\"][\"array\"],sampling_rate=sampling_rate,return_tensors=\"pt\")withtorch.no_grad():outputs=model(**inputs.to(\"cuda:4\"),output_hidden_states=True)last_hidden_states=outputs.last_hidden_stateprint(last_hidden_states)The above outputs a tensor with only NaNs. This does not occur if we load the model to the cpu first and then move it to the gpu. (model.to(\"cuda:4\"))Expected behaviorThe hidden states are not NaN when the model is loaded directly to the gpu.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_902.txt:\n",
      "Title: Add FastSpeech2\n",
      "URL: https://github.com/huggingface/transformers/issues/15166\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_725.txt:\n",
      "Title: an inplace operation preventing TorchDistributor training\n",
      "URL: https://github.com/huggingface/transformers/issues/25130\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_719.txt:\n",
      "Title: MassFormer\n",
      "URL: https://github.com/huggingface/transformers/issues/25293\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_718.txt:\n",
      "Title: BertForSequenceClassification does not support 'device_map':\"auto\" yet\n",
      "URL: https://github.com/huggingface/transformers/issues/25296\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_724.txt:\n",
      "Title: Add PromptTemplate and allow for default PromptTemplate in model configuration\n",
      "URL: https://github.com/huggingface/transformers/issues/25147\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_268.txt:\n",
      "Title: Type mis-match in function make_log_bucket_position() of TF DeBERTa V2\n",
      "URL: https://github.com/huggingface/transformers/issues/31988\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.35Python version: 3.12.3Huggingface_hub version: 0.23.2Safetensors version: 0.4.3Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU): 2.3.0+cu121 (True)Tensorflow version (GPU): 2.16.1 (True)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: YesUsing distributed or parallel set-up in script?: YesWho can help?@ArthurZucker,@Rocketknight1InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWhile I was trying to make TFDebertaV2Model work with mixed precision and checking the code in modeling_tf_deberta_v2.py, I found the below code block (in function make_log_bucket_position()) which may cause type mis-match error when executing.from transformers.models.deberta_v2.modeling_tf_deberta_v2 import make_log_bucket_position\n",
      "import tensorflow as tf\n",
      "\n",
      "relative_pos = tf.constant([1,2,3,4], tf.int32)\n",
      "bucket_size = tf.constant(5, tf.int32)\n",
      "max_position = tf.constant(4, tf.int32)\n",
      "make_log_bucket_position(relative_pos, bucket_size, max_position)This code throws the following error message:Traceback (most recent call last):File \"\", line 1, inFile \"/home/swlee/miniconda3/envs/tf216/lib/python3.12/site-packages/transformers/models/deberta_v2/modeling_tf_deberta_v2.py\", line 551, in make_log_bucket_positiontf.cast(tf.math.log(abs_pos / mid), tf.float32) / tf.math.log((max_position - 1) / mid) * (mid - 1)~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~File \"/home/swlee/miniconda3/envs/tf216/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handlerraise e.with_traceback(filtered_tb) from NoneFile \"/home/swlee/miniconda3/envs/tf216/lib/python3.12/site-packages/tensorflow/python/ops/math_ops.py\", line 1412, in _truediv_python3raise TypeError(f\"xandymust have the same dtype, \"TypeError:xandymust have the same dtype, got tf.float32 != tf.float64.Expected behaviorNo TypeError and returns bucket_pos of type int32[in modeling_tf_deberta_v2.py](lines: 550, 551, 552, 553 in function make_log_bucket_position()]tf.math.ceil(\n",
      "      tf.cast(tf.math.log(abs_pos / mid), tf.float32) / tf.math.log((max_position - 1) / mid) * (mid - 1)\n",
      "  )\n",
      "  + mid(correction would be)tf.math.ceil(\n",
      "      tf.cast(tf.math.log(abs_pos / mid), tf.float32) / tf.cast(tf.math.log((max_position - 1) / mid), tf.float32) * tf.cast(mid - 1, tf.float32)  # in graph mode\n",
      "  )\n",
      "  + tf.cast(mid, tf.float32)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_283.txt:\n",
      "Title: Greedy sampling gives a warning message\n",
      "URL: https://github.com/huggingface/transformers/issues/31839\n",
      "Body:\n",
      "System InfoWith new version starting from 4.39, performing greedy search gives a warning:You should set do_sample=True or unset temperature.I am loading pretrained llama2-7b-chat-hf model. I understand that by default temperature is set to 0.6, so I explicitly set it to 0 while calling generate function.Something like this: model.generate(do_sample=False, temperature=0)But I get a warning message that either set do_sample to True or unset temperatureIs the warning hampering the greedy decoding process or can I ignore it?(FYI I have tried with do_sample=True and top_k=1 which should ideally be same as greedy decoding. But just wanted to confirm if do_sample=True really gives greedy decoding results)If we cannot ignore it, then how should I unset the temperature?Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionStep 1. Load the modelmodel = \"meta-llama/Llama-2-7b-chat-hf\"tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)model = AutoModelForCausalLM.from_pretrained(model, load_in_4bit=True, token=access_token)Step 2: Run the generationencoded_input = encoded_input.to(device)generated_text = model.generate(**encoded_input, max_new_tokens=4096, do_sample=False, top_k=1, temperature=0, top_p=0, return_dict_in_generate=True, output_scores=True)Expected behaviorThe model should the words with the highest probability at each generation step\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_91.txt:\n",
      "Title: LR = 0 when using DeepSpeed Config and LORA on Trainer.\n",
      "URL: https://github.com/huggingface/transformers/issues/33086\n",
      "Body:\n",
      "System InfoIf DeepSpeed Config has optimizer/scheduler/fp16 config,will showing warning andlossNot Converges in training:tried to get lr value before scheduler/optimizer started stepping, returning lr=0Then i deleted the config that optimizer/scheduler/fp16 in Deepspeed Config, and config by TraningArguments.He no longer displays this warning and converges normally.transformersversion: 4.42.4Platform: Linux-5.4.241-1-tlinux4Python version: 3.9.16Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.1.0+cpu (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using NPU in script?:NPU type: Ascend910B2CCANN version: 8.0.RC2Who can help?@muellerzr@SunMarc@muellerzrReproductionpeft_config = LoraConfig(\n",
      "            r=128,\n",
      "            lora_alpha=8,\n",
      "            target_modules = ['q_proj','v_proj','k_proj','o_proj'] ,\n",
      "            lora_dropout=0.05,\n",
      "            # bias=\"none\",\n",
      "            # init_lora_weights=\"olora\",\n",
      "            task_type=\"SEQ_2_SEQ_LM\"\n",
      "        )\n",
      "\n",
      "model = get_peft_model(model,peft_config)\n",
      "args = Seq2SeqTrainingArguments(deepspeed = ... )\n",
      "trainer = Trainer(args...)\n",
      "trainer.train()Expected behaviornot show warning and Converges .\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_85.txt:\n",
      "Title: XLMRobertaTokenizer attribute has disappeared from transformers.models.xlm_roberta\n",
      "URL: https://github.com/huggingface/transformers/issues/33104\n",
      "Body:\n",
      "System InfoOracle Linux 9.4 (Oracle-Linux-9.4-2024.07.31-0)Python 3.12.5 (using PyEnv and .venv environment)transformers==4.44.2Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionInstall Oracle OML4Py 2.0 client library as describedhereInstall tranformer library with pip install transformersAfter install, start python and executeimport oml.utilsWe getPython 3.12.5 (main, Aug 21 2024, 09:28:16) [GCC 11.4.1 20231218 (Red Hat 11.4.1-3.0.1)] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> import oml.utils\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/home/opc/vector/.venv/lib/python3.12/site-packages/oml/__init__.py\", line 77, in <module>\n",
      "    from oml.utils import *\n",
      "  File \"/home/opc/vector/.venv/lib/python3.12/site-packages/oml/utils/__init__.py\", line 23, in <module>\n",
      "    from .embeddings import EmbeddingModelConfig,EmbeddingModel\n",
      "  File \"oml/utils/embeddings.py\", line 28, in init oml.utils.embeddings\n",
      "  File \"/home/opc/vector/.venv/lib/python3.12/site-packages/oml/utils/_pipeline/__init__.py\", line 22, in <module>\n",
      "    from .PipelineBuilder import PipelineBuilder\n",
      "  File \"oml/utils/_pipeline/PipelineBuilder.py\", line 25, in init oml.utils._pipeline.PipelineBuilder\n",
      "  File \"oml/utils/_pipeline/steps.py\", line 38, in init oml.utils._pipeline.steps\n",
      "  File \"/home/opc/vector/.venv/lib/python3.12/site-packages/oml/utils/_onnx_export/__init__.py\", line 22, in <module>\n",
      "    from .tokenizer_export import export_tokenizer\n",
      "  File \"oml/utils/_onnx_export/tokenizer_export.py\", line 82, in init oml.utils._onnx_export.tokenizer_export\n",
      "  File \"/home/opc/vector/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1596, in __getattr__\n",
      "    raise AttributeError(f\"module {self.__name__} has no attribute {name}\")\n",
      "AttributeError: module transformers.models.xlm_roberta has no attribute XLMRobertaTokenizerExpected behaviorImport sucessfully, with no error\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_446.txt:\n",
      "Title: Can Transformers AutoModel Support to(dml)\n",
      "URL: https://github.com/huggingface/transformers/issues/30109\n",
      "Body:\n",
      "Feature requestWindowsRX6800XT  Rocmuse DirectML to speed upCan AutoModel.to()  support DirectMLMotivationit useful for me to use glmYour contributionstable diffusion support DirectML\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_52.txt:\n",
      "Title: What's going on with T5 x torch.compile ?\n",
      "URL: https://github.com/huggingface/transformers/issues/33221\n",
      "Body:\n",
      "System InfoHi Team,First of all huge thanks for all the great work you are doing.Recently, I was benchmarking inference for T5 model on ‪AWS EC2 ( G6E machine with L40 GPU) for batch sizes of 1, 2, 4.I have heard tons about torch. compile and wanted to try it out and see if it reduces the inference time. Surprisingly, it did the other way around. On average, I saw an increase of ~1 sec in inference time for a sample size of 50 with a length of each sample ranging from [2200, 3000] characters, with an average of around 2550 chars.I had a chat with a friend about this who told me that T5 is not a very suitable architecture for compilation yet and there are lots of graphbreaks. With his advice, I decided to open an issue here.From my experience, T5 is still a very good model and I would want to see it work seamlessly with torch compile. If chance comes, I am ready to put my own time into this and contribute to the cause. Let me know what you think.Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction‪AWS EC2 ( G6E machine with L40 GPU) for batch sizes of 1, 2, and 4.Expected behaviorThe inference time should reduce post compilation.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_491.txt:\n",
      "Title: Add Microsoft's Code Reviewer model as a dedicated model\n",
      "URL: https://github.com/huggingface/transformers/issues/29490\n",
      "Body:\n",
      "Model descriptionThe model has three goals:Provide feedback on the quality of the code change.Provide feedback on the code change itselfWhen give a code change and a review comment, perform the required action to rectify the issue identified in the commentThe biggest reason for adding this to the HuggingFace library as a dedicated model is so the model size can be expanded and retrained, and potentially provide more accurate and meaningful output.The model has some files in HuggingFace at the moment, but the implementation is incomplete. As seen in the GitHub repo, the T5 model is augmented and the state_dict is loaded on top of this modified model. Ideally, everything should live in HuggingFace.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationCurrent weights and model implementation:https://huggingface.co/microsoft/codereviewerPaper:https://arxiv.org/abs/2203.09095GitHub repo:https://github.com/microsoft/CodeBERT/tree/master/CodeReviewer\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_485.txt:\n",
      "Title: Can we change mistral attention bias=False to bias=config.attention_bias ？\n",
      "URL: https://github.com/huggingface/transformers/issues/29553\n",
      "Body:\n",
      "just like llama attention\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_678.txt:\n",
      "Title: Add ProPainter to transformers\n",
      "URL: https://github.com/huggingface/transformers/issues/26360\n",
      "Body:\n",
      "Model descriptionProPainter can be an excellent addition to Transformers given its impressive inpainting capabilities.Relevant LinksPaper -https://arxiv.org/abs/2309.03897Project Page -https://shangchenzhou.com/projects/ProPainter/Original Code -https://github.com/sczhou/ProPainterWeights -https://github.com/sczhou/ProPainter/releases/tag/v0.1.0(Needs to be merged into a unified one)Author -@sczhouOpen source statusThe model implementation is availableThe model weights are available@amyeroberts@ArthurZuckerIf you think this is a valuable addition I'm more than happy to work on it.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_136.txt:\n",
      "Title: audio pipeline utility ffmpeg_microphone_live only works with PulseAudio in Linux (KDE Manjaro)\n",
      "URL: https://github.com/huggingface/transformers/issues/32660\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: Linux-6.10.3-1-cachyos-bore-x86_64-with-glibc2.40Python version: 3.12.4Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.0+cu121 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Who can help?@NarsilInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfollowhttps://huggingface.co/learn/audio-course/chapter7/voice-assistantExpected behaviorThe course's code running in my local Linux machine does not record anything, I tried to use the latest ffmpeg to record the sound with commandffpmeg -f alsa -i default -t 5 test.wavand got an empty wav file, thenffmpeg -f pulse -i default -t 5 test.wavworked perfectly.So I edited the file,venv/lib/python3.12/site-packages/transformers/pipelines/audio_utils.pyat line 30, changedfomat_ = \"alsa\"intoformat_ = \"pulse\", then the code works as expected and successfully recognised the wake word.Can you please look into this issue with Linux audio system and make it compatible with both ALSA and PulseAudio?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_122.txt:\n",
      "Title: Multiprocessing support\n",
      "URL: https://github.com/huggingface/transformers/issues/32864\n",
      "Body:\n",
      "Running model forwards within a process seems to get stuck. I tried to setTOKENIZERS_PARALLELISMtotrueandfalsebut unfortunately both couldn't help 🥲System Infotransformers-cli env:- `transformers` version: 4.44.0\n",
      "- Platform: Linux-6.10.0-linuxkit-aarch64-with-glibc2.35\n",
      "- Python version: 3.10.14\n",
      "- Huggingface_hub version: 0.24.5\n",
      "- Safetensors version: 0.4.4\n",
      "- Accelerate version: 0.31.0\n",
      "- Accelerate config:    not found\n",
      "- PyTorch version (GPU?): 2.4.0 (False)\n",
      "- Tensorflow version (GPU?): not installed (NA)\n",
      "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
      "- Jax version: not installed\n",
      "- JaxLib version: not installed\n",
      "- Using distributed or parallel set-up in script?: yesWho can help?@ArthurZucker@ganteInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionMinimal example:fromtransformersimportAutoModelForCausalLM,AutoTokenizerfrommultiprocessimportProcess,Queueimportosos.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\"model=AutoModelForCausalLM.from_pretrained(\"gpt2\")tokenizer=AutoTokenizer.from_pretrained(\"gpt2\")tok_ids=tokenizer.encode(\"Multiprocessing with Hugging Face could be an \",return_tensors=\"pt\")deffwd(model,tok_ids,queue):print(\"Starting process\")print(f\"{os.environ['TOKENIZERS_PARALLELISM']=}\")print(f\"{type(model)=}\")print(f\"{tok_ids=}\")try:outs=model(tok_ids)exceptExceptionase:print(f\"Error:{e}\")print(f\"{outs=}\")queue.put(outs)queue=Queue()pr=Process(target=fwd,args=(model,tok_ids,queue))pr.start()pr.join()outs=queue.get()print(outs)printsStarting process\n",
      "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
      "type(model)=<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
      "tok_ids=tensor([[15205,   541,   305,   919,   278,   351, 12905,  2667, 15399,   714,\n",
      "           307,   281,   220]])Expected behaviorShouldn't get stuck.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_644.txt:\n",
      "Title: example code problem\n",
      "URL: https://github.com/huggingface/transformers/issues/27092\n",
      "Body:\n",
      "while trying the example fromhttps://huggingface.co/amazon/MistralLiteThis is the result.python examplecode.pySpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.Traceback (most recent call last):File \"/home/chris/ai/text-generation-webui/amazonmistral/examplecode.py\", line 8, inmodel = AutoModelForCausalLM.from_pretrained(model_id,File \"/home/chris/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 571, in from_pretrainedreturn model_class.from_pretrained(File \"/home/chris/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3076, in from_pretrainedconfig = cls._check_and_enable_flash_attn_2(config, torch_dtype=torch_dtype, device_map=device_map)File \"/home/chris/anaconda3/envs/textgen/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 1265, in _check_and_enable_flash_attn_2raise ValueError(ValueError: The current architecture does not support Flash Attention 2.0. Please open an issue on GitHub to request support for this architecture:https://github.com/huggingface/transformers/issues/new\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_877.txt:\n",
      "Title: Add missing tokenizer test files [:building_construction: in progress]\n",
      "URL: https://github.com/huggingface/transformers/issues/16627\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_683.txt:\n",
      "Title: Make the skip_batches_dataloader used in the Trainer customizable\n",
      "URL: https://github.com/huggingface/transformers/issues/26289\n",
      "Body:\n",
      "Feature requestIn order to allow customizing the skip dataloader, I propose to add a customizable method calledget_skip_dataloaderfor example, which by default will contain the current logic but can be changed by inheriting the Trainer.MotivationI am using a themosaicML streamingframework to train models.In order to use their mid epoch resumption feature of this framework, you have to:use their StreamingDatalaoder, this is already handled by overwritingget_train_dataloadersave the dataloader state dict. This is can be handled with a callback.load the dataloader state dict when resuming training. This can't be done currently because the function responsible for this (skip_first_batches) of Accelerate is not customizableI am able to overcome this problem by patchingskip_first_batches, but on official to do it would be much more comfortable.Your contributionI can provide a PR if you think this is a good idea\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_697.txt:\n",
      "Title: Add D_Nikud model\n",
      "URL: https://github.com/huggingface/transformers/issues/25839\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_898.txt:\n",
      "Title: Adding RelationExtraction head to layoutLMv2 and layoutXLM models\n",
      "URL: https://github.com/huggingface/transformers/issues/15451\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_126.txt:\n",
      "Title: Jamba model: 'AttributeError: 'HybridMambaAttentionDynamicCache' object has no attribute '_modules''\n",
      "URL: https://github.com/huggingface/transformers/issues/32853\n",
      "Body:\n",
      "System InfoTransformers 4.44PyTorch version: 2.3.1Ubuntu 24.04A100 GPUWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionAlmost the exact same sample code found onhttps://huggingface.co/ai21labs/Jamba-v0.1The only modification isdevice_mapfrom transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\",device_map=\"cuda\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n",
      "\n",
      "input_ids = tokenizer(\"In the recent Super Bowl LVIII,\", return_tensors='pt').to(model.device)[\"input_ids\"]\n",
      "\n",
      "outputs = model.generate(input_ids, max_new_tokens=216)\n",
      "\n",
      "print(tokenizer.batch_decode(outputs)Expected behaviorThe error is thrown upon trying to perform inference. The easiest fix I have seen is by going to 'modeling_jamba.py' and makingHybridMambaAttentionDynamicCacheinherit from 'nn.Module' as such:class HybridMambaAttentionDynamicCache(DynamicCache, nn.Module):Then adding the superclass intitialization to the '__init__' method:def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n",
      "    super(HybridMambaAttentionDynamicCache, self).__init__()\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_132.txt:\n",
      "Title: Can't load Llama's tokenizer with add_prefix_space=True parameter.\n",
      "URL: https://github.com/huggingface/transformers/issues/32682\n",
      "Body:\n",
      "System InfoI'm using transformers 4.44.0 but here is a full list.I'm on Pop!_OS 22.04 jammy (x86-64), kernel 6.9.3-76060903-genericName                     Version       Build                     Channel    \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "  _libgcc_mutex            0.1           conda_forge               conda-forge\n",
      "  _openmp_mutex            4.5           2_kmp_llvm                conda-forge\n",
      "  accelerate               0.33.0        pyhd8ed1ab_0              conda-forge\n",
      "  aiohttp                  3.9.5         py39hd1e30aa_0            conda-forge\n",
      "  aiosignal                1.3.1         pyhd8ed1ab_0              conda-forge\n",
      "  async-timeout            4.0.3         pyhd8ed1ab_0              conda-forge\n",
      "  attrs                    24.1.0        pyh71513ae_0              conda-forge\n",
      "  aws-c-auth               0.7.22        h96bc93b_2                conda-forge\n",
      "  aws-c-cal                0.6.14        h88a6e22_1                conda-forge\n",
      "  aws-c-common             0.9.19        h4ab18f5_0                conda-forge\n",
      "  aws-c-compression        0.2.18        h83b837d_6                conda-forge\n",
      "  aws-c-event-stream       0.4.2         ha47c788_12               conda-forge\n",
      "  aws-c-http               0.8.1         h29d6fba_17               conda-forge\n",
      "  aws-c-io                 0.14.8        h21d4f22_5                conda-forge\n",
      "  aws-c-mqtt               0.10.4        h759edc4_4                conda-forge\n",
      "  aws-c-s3                 0.5.9         h594631b_3                conda-forge\n",
      "  aws-c-sdkutils           0.1.16        h83b837d_2                conda-forge\n",
      "  aws-checksums            0.1.18        h83b837d_6                conda-forge\n",
      "  aws-crt-cpp              0.26.9        he3a8b3b_0                conda-forge\n",
      "  aws-sdk-cpp              1.11.329      hba8bd5f_3                conda-forge\n",
      "  brotli-python            1.1.0         py39h3d6467e_1            conda-forge\n",
      "  bzip2                    1.0.8         h4bc722e_7                conda-forge\n",
      "  c-ares                   1.32.3        h4bc722e_0                conda-forge\n",
      "  ca-certificates          2024.7.4      hbcca054_0                conda-forge\n",
      "  certifi                  2024.7.4      pyhd8ed1ab_0              conda-forge\n",
      "  cffi                     1.16.0        py39h7a31438_0            conda-forge\n",
      "  charset-normalizer       3.3.2         pyhd8ed1ab_0              conda-forge\n",
      "  colorama                 0.4.6         pyhd8ed1ab_0              conda-forge\n",
      "  cuda-cudart              12.4.127      0                         nvidia     \n",
      "  cuda-cupti               12.4.127      0                         nvidia     \n",
      "  cuda-libraries           12.4.0        0                         nvidia     \n",
      "  cuda-nvrtc               12.4.127      0                         nvidia     \n",
      "  cuda-nvtx                12.4.127      0                         nvidia     \n",
      "  cuda-opencl              12.4.127      0                         nvidia     \n",
      "  cuda-runtime             12.4.0        ha804496_0                conda-forge\n",
      "  cuda-version             11.8          h70ddcb2_3                conda-forge\n",
      "  cudatoolkit              11.8.0        h4ba93d1_13               conda-forge\n",
      "  cudnn                    8.9.7.29      hbc23b4c_3                conda-forge\n",
      "  datasets                 2.20.0        pyhd8ed1ab_0              conda-forge\n",
      "  dill                     0.3.8         pyhd8ed1ab_0              conda-forge\n",
      "  filelock                 3.15.4        pyhd8ed1ab_0              conda-forge\n",
      "  freetype                 2.12.1        h267a509_2                conda-forge\n",
      "  frozenlist               1.4.1         py39hd1e30aa_0            conda-forge\n",
      "  fsspec                   2024.5.0      pyhff2d567_0              conda-forge\n",
      "  gflags                   2.2.2         he1b5a44_1004             conda-forge\n",
      "  glog                     0.7.1         hbabe93e_0                conda-forge\n",
      "  gmp                      6.3.0         hac33072_2                conda-forge\n",
      "  gmpy2                    2.1.5         py39h048c657_1            conda-forge\n",
      "  h2                       4.1.0         pyhd8ed1ab_0              conda-forge\n",
      "  hpack                    4.0.0         pyh9f0ad1d_0              conda-forge\n",
      "  huggingface_hub          0.24.5        pyhd8ed1ab_0              conda-forge\n",
      "  hyperframe               6.0.1         pyhd8ed1ab_0              conda-forge\n",
      "  icu                      73.2          h59595ed_0                conda-forge\n",
      "  idna                     3.7           pyhd8ed1ab_0              conda-forge\n",
      "  jinja2                   3.1.4         pyhd8ed1ab_0              conda-forge\n",
      "  keyutils                 1.6.1         h166bdaf_0                conda-forge\n",
      "  krb5                     1.21.3        h659f571_0                conda-forge\n",
      "  lcms2                    2.16          hb7c19ff_0                conda-forge\n",
      "  ld_impl_linux-64         2.40          hf3520f5_7                conda-forge\n",
      "  lerc                     4.0.0         h27087fc_0                conda-forge\n",
      "  libabseil                20240116.2    cxx17_he02047a_1          conda-forge\n",
      "  libarrow                 16.1.0        hcb6531f_6_cpu            conda-forge\n",
      "  libarrow-acero           16.1.0        hac33072_6_cpu            conda-forge\n",
      "  libarrow-dataset         16.1.0        hac33072_6_cpu            conda-forge\n",
      "  libarrow-substrait       16.1.0        h7e0c224_6_cpu            conda-forge\n",
      "  libblas                  3.9.0         23_linux64_openblas       conda-forge\n",
      "  libbrotlicommon          1.1.0         hd590300_1                conda-forge\n",
      "  libbrotlidec             1.1.0         hd590300_1                conda-forge\n",
      "  libbrotlienc             1.1.0         hd590300_1                conda-forge\n",
      "  libcblas                 3.9.0         23_linux64_openblas       conda-forge\n",
      "  libcrc32c                1.1.2         h9c3ff4c_0                conda-forge\n",
      "  libcublas                12.4.2.65     0                         nvidia     \n",
      "  libcufft                 11.2.0.44     0                         nvidia     \n",
      "  libcufile                1.9.1.3       0                         nvidia     \n",
      "  libcurand                10.3.5.147    0                         nvidia     \n",
      "  libcurl                  8.8.0         hca28451_1                conda-forge\n",
      "  libcusolver              11.6.0.99     0                         nvidia     \n",
      "  libcusparse              12.3.0.142    0                         nvidia     \n",
      "  libdeflate               1.20          hd590300_0                conda-forge\n",
      "  libedit                  3.1.20191231  he28a2e2_2                conda-forge\n",
      "  libev                    4.33          hd590300_2                conda-forge\n",
      "  libevent                 2.1.12        hf998b51_1                conda-forge\n",
      "  libffi                   3.4.2         h7f98852_5                conda-forge\n",
      "  libgcc-ng                14.1.0        h77fa898_0                conda-forge\n",
      "  libgfortran-ng           14.1.0        h69a702a_0                conda-forge\n",
      "  libgfortran5             14.1.0        hc5f4f2c_0                conda-forge\n",
      "  libgoogle-cloud          2.24.0        h2736e30_0                conda-forge\n",
      "  libgoogle-cloud-storage  2.24.0        h3d9a0c8_0                conda-forge\n",
      "  libgrpc                  1.62.2        h15f2491_0                conda-forge\n",
      "  libhwloc                 2.11.1        default_hecaa2ac_1000     conda-forge\n",
      "  libiconv                 1.17          hd590300_2                conda-forge\n",
      "  libjpeg-turbo            3.0.0         hd590300_1                conda-forge\n",
      "  liblapack                3.9.0         23_linux64_openblas       conda-forge\n",
      "  libmagma                 2.7.2         h09b5827_2                conda-forge\n",
      "  libmagma_sparse          2.7.2         h09b5827_3                conda-forge\n",
      "  libnghttp2               1.58.0        h47da74e_1                conda-forge\n",
      "  libnpp                   12.2.5.2      0                         nvidia     \n",
      "  libnvfatbin              12.4.127      0                         nvidia     \n",
      "  libnvjitlink             12.4.99       0                         nvidia     \n",
      "  libnvjpeg                12.3.1.89     0                         nvidia     \n",
      "  libopenblas              0.3.27        pthreads_hac2b453_1       conda-forge\n",
      "  libparquet               16.1.0        h6a7eafb_6_cpu            conda-forge\n",
      "  libpng                   1.6.43        h2797004_0                conda-forge\n",
      "  libprotobuf              4.25.3        h08a7969_0                conda-forge\n",
      "  libre2-11                2023.09.01    h5a48ba9_2                conda-forge\n",
      "  libsentencepiece         0.2.0         he81a138_2                conda-forge\n",
      "  libsqlite                3.46.0        hde9e2c9_0                conda-forge\n",
      "  libssh2                  1.11.0        h0841786_0                conda-forge\n",
      "  libstdcxx-ng             14.1.0        hc0a3c3a_0                conda-forge\n",
      "  libthrift                0.19.0        hb90f79a_1                conda-forge\n",
      "  libtiff                  4.6.0         h1dd3fc0_3                conda-forge\n",
      "  libtorch                 2.3.1         cuda118_h7aef8b2_300      conda-forge\n",
      "  libutf8proc              2.8.0         h166bdaf_0                conda-forge\n",
      "  libuv                    1.48.0        hd590300_0                conda-forge\n",
      "  libwebp-base             1.4.0         hd590300_0                conda-forge\n",
      "  libxcb                   1.15          h0b41bf4_0                conda-forge\n",
      "  libxml2                  2.12.7        hc051c1a_1                conda-forge\n",
      "  libzlib                  1.2.13        h4ab18f5_6                conda-forge\n",
      "  llvm-openmp              18.1.7        ha31de31_0                conda-forge\n",
      "  lz4-c                    1.9.4         hcb278e6_0                conda-forge\n",
      "  markupsafe               2.1.5         py39hd1e30aa_0            conda-forge\n",
      "  mkl                      2023.2.0      h84fe81f_50496            conda-forge\n",
      "  mpc                      1.3.1         hfe3b2da_0                conda-forge\n",
      "  mpfr                     4.2.1         h38ae2d0_2                conda-forge\n",
      "  mpmath                   1.3.0         pyhd8ed1ab_0              conda-forge\n",
      "  multidict                6.0.5         py39hd1e30aa_0            conda-forge\n",
      "  multiprocess             0.70.16       py39hd1e30aa_0            conda-forge\n",
      "  nccl                     2.22.3.1      hee583db_1                conda-forge\n",
      "  ncurses                  6.5           h59595ed_0                conda-forge\n",
      "  networkx                 3.2.1         pyhd8ed1ab_0              conda-forge\n",
      "  numpy                    1.26.4        py39h474f0d3_0            conda-forge\n",
      "  openjpeg                 2.5.2         h488ebb8_0                conda-forge\n",
      "  openssl                  3.3.1         h4bc722e_2                conda-forge\n",
      "  orc                      2.0.1         h17fec99_1                conda-forge\n",
      "  packaging                24.1          pyhd8ed1ab_0              conda-forge\n",
      "  pandas                   2.2.2         py39hfc16268_1            conda-forge\n",
      "  pillow                   10.3.0        py39h90c7501_0            conda-forge\n",
      "  pip                      24.2          pyhd8ed1ab_0              conda-forge\n",
      "  protobuf                 4.25.3        py39h1be52a0_0            conda-forge\n",
      "  psutil                   6.0.0         py39hd3abc70_0            conda-forge\n",
      "  pthread-stubs            0.4           h36c2ea0_1001             conda-forge\n",
      "  pyarrow                  16.1.0        py39h8003fee_1            conda-forge\n",
      "  pyarrow-core             16.1.0        py39h176f5a7_1_cpu        conda-forge\n",
      "  pyarrow-hotfix           0.6           pyhd8ed1ab_0              conda-forge\n",
      "  pycparser                2.22          pyhd8ed1ab_0              conda-forge\n",
      "  pysocks                  1.7.1         pyha2e5f31_6              conda-forge\n",
      "  python                   3.9.7         hf930737_3_cpython        conda-forge\n",
      "  python-dateutil          2.9.0         pyhd8ed1ab_0              conda-forge\n",
      "  python-tzdata            2024.1        pyhd8ed1ab_0              conda-forge\n",
      "  python-xxhash            3.4.1         py39hd1e30aa_0            conda-forge\n",
      "  python_abi               3.9           4_cp39                    conda-forge\n",
      "  pytorch                  2.3.1         cuda118_py39hd3e083d_300  conda-forge\n",
      "  pytorch-cuda             12.4          hc786d27_6                pytorch    \n",
      "  pytorch-mutex            1.0           cpu                       pytorch    \n",
      "  pytz                     2024.1        pyhd8ed1ab_0              conda-forge\n",
      "  pyyaml                   6.0.1         py39hd1e30aa_1            conda-forge\n",
      "  re2                      2023.09.01    h7f4b329_2                conda-forge\n",
      "  readline                 8.2           h8228510_1                conda-forge\n",
      "  regex                    2024.7.24     py39hcd6043d_0            conda-forge\n",
      "  requests                 2.32.3        pyhd8ed1ab_0              conda-forge\n",
      "  s2n                      1.4.15        he19d79f_0                conda-forge\n",
      "  safetensors              0.4.3         py39h9fdd4d6_0            conda-forge\n",
      "  sentencepiece            0.2.0         hf3d152e_2                conda-forge\n",
      "  sentencepiece-python     0.2.0         py39h4f2c52d_2            conda-forge\n",
      "  sentencepiece-spm        0.2.0         he81a138_2                conda-forge\n",
      "  setuptools               72.1.0        pyhd8ed1ab_0              conda-forge\n",
      "  six                      1.16.0        pyh6c4a22f_0              conda-forge\n",
      "  sleef                    3.6.1         h3400bea_1                conda-forge\n",
      "  snappy                   1.2.1         ha2e4443_0                conda-forge\n",
      "  sqlite                   3.46.0        h6d4b2fc_0                conda-forge\n",
      "  sympy                    1.13.0        pypyh2585a3b_103          conda-forge\n",
      "  tbb                      2021.12.0     h434a139_3                conda-forge\n",
      "  tk                       8.6.13        noxft_h4845f30_101        conda-forge\n",
      "  tokenizers               0.19.1        py39hc62d755_0            conda-forge\n",
      "  torchaudio               2.3.1         py39_cpu                  pytorch    \n",
      "  torchvision              0.18.1        cuda118py39heff4aec_1     conda-forge\n",
      "  tqdm                     4.66.5        pyhd8ed1ab_0              conda-forge\n",
      "  transformers             4.44.0        pyhd8ed1ab_0              conda-forge\n",
      "  typing-extensions        4.12.2        hd8ed1ab_0                conda-forge\n",
      "  typing_extensions        4.12.2        pyha770c72_0              conda-forge\n",
      "  tzdata                   2024a         h0c530f3_0                conda-forge\n",
      "  urllib3                  2.2.2         pyhd8ed1ab_1              conda-forge\n",
      "  websockets               12.0          py39hd1e30aa_0            conda-forge\n",
      "  wheel                    0.44.0        pyhd8ed1ab_0              conda-forge\n",
      "  xorg-libxau              1.0.11        hd590300_0                conda-forge\n",
      "  xorg-libxdmcp            1.1.3         h7f98852_0                conda-forge\n",
      "  xxhash                   0.8.2         hd590300_0                conda-forge\n",
      "  xz                       5.2.6         h166bdaf_0                conda-forge\n",
      "  yaml                     0.2.5         h7f98852_2                conda-forge\n",
      "  yarl                     1.9.4         py39hd1e30aa_0            conda-forge\n",
      "  zlib                     1.2.13        h4ab18f5_6                conda-forge\n",
      "  zstandard                0.23.0        py39h623c9ba_0            conda-forge\n",
      "  zstd                     1.5.6         ha6fb4c9_0                conda-forgeWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", add_prefix_space=True)This will raise an errorTypeError: not a stringExpected behaviorIt should load without error, like other models.I saw a few closed issues (e.g#29625) related to this problem, but it's still happening.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_654.txt:\n",
      "Title: Add quantization_config in AutoModelForCausalLM.from_config()\n",
      "URL: https://github.com/huggingface/transformers/issues/26901\n",
      "Body:\n",
      "Feature requestAdd quantization_config feature to AutoModelForCausalLM from config .I am trying to pretrain a model from scratch and use bits and bytes so that It can be trained on less computation expensive  machines.Below is my quantization config :bnb_config = BitsAndBytesConfig(\n",
      "    load_in_4bit=True,\n",
      "    bnb_4bit_use_double_quant=True,\n",
      "    bnb_4bit_quant_type=\"nf4\",\n",
      "    bnb_4bit_compute_dtype=torch.bfloat16\n",
      ")When I attempted to take the config of certain model from_pretrained function  it failed and raised a Type Error mentioned below.from transformers import AutoConfig, AutoModelForCausalLM\n",
      "config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
      "model = AutoModelForCausalLM.from_config(config,quantization_config=bnb_config, device_map={\"\":0})The Error:---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "Cell In[23], line 7\n",
      "      3 # Download configuration from huggingface.co and cache.\n",
      "      5 configy = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
      "----> 7 modely = AutoModelForCausalLM.from_config(configy,quantization_config=bnb_config, device_map={\"\":0})\n",
      "\n",
      "File ~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:441, in _BaseAutoModelClass.from_config(cls, config, **kwargs)\n",
      "    439 elif type(config) in cls._model_mapping.keys():\n",
      "    440     model_class = _get_model_class(config, cls._model_mapping)\n",
      "--> 441     return model_class._from_config(config, **kwargs)\n",
      "    443 raise ValueError(\n",
      "    444     f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n",
      "    445     f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\n",
      "    446 )\n",
      "\n",
      "File ~/miniconda3/envs/ai/lib/python3.10/site-packages/transformers/modeling_utils.py:1192, in PreTrainedModel._from_config(cls, config, **kwargs)\n",
      "   1190         model = cls(config, **kwargs)\n",
      "   1191 else:\n",
      "-> 1192     model = cls(config, **kwargs)\n",
      "   1194 # restore default dtype if it was modified\n",
      "   1195 if dtype_orig is not None:\n",
      "\n",
      "TypeError: MistralForCausalLM.__init__() got an unexpected keyword argument 'quantization_config'MotivationI had tried a work around by saving the model from the loaded config details from the model and then load the same model with quantization config .I believe this process could get fixed and we can enable/add quantization while loading the model from the config itself.Your contributionfrom transformers import AutoConfig, AutoModelForCausalLM\n",
      "config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
      "model = AutoModelForCausalLM.from_config(config)\n",
      "model.save_pretrained(MODEL_NAME_PATH)\n",
      "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME_PATH, quantization_config=bnb_config, device_map={\"\":0})\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_42.txt:\n",
      "Title: M2M100Tokenizer vocabulary size is not equal to the m2m embedding_size for the \"facebook/m2m100_418M\" model.\n",
      "URL: https://github.com/huggingface/transformers/issues/33240\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.3Platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.31Python version: 3.12.3Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.32.1Accelerate config: \tnot foundPyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA A100-SXM4-80GBWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun the code snippet below:fromtransformersimportM2M100Config,M2M100ForConditionalGeneration,M2M100Tokenizermodel=M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")tokenizer=M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")print(len(tokenizer)==model.get_input_embeddings().weight.shape[0])Expected behaviorI would expect the result of the above code to be True, however it is False.Since the size of the tokenizer vocabulary and theembedding_sizeof the model are different, this causes unwanted behaviors. For example, inexamples/pytorch/translation/run_translation.pythere is this code fragment in charge of performing this same check, and in case it is not fulfilled it resizes the model embeddings.embedding_size=model.get_input_embeddings().weight.shape[0]iflen(tokenizer)>embedding_size:model.resize_token_embeddings(len(tokenizer))\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_56.txt:\n",
      "Title: MultiTask Classification and label_names on Trainer\n",
      "URL: https://github.com/huggingface/transformers/issues/33193\n",
      "Body:\n",
      "System InfoTransformers: 4.40.2Who can help?@muellerzr@SunMarc@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI’m working on a multi task classification with DistilBert with 4 labels.I started training the model and it finished the first epoch, then it starts evaluation and throws the error below at the end of the evaluation. If I take theload_best_model_at_endout of the trainer args it runs the eval, but i get no eval loss. I also ran predict and I found out that I gotlabel_ids=None. It seems that labels_names is not correctly working and is not passed to the predict.I ran:for batch in trainer.get_eval_dataloader(data['test']): print(batch) breakAnd got the follwoing:{'input_ids': tensor([[  101, 67618, 10671,  ...,     0,     0,     0], [  101, 67618, 10671,  ...,   169, 12211,   102], [  101, 27746, 13386,  ...,     0,     0,     0], [  101, 73219, 14002,  ...,     0,     0,     0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0], [1, 1, 1,  ..., 1, 1, 1], [1, 1, 1,  ..., 0, 0, 0], [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'line_labels': tensor([3, 1, 1, 1], device='cuda:0'), 'cat_labels': tensor([ 9, 16, 16, 16], device='cuda:0'), 'sub_cat_labels': tensor([77, 48, 48, 48], device='cuda:0'), 'motive_labels': tensor([ 2, 34, 34, 34], device='cuda:0')}I really need help figuring out what is going on here I out of options I can’t understand what is going on. If you could shed a light I would appreciate it. I'm really at the point where I feel I need to use a custom trainer, because I have no more solution to try.Code:`Defining the metricsLINE_METRIC = evaluate.load(\"f1\")CAT_METRIC = evaluate.load(\"f1\")SUB_CAT_METRIC = evaluate.load(\"f1\")MOTIVE_METRIC = evaluate.load(\"f1\")def compute_metrics(eval_pred):print(eval_pred)all_logits, all_labels = eval_predlogits_line, logits_cat, logits_sub_cat, logits_motive = all_logitsline_labels, cat_labels, sub_cat_labels, motive_labels = all_labelsline_predictions = np.argmax(logits_line, axis=-1)\n",
      "cat_predictions = np.argmax(logits_cat, axis=-1)\n",
      "sub_cat_predictions = np.argmax(logits_sub_cat, axis=-1)\n",
      "motive_predictions = np.argmax(logits_motive, axis=-1)\n",
      "\n",
      "print(\"PRED\")\n",
      "print(line_predictions, cat_predictions, sub_cat_predictions, motive_predictions)\n",
      "\n",
      "line_computed_metrics = LINE_METRIC.compute(predictions=line_predictions, references=line_labels, average='weighted')\n",
      "cat_computed_metrics = CAT_METRIC.compute(predictions=cat_predictions, references=cat_labels, average='weighted')\n",
      "sub_cat_computed_metrics = SUB_CAT_METRIC.compute(predictions=sub_cat_predictions, references=sub_cat_labels, average='weighted')\n",
      "motive_computed_metrics = MOTIVE_METRIC.compute(predictions=motive_predictions, references=motive_labels, average='weighted')\n",
      "\n",
      "print(\"SCORE\")\n",
      "print(line_computed_metrics, cat_computed_metrics, sub_cat_computed_metrics, motive_computed_metrics)\n",
      "\n",
      "return {\n",
      "    'f1_line': line_computed_metrics['f1'],\n",
      "    'f1_cat': cat_computed_metrics['f1'],\n",
      "    'f1_sub_cat': sub_cat_computed_metrics['f1'],\n",
      "    'f1_motive': motive_computed_metrics['f1'],\n",
      "}`output_directory = RESULTS_DIRECTORY evaluation_strategy = 'epoch' per_device_train_batch_size = 4 per_device_eval_batch_size = 4 gradint_accumulation_steps = 2 learning_rate = 2e-5 weight_decay = 0.01 max_grad_norm = 1 num_train_epochs = NUM_TRAIN_EPOCHS lr_scheduler_type = 'linear' warmup_ratio = 0.05 logging_dir = LOGGING_DIRECTORY logging_strategy = 'epoch' save_strategy = 'epoch' save_total_limit = 1 **label_names = ['line_labels', 'cat_labels', 'sub_cal_label','motive_labels']** load_best_model_at_end = True metric_for_best_model = 'eval_f1_cat' greater_is_better = True label_smoothing_factor = 0 #report_to = 'tensorboard' gradient_checkpointing = False`Setup training argumentstraining_args = TrainingArguments(output_dir=output_directory,evaluation_strategy=evaluation_strategy,learning_rate=learning_rate,per_device_train_batch_size=per_device_train_batch_size,per_device_eval_batch_size=per_device_eval_batch_size,num_train_epochs=num_train_epochs,weight_decay=weight_decay,logging_dir=logging_dir,label_names=label_names,max_grad_norm=max_grad_norm,lr_scheduler_type=lr_scheduler_type,warmup_ratio=warmup_ratio,logging_strategy=logging_strategy,save_strategy=save_strategy,save_total_limit=save_total_limit,load_best_model_at_end=load_best_model_at_end,#metric_for_best_model=metric_for_best_model,#greater_is_better=greater_is_better,label_smoothing_factor=label_smoothing_factor,#report_to=report_to,gradient_checkpointing=gradient_checkpointing)#early_stop_callback = EarlyStoppingCallback(3)Initialize the Trainertrainer = Trainer(model=model,args=training_args,train_dataset=data['train'],eval_dataset=data['test'],#tokenizer=tokenizer,compute_metrics=compute_metrics,data_collator=data_collator,#callbacks=[early_stop_callback])`Expected behaviorError:`KeyError                                  Traceback (most recent call last)Cell In[36], line 1----> 1 trainer.train()File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1859, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)1857         hf_hub_utils.enable_progress_bars()1858 else:-> 1859     return inner_training_loop(1860         args=args,1861         resume_from_checkpoint=resume_from_checkpoint,1862         trial=trial,1863         ignore_keys_for_eval=ignore_keys_for_eval,1864     )File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2298, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)2295     self.control.should_training_stop = True2297 self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)-> 2298 self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)2300 if DebugOption.TPU_METRICS_DEBUG in self.args.debug:2301     if is_torch_xla_available():2302         # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2673, in Trainer._maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)2670         self.lr_scheduler.step(metrics[metric_to_check])2672 if self.control.should_save:-> 2673     self._save_checkpoint(model, trial, metrics=metrics)2674     self.control = self.callback_handler.on_save(self.args, self.state, self.control)File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2765, in Trainer.save_checkpoint(self, model, trial, metrics)2763 if not metric_to_check.startswith(\"eval\"):2764     metric_to_check = f\"eval_{metric_to_check}\"-> 2765 metric_value = metrics[metric_to_check]2767 operator = np.greater if self.args.greater_is_better else np.less2768 if (2769     self.state.best_metric is None2770     or self.state.best_model_checkpoint is None2771     or operator(metric_value, self.state.best_metric)2772 ):KeyError: 'eval_loss'`\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_81.txt:\n",
      "Title: Add multi image prompts to multimodal LLMs that support it (PaliGemma)\n",
      "URL: https://github.com/huggingface/transformers/issues/33113\n",
      "Body:\n",
      "Feature requestAdding the ability to pass many images per prompt to PaliGemma. This would mean, among other changes, to change the argument type ofimageson PaliGemmaProcessor to allow array[array[torch.Tensor]] for batch processing.MotivationThe model was trained for multi image / short video tasks so it should be able to take such inputs.Your contributionI could document this if this is supported and I missed it.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_318.txt:\n",
      "Title: Skipping cudagraphs for unknown reason\n",
      "URL: https://github.com/huggingface/transformers/issues/31645\n",
      "Body:\n",
      "System InfoCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.transformersversion: 4.41.2Platform: Linux-5.15.0-112-generic-x86_64-with-glibc2.35Python version: 3.10.13Huggingface_hub version: 0.23.4Safetensors version: 0.4.2Accelerate version: 0.28.0Accelerate config:    not foundPyTorch version (GPU?): 2.1.2 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI readissue 30055andissue 30351, and Llama works well withcache_implementation=\"static\". However, I am trying to usetorch.compilefor other models such aspythiaandphi-2where thecache_implementation=\"static\"is not appliable, and it will produce errors like:[2024-06-26 18:18:57,065] [0/0] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat_3                                        \n",
      "[2024-06-26 18:18:57,065] [0/0] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat_2                                        \n",
      "[2024-06-26 18:18:57,065] [0/0] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat_1\n",
      "[2024-06-26 18:18:57,065] [0/0] torch._inductor.fx_passes.split_cat: [WARNING] example value absent for node: cat\n",
      "skipping cudagraphs for unknown reason\n",
      "...\n",
      "...\n",
      "  File \"/tmp/torchinductor_hc29225/bi/cbiig6bqpidhtncuswvfxwqqjwoiiswlwlrnh7eobbwm4wjlvpts.py\", line 15465, in call\n",
      "    extern_kernels.addmm(arg4_1, reinterpret_tensor(buf3, (16, 2560), (2560, 1), 0), reinterpret_tensor(arg3_1, (2560, 2560), (1, 2560), 0), alpha=1, beta=\n",
      "1, out=buf4)\n",
      "RuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'Here is my code for reproducing the errors.importtorchfromtransformersimportAutoModelForCausalLM,AutoTokenizer# Llama works well with  cache_implementation=\"static\", but other types of models do not have the configuration.# model     = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, attn_implementation=\"sdpa\", token=access_token).cuda().eval()# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=access_token)# model     = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-2.8b\" , torch_dtype=torch.float16, trust_remote_code=True).cuda().eval()# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-2.8b\", trust_remote_code=True)model=AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\",torch_dtype=torch.float16,trust_remote_code=True)tokenizer=AutoTokenizer.from_pretrained(\"microsoft/phi-2\",trust_remote_code=True)model.forward=torch.compile(model.forward,mode=\"reduce-overhead\",fullgraph=True)inputs=tokenizer([\"<s> [INST] Write an essay about large language models [/INST]\"],return_tensors=\"pt\").to(model.device)max_new_tokens=64fn=lambda:model.generate(**inputs,do_sample=False,# cache_implementation=\"static\", # this only works for Llama-2max_new_tokens=max_new_tokens,pad_token_id=tokenizer.pad_token_id,temperature=None,top_p=None)fn()Expected behaviorThe models such aspythiaandphi-2can run withtorch.compileand a clear latency improvement can be observed.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_95.txt:\n",
      "Title: Regression in generating text with Phi-3-mini-4k-instruct with a long prompt (gibberish in v4.42+)\n",
      "URL: https://github.com/huggingface/transformers/issues/32945\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.1Platform: Linux-6.5.0-1023-aws-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.24.1Safetensors version: 0.4.2Accelerate version: 0.30.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noUsing GPU in script?: yesGPU type: Tesla T4Who can help?@ArthurZucker@zucchini-nlpInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductioninput_textcopied from the associated ONNX-runtime bug ticket (see below)importtorchfromtransformersimportGenerationConfig,AutoModelForCausalLM,AutoTokenizermodel_identifier='microsoft/Phi-3-mini-4k-instruct'model=AutoModelForCausalLM.from_pretrained(model_identifier,device_map='cuda',torch_dtype=torch.bfloat16)tokenizer=AutoTokenizer.from_pretrained(model_identifier)input_text=\"\"\"<|user|>Tell me about Paris, France.<|end|><|assistant|>Paris, the capital city of France, is renowned for its rich history, iconic landmarks, and vibrant culture. Known as \"The City of Light,\" Paris is situated in the north-central part of the country along the Seine River.Here are some key aspects of Paris:1. Landmarks: Paris is home to numerous famous landmarks, including the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Champs-Élysées. The Eiffel Tower, built in 1889, is an iconic symbol of Paris and attracts millions of tourists each year. The Louvre Museum, the world's largest art museum, houses thousands of works of art, including the Mona Lisa and the Venus de Milo.2. History: Paris has a rich history dating back to the 3rd century BC, when it was founded by a Celtic tribe called the Parisii. Over the centuries, the city has been influenced by various cultures, including the Romans, the Franks, and the Normans. The French Revolution in the late 18th century marked a significant turning point in Paris's history, leading to the establishment of the modern French Republic.3. Culture: Paris is a global center for art, fashion, gastronomy, and culture. The city is home to numerous museums, including the Centre Pompidou, Musée d'Orsay, and Musée Rodin. Paris is also known for its fashion industry, with many famous designers having their origins in the city. The city's cuisine is also highly regarded, with a focus on fresh ingredients, and a wide variety of dishes, including French classics like coq au vin, boeuf bourguignon, and crêpes.4. Architecture: Parisian architecture is characterized by its diverse styles, ranging from Gothic and Romanesque to Art Nouveau and Art Deco. The city's famous Haussmannian buildings, designed by Baron Haussmann in the mid-19th century, are known for their uniform facades, wrought-iron balconies, and large windows.5. Transportation: Paris has an extensive public transportation system, including the Paris Métro, RER (suburban trains), and buses. The city's iconic yellow taxis are also a popular mode of transportation.6. Language: The official language of Paris is French, and the city's residents are known for their charm and politeness.7. Festivals and Events: Paris hosts numerous festivals and events throughout the year, including the annual Bastille Day celebrations, the Paris Fashion Week, and the famous annual New Year's Eve fireworks on the Eiffel Tower.8. Geography: Paris is located in the north-central part of France, with the Seine River running through the city. The city's geography is characterized by rolling hills and picturesque parks, such as the Bois de Boulogne and the Jardin des Tuileries.9. Population: As of 2021, Paris has an estimated population of around 2.2 million residents, with the metropolitan area housing over 12 million people.In summary, Paris is a city steeped in history, culture, and art, with a unique blend of architectural styles and a vibrant atmosphere that continues to captivate millions of visitors each year.<|end|><|user|>Please give me a list of 5 architectural landmarks in Paris, France.<|end|><|assistant|>1. Eiffel Tower: Designed by Gustave Eiffel and completed in 1889, the Eiffel Tower is an iconic symbol of Paris and France. Standing at 324 meters tall, it was the tallest man-made structure in the world until the completion of the Chrysler Building in New York in 1930. The Eiffel Tower is made of wrought iron and offers visitors stunning views of the city from its three levels.2. Notre-Dame Cathedral: Located on the Île de la Cité, Notre-Dame Cathedral is a masterpiece of French Gothic architecture. Construction began in the 12th century and continued for over 200 years, with the cathedral's completion in the 14th century. The cathedral is famous for its intricate facade, stained-glass windows, and the iconic gargoyles and chimeras.3. Louvre Museum: Originally built as a fortress in the 12th century, the Louvre Museum is now the world's largest art museum and a historic monument in Paris. The museum's most famous landmark is the iconic glass pyramid entrance, designed by architect I. M. Pei in the 1980s. The Louvre houses over 380,000 works of art, including the Mona Lisa and the Venus de Milo.4. Sacré-Cœur Basilica: The Sacré-Cœur Basilica, also known as the Basilique du Sacré-Cœur, is a Roman Catholic church and minor basilica located at the summit of the butte Montmartre, the highest point in Paris. The basilica was designed by Paul Abadie and dedicated in 1914. Its white domes and lavender-colored travertine stone make it a distinctive landmark in the Paris skyline.5. Arc de Triomphe: The Arc de Triomphe is a monumental structure located at the western end of the Champs-Élysées. Commissioned by Napoleon in 1806, the Arc was designed by Jean-François-Thérèse Chalgrin and completed in 1836. The monument honors those who fought and died for France during the French Revolutionary and Napoleonic Wars. The Arc features sculptural reliefs and inscriptions, and its façade is adorned with the names of 357 generals and 660 soldiers.These five architectural landmarks showcase the diverse styles and historical periods of Paris, from Gothic to Neoclassical, and from the 19th to the 20th centuries. Each landmark has its unique features and contributes to the city's rich architectural heritage.<|end|><|user|>Please give me a list of 10 famous items displayed in the Louvre Museum. Thanks!<|end|><|assistant|>1. Mona Lisa: The Mona Lisa, painted by Leonardo da Vinci in the early 16th century, is arguably the most famous painting in the world. The portrait is known for its enigmatic smile and masterful use of sfumato, a technique that creates a soft, hazy effect.2. Venus de Milo: This ancient Greek statue, believed to have been created around 130-100 BC, is a masterpiece of Hellenistic sculpture. The Venus de Milo is renowned for its graceful beauty and the mystery surrounding its missing arms.3. Winged Victory of Samothrace: This Hellenistic sculpture, dating back to the 2nd century BC, depicts the Greek goddess Nike, the personification of victory. The sculpture is celebrated for its dynamic movement and intricate details.4. Liberty Leading the People: This iconic painting by Eugène Delacroix, created in 1830, commemorates the July Revolution in France. The artwork depicts a woman personifying Liberty leading a group of revolutionaries over the bodies of the fallen.5. The Wedding at Cana: A 1516 painting by Veronese, The Wedding at Cana is a large-scale work that depicts the biblical story of Jesus turning water into wine at a wedding feast. The painting is known for its vibrant colors and intricate details.6. The Raft of the Medusa: This 1819 painting by Théodore Géricault is a powerful depiction of the aftermath of the shipwreck of the French frigate Méduse. The painting is famous for its dramatic composition and emotional intensity.7. The Coronation of Napoleon: This 1805 painting by Jacques-Louis David portrays the coronation of Napoleon Bonaparte as Emperor of the French. The artwork is a masterpiece of neoclassical style and captures the pomp and ceremony of the event.8. The Death of Socrates: A 1914 painting by Pablo Picasso, The Death of Socrates depicts the moment when the ancient Greek philosopher Socrates drinks the poison hemlock, as ordered by the Athenian government. The painting is a powerful expression of the existential themes associated with Socrates' death.9. The Turkish Bath: A 1887 painting by Puvis de Chavannes, The Turkish Bath is a large-scale work that depicts a scene of relaxation and leisity in a traditional Turkish bath. The painting is known for its subdued color palette and serene atmosphere.10. The Wedding at Cana: A 1901 painting by Giovanni Boldini, The Wedding at Cana is a lively depiction of a festive Italian wedding. The painting is characterized by Boldini's signature flamboyant style and his ability to capture the essence of the subjects with a sense of movement and energy.These 10 famous items in the Louvre Museum showcase a diverse range of artistic styles and periods, from ancient Greek and Roman sculptures to Renaissance and 19th-century paintings. The collection at the Louvre is one of the world's largest and most important museums, housing over 800,000 works of art, and these famous items are just a small selection of the museum's vast collection.<|end|><|user|>Please give me a list of 10 famous French dishes.<|end|><|assistant|>\"\"\"tokenized=tokenizer(input_text,return_tensors='pt').to('cuda')output=model.generate(**tokenized,max_new_tokens=1000)print(tokenizer.batch_decode(output,skip_special_tokens=True)[0])The first few tokens are reasonable, but the response quickly turns to gibberish:1. Coq au vin: Coq au vin is a classic French dish that consists of chicken cooked in red wine. The dish is typically braised in red wine and served with bacon, and it is a dish. The dish. The French cuisine is a dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The dish. The city. The dish. The city. The of France. Paris. Paris. The city. Paris. Paris. The city. Paris. The city. Paris Paris. Paris. The city. Paris. The city. Paris, the city. Paris, France. Paris, the city. Paris, the city. Paris, the city. Paris, Paris, the city. The city, Paris, Paris, Paris, Paris, Paris, Parisine Paris. of France. Paris. of the Seine. The Eouveen France. Parisinean. The Eiffreaux. The Eroses. The Eiffra Paris, the Eiffreon Paris, the E. The Eggre France, the Eiffre, the Eau, Paris, Paris, Paris, Paris. Paris, Paris, the Paris, the Eighth Paris, the Eaking Paris, Parises, Parisian Paris, Paris, Parisine Parisine Paris, Paris, Paris, France, France, France, Paris, Parisesparis, France, the city France, France, Paris, France, Paris, France, France, Paris, France. France. France. France. France. France, France of France. Parises. Paris, Paris, Paris, Paris, Paris of France. Paris, Parisian France of France of France. France, France, Paris, Paris, France, Paris, France, and the Eunicartic. The E98, the Frenchespars, Paris, France, France, France, France, France, France, the E Seine, France, France, Paris. The city, Paris. The city-artries theater. The Loueshe, Paris, the Eauts, Parises, the Esphere, France, France, France, France, France, France, the Frenchespars, the city-en, Paris, the French, the city. Theater,\n",
      "sight, theater of France of France. France. The Seine. The elle France. The unarchon France, the cityenac France, France, Franceen, 7ives, France, France, France, France France France, France, France, France, France, France.unen,\n",
      "Par-separenighten, theater, the Frenches,\n",
      "-selle, France, France, France Franceen,sight, Paris, for the, Parises, Parises, Parisen, France, and for the Parisen, the Frenchsighten, the Frenchen, France, and, France, France, France, the Seine the Seine, the Frenchsagianasterns, France,\n",
      "s,s,selle, France, the Frenchsons of the Frenchselle,suns,sun France,\n",
      "The Orsights, France,sightsellebelle,sights, 8enights,sightsse,sunakes,s.sorun-Dreccendernses and,says,sight, theorays,s and the citys,sights,s0sights, the Frenchen,s, the\n",
      "swe, the by France,s,selle, and, and,sightakes,s,s to France,s, as the ndauts, France, Franceen, France,unagautheas of the Seine,s, mostoraunora, France, Franceenonauton,s,sunautunelles,sunartheautsunununheunun-sunseanunautenimeasternsartunoraunays,sunsighten France-Relleasternsouth-art-beccanks,sunartunserbyunresunautyunicelle,s bysweora by by by8artenartun bys8artun by Franceunreighs and French,sights ands,s bys by-byunart by its,sights,sightemsqueen ands,spars by-Expected behaviorThe output should not be gibberish. For comparison, with an earlier version of transformers the output is good:1. Coq au Vin: Coq au Vin is a classic French dish that translates to \"rooster in wine.\" The dish consists of chicken braised with wine, lardons, mushrooms, and garlic. It is a hearty and flavorful dish that is often served with potatoes or rice.\n",
      "\n",
      "2. Boeuf Bourguignon: Boeuf Bourguignon is a traditional French beef stew that is made with beef braised in red wine, typically Burgundy, along with mushrooms, onions, and carrots. The dish is known for its rich and savory flavor and is often served with potatoes or noodles.\n",
      "\n",
      "3. Ratatouille: Ratatouille is a vegetable stew that originated in Nice, France. The dish is made with tomatoes, eggplant, zucchini, bell peppers, onions, and garlic, and is seasoned with herbs such as thyme, basil, and oregano. Ratatouille is a healthy and flavorful dish that is often served as a side or as a main course.\n",
      "\n",
      "4. Quiche Lorraine: Quiche Lorraine is a savory pie that originated in the Lorraine region of France. The dish is made with a pastry crust filled with a mixture of eggs, cream, and bacon or ham. Quiche Lorraine is a versatile dish that can be served as a breakfast, lunch, or dinner.\n",
      "\n",
      "5. Bouillabaisse: Bouillabaisse is a traditional Provençal fish stew that originated in the port city of Marseille. The dish is made with a variety of fish, shellfish, and vegetables, and is seasoned with saffron, fennel, and orange peel. Bouillabaisse is a flavorful and aromatic dish that is often served with crusty bread.\n",
      "\n",
      "6. Cassoulet: Cassoulet is a hearty bean stew that originated in the south of France. The dish is made with white beans, meat (usually pork, duck, or sausage), and vegetables such as carrots, onions, and garlic. Cassoulet is a comforting and filling dish that is often served with crusty bread.\n",
      "\n",
      "7. Tarte Tatin: Tarte Tatin is a classic French upside-down apple tart that is made with caramelized apples and puff pastry. The dish is named after the Tatin sisters, who are credited with its creation. Tarte Tatin is a delicious and elegant dessert that is often served with vanilla ice cream.\n",
      "\n",
      "8. Crème Brûlée: Crème Brûlée is a creamy custard dessert that is topped with a layer of caramelized sugar. The dish is made with heavy cream, egg yolks, sugar, and vanilla, and is known for its rich and creamy texture. Crème Brûlée is a popular dessert that is often served in fine dining restaurants.\n",
      "\n",
      "9. Croque Monsieur: Croque Monsieur is a classic French sandwich that is made with ham, cheese, and a béchamel sauce. The dish is topped with a layer of cheese and broiled until golden brown. Croque Monsieur is a delicious and satisfying meal that is often served as a lunch or dinner.\n",
      "\n",
      "10. Crêpes: Crêpes are thin, delicate pancakes that are a staple of French cuisine. Crêpes can be filled with a variety of sweet or savory ingredients, such as Nutella, jam, cheese, ham, or eggs. Crêpes are a versatile and delicious dish that can be enjoyed for breakfast, lunch, or dinner.I first noticed this when performance dropped dramatically after a routine upgrade of transformers. Usinggit bisect, I was able to establish that the commit introducing the issue was730a440. In the course of investigating this, I found similar issues in other transformer libraries; seemingly the same issue exists in bothonnxruntimeandllama.cpp.In my experience, the number of tokens needed in the prompt to make the results nonsense isn't exactly 2048, but a bit higher; in the prompt I was using when I first noticed this (which I unfortunately can't share), I saw a breakdown after 2265 tokens.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_324.txt:\n",
      "Title: Add argument to set number of eval steps in Trainer\n",
      "URL: https://github.com/huggingface/transformers/issues/31561\n",
      "Body:\n",
      "Feature requestI would like to add an argument to theTrainerclass that allows for setting the number of eval steps (batches) for the evaluation procedure during training.The current behavior is to fully iterate through the entire dataset provided by theeval_datasetargument. However, when using a very large evaluation dataset, the evaluation process can take a long time. Especially for debugging, it would be helpful to be able to explicitly specify how many batches you would like to use as part of the evaluation procedure.MotivationCurrently, I am using dataset streaming to train and evaluate models (motivated by large dataset size). Myeval_strategyargument is set to\"steps\"since I'm using the streaming dataset, and so the Trainer runs the evaluation procedure everyeval_stepsnumber of steps. However, because there is no way to control how many steps are run during the evaluation procedure, the Trainer iterates through the entire eval dataset, which can be very time-consuming for a large dataset. Ideally, we could specify how many evaluation steps to run inside the evaluation procedure.Note that the argument I would like to add is different than the currenteval_stepsargument -- the currenteval_stepsargument specifies \"Number of update stepsbetweentwo evaluations\" but what I would like to specify is the number of stepswithinthe evaluations.Your contributionI would be happy to contribute code for this if this is deemed a relevant feature request and there isn't existing functionality that I'm unaware of.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_442.txt:\n",
      "Title: EncoderDecoderModel with XLM-R\n",
      "URL: https://github.com/huggingface/transformers/issues/30211\n",
      "Body:\n",
      "System Infoprivate setup:transformersversion: 4.35.2Platform: Linux-5.4.0-42-generic-x86_64-with-glibc2.29Python version: 3.8.10Huggingface_hub version: 0.19.4Safetensors version: 0.4.1Accelerate version: 0.25.0Accelerate config:    not foundPyTorch version (GPU?): 2.1.2+cu118 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: Tesla V100Using distributed or parallel set-up in script?: NoGoogle Colab setup:transformersversion: 4.38.2Platform: Linux-6.1.58+-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: 0.29.2Accelerate config: \tnot foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): 2.15.0 (True)Flax version (CPU?/GPU?/TPU?): 0.8.2 (gpu)Jax version: 0.4.26JaxLib version: 0.4.26Using GPU in script?: T4 GPUUsing distributed or parallel set-up in script?: NoWho can help?@ArthurZucker@patrickvonplatenInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThose are the training steps for an instruction-tuned shared EncoderDecoder xlm-r model:git clonehttps://gitlab.com/Bachstelze/instructionbert.gitpip install -r ./requirements.txtcreate the dataset filtered down to the context size of xlm-r. The last command parameter would sort the training set by length. For a faster reproduction set it to False:python3 instructionbert/load_multilingual_dataset.py \"FacebookAI/xlm-roberta-base\" 512 Falsetry to train the encoderDecoderModel:python3 test_train_multilingual.py \"FacebookAI/xlm-roberta-base\" 512 True \"adamw_bnb_8bit\" 1 8 50000 0.0001 1 1those ordered training parameters are changeable: model_name, maximal_length, bool_fp16, optimizer, dataloader_workers, batch_size, warmup, lr, accumulation, epochsExpected behaviorThe XLM-R model is supported as encoderDecoderModel, so it should train just like other smallermBERTor monolingualRobertamodels.This \"IndexError: index out of range in self\" is thrown by training it with matching context size of the filtered dataset and model:python3 test_train_multilingual.py \"/media/data/models/xlm-roberta-base\" 512 False \"adamw_hf\" 1 1 20000 0.00001 1 1\n",
      "load the saved and prefiltered dataset from the csv\n",
      "1110446\n",
      "['Heestan waxaa qada Khalid Haref Ahmed \\nOO ku Jiray Kooxdii Dur Dur!', \"Habeen ma hurdoo\\nAday horjoogoo\\nDharaar ma hargalo\\nAduun baabay helayee\\nRuntii ku helayoo\\nCaawaan iman iman\\nOonkaan u liitay\\nIga ba'ay harraadkiisa\\n\\nHannaan wanaageey\\nHadal macaaneey\\nWadnahaad haleeshayoo\\nWaad hirgalaysaa\\nRuntii ku helayoo\\nMaantaan iman iman\\nOonkaan u liitay\\nIga ba'ay harraadkiisa\\n\\nOlolaha jacaylkeenna\\nYididdiiladeeniyo\\nuur midoo fiyowbaan\\nku abaabulaynaa\\nUbixii aan beernaan\\nku intifaacsanaynaa\\nCaawaan iman iman\\nOonkaan u liitay\\nIga ba'ay harraadkiisa\\n\\nAfar gu' iyo dheeraad\\nAxdigaynu taagnay\\nAyaan dantiyo guur\\nu adkaynay gaarnoo\\nMarwadayda noqotoo\\nUbad daadahaysee\\nCaawaan iman iman\\nOonkaan u liitay\\nIga ba'ay harraadkiisa...\"]\n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "accumulated batch size: 1\n",
      "data size without validation set: 1110346\n",
      "max train steps: 1110346\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                           | 0/1110346 [00:00<?, ?it/s]/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "{'loss': 12.575, 'learning_rate': 5.0000000000000004e-08, 'epoch': 0.0}\n",
      "{'eval_loss': 12.56555461883545, 'eval_runtime': 100.4938, 'eval_samples_per_second': 0.995, 'eval_steps_per_second': 0.995, 'learning_rate': 5.0000000000000004e-08, 'epoch': 0.0}\n",
      "{'loss': 12.5526, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}\n",
      "{'eval_loss': 12.492094993591309, 'eval_runtime': 99.7907, 'eval_samples_per_second': 1.002, 'eval_steps_per_second': 1.002, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}\n",
      "{'loss': 12.4601, 'learning_rate': 1.5000000000000002e-07, 'epoch': 0.0}\n",
      "{'eval_loss': 12.378839492797852, 'eval_runtime': 103.8677, 'eval_samples_per_second': 0.963, 'eval_steps_per_second': 0.963, 'learning_rate': 1.5000000000000002e-07, 'epoch': 0.0}\n",
      "{'loss': 12.3297, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}\n",
      "{'eval_loss': 12.233966827392578, 'eval_runtime': 105.7982, 'eval_samples_per_second': 0.945, 'eval_steps_per_second': 0.945, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}\n",
      "  0%|                                            | 497/1110346 [41:38<1299:57:07,  4.22s/it]Traceback (most recent call last):\n",
      "  File \"test_train_multilingual.py\", line 69, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/trainer.py\", line 1555, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/trainer.py\", line 1860, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/trainer.py\", line 2725, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/trainer.py\", line 2748, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py\", line 622, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 957, in forward\n",
      "    outputs = self.roberta(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 830, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 131, in forward\n",
      "    position_embeddings = self.position_embeddings(position_ids)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n",
      "    return F.embedding(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/functional.py\", line 2233, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "IndexError: index out of range in self\n",
      "  0%|          | 497/1110346 [41:40<1550:51:23,  5.03s/it]In google colab thisCUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmulis thrown.The training completes by reducing the filtered dataset to e.g. 510 tokens. Inferencing such a model throws this RuntimeError:Write a tweet for a new transformer model with hashtags #instruction #nlp #generation #encoder #decoder\n",
      "Traceback (most recent call last):\n",
      "  File \"test_train_multilingual.py\", line 110, in <module>\n",
      "    test_run(1)\n",
      "  File \"test_train_multilingual.py\", line 104, in test_run\n",
      "    output_ids = multilingualInstructionBERT.sharedModel.generate(input_ids.cuda(), do_sample=True, max_new_tokens=int(maximal_length))\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1719, in generate\n",
      "    return self.sample(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/generation/utils.py\", line 2801, in sample\n",
      "    outputs = self(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/accelerate/utils/operations.py\", line 680, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/accelerate/utils/operations.py\", line 668, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py\", line 622, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 957, in forward\n",
      "    outputs = self.roberta(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 837, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 525, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 440, in forward\n",
      "    cross_attention_outputs = self.crossattention(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 341, in forward\n",
      "    self_outputs = self.self(\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/hilsenbek/workspace/monolingualInstructionBERT/lib/python3.8/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\", line 277, in forward\n",
      "    context_layer = torch.matmul(attention_probs, value_layer)\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmStridedBatchedExFix( handle, opa, opb, m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, (void*)(&fbeta), c, CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_456.txt:\n",
      "Title: Nested Tensor support\n",
      "URL: https://github.com/huggingface/transformers/issues/29983\n",
      "Body:\n",
      "Feature requestSupport of NestedTensor.Conventional Tensor is dense.For natural language tasks, we also need to pad input tensor with various length to the longest tensor so that it can be processed, and use a mask tensor to indicate the padding positions. This work around does work, but introduces unnecessary computation and have extra memory costs, especially in FFN, where padding is not necessary at all.Nested Tensor stores tensors in a list so that it does not need paddings.Motivationreduce memory costs.speed up process in FFN and Attention (with FlashAttention).AlternativesPyTorch has its own nested_tensor implementation. However, its progress is very slow and unlikely to be useful in the coming months.Your contribution# pylint: disable=protected-accessfrom__future__importannotationsfromtypingimportAny,Callable,Iterable,Mapping,Sequence,SupportsFloatimporttorchfromtorchimportTensorfromtorch.utils.data._utils.collateimportdefault_collate_fn_mapfrom..utilsimportmethod_cachefrom.functionalimportmask_tensor,pad_tensorfrom.utilsimportTorchFuncRegistryclassPNTensor(Tensor):r\"\"\"Wrapper for tensors to be converted to `NestedTensor`.`PNTensor` is a subclass of `torch.Tensor`.It implements two additional methods as `NestedTensor`: `tensor` and `mask`.Although it is possible to directly construct `NestedTensor` in dataset,the best practice is to do so is in `collate_fn`.`PNTensor` is introduced to smooth the process.Convert tensors that will be converted to `NestedTensor` to a `PNTensor`,and PyTorch Dataloader will automatically collate `PNTensor` to `NestedTensor`.\"\"\"@propertydeftensor(self)->Tensor:r\"\"\"Identical to `self`.Returns:(torch.Tensor):Examples:>>> tensor = torch.tensor([1, 2, 3])>>> pn_tensor = PNTensor([1, 2, 3])>>> bool((tensor == pn_tensor).all())True>>> bool((tensor == pn_tensor.tensor).all())True\"\"\"returnself@propertydefmask(self)->Tensor:r\"\"\"Identical to `torch.ones_like(self)`.Returns:(torch.Tensor):Examples:>>> tensor = torch.tensor([1, 2, 3])>>> pn_tensor = PNTensor([1, 2, 3])>>> bool((pn_tensor.mask == torch.ones_like(pn_tensor)).all().item())True\"\"\"returntorch.ones_like(self)defnew_empty(self,*args,**kwargs):returnPNTensor(super().new_empty(*args,**kwargs))classNestedTensor:r\"\"\"Wrap an iterable of tensors into a single tensor with a mask.In sequence to sequence tasks, elements of a batch are usually not of the same length.This made it tricky to use a single tensor to represent a batch of sequences.`NestedTensor` allows to store a sequence of tensors of different lengths in a single object.It also provides a mask that can be used to retrieve the original sequence of tensors.When calling `__getitem__(arg)` on a `NestedTensor`, it has two return type:1. if arg is `int` or `slice`, returns a tuple of two `tensor`s, representing data and padding mask.2. if arg is a `tuple`, return a new `NestedTensor` with specified shape.Attributes:_storage: The sequence of tensors.tensor: padded tensor.mask: mask tensor.batch_first:  Whether the first dimension of the tensors is the batch dimension.If `True`, the first dimension is the batch dimension, i.e., `B, N, *`.If `False`, the first dimension is the sequence dimension, i.e., `N, B, *`padding_value: The padding value used to in padded tensor.mask_value: The mask value used in mask tensor.Args:tensors:batch_first:padding_value:mask_value:Raises:ValueError: If `tensors` is not an iterable.ValueError: If `tensors` is empty.Notes:We have rewritten the `__getattr__` function to support as much native tensor operations as possible.However, not all operations are tested.Please file an issue if you find any bugs.Examples:>>> nested_tensor = NestedTensor(torch.tensor([1, 2, 3]), torch.tensor([4, 5]))>>> nested_tensor.shapetorch.Size([2, 3])>>> nested_tensor.devicedevice(type='cpu')>>> nested_tensor.dtypetorch.int64>>> nested_tensor.tensortensor([[1, 2, 3],[4, 5, 0]])>>> nested_tensor.masktensor([[ True,  True,  True],[ True,  True, False]])>>> nested_tensor.to(torch.float).tensortensor([[1., 2., 3.],[4., 5., 0.]])>>> nested_tensor.half().tensortensor([[1., 2., 3.],[4., 5., 0.]], dtype=torch.float16)>>> nested_tensor[:](tensor([[1, 2, 3],[4, 5, 0]]), tensor([[ True,  True,  True],[ True,  True, False]]))>>> nested_tensor[1](tensor([4, 5]), tensor([True, True]))>>> nested_tensor[:, 1:]NestedTensor([[2, 3],[5, 0]])>>> nested_tensor.tolist()[[1, 2, 3], [4, 5]]>>> NestedTensor(*[[1, 2, 3], [4, 5]])NestedTensor([[1, 2, 3],[4, 5, 0]])\"\"\"__storage:Sequence[Tensor]batch_first:bool=Truepadding_value:SupportsFloat=0.0mask_value:bool=Falsedef__init__(self,*tensors:Iterable[Tensor],batch_first:bool=True,padding_value:SupportsFloat=0.0,mask_value:bool=False,\n",
      "    )->None:iflen(tensors)==1andisinstance(tensors,Sequence):tensors=tensors[0]# type: ignoreself._storage=tensorsself.batch_first=batch_firstself.padding_value=padding_valueself.mask_value=mask_value@propertydef_storage(self):returnself.__storage@_storage.setterdef_storage(self,tensors:Sequence):ifnotisinstance(tensors,Iterable):raiseValueError(f\"tensors must be an Iterable, bug got{type(tensors)}.\")tensors=list(tensors)iflen(tensors)==0:raiseValueError(\"tensors must be a non-empty Iterable.\")ifnotisinstance(tensors[0],Tensor):tensors=[torch.tensor(tensor)fortensorintensors]self.__storage=tensorsdefstorage(self):returnself._storage@propertydeftensor(self)->Tensor:r\"\"\"Return a single tensor by padding all the tensors.Returns:(torch.Tensor):Examples:>>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])>>> nested_tensor.tensortensor([[1, 2, 3],[4, 5, 0]])\"\"\"returnself._tensor(tuple(self._storage),self.batch_first,self.padding_value)@propertydefmask(self)->Tensor:r\"\"\"Padding mask of `tensor`.Returns:(torch.Tensor):Examples:>>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])>>> nested_tensor.masktensor([[ True,  True,  True],[ True,  True, False]])\"\"\"returnself._mask(tuple(self._storage),self.batch_first,self.mask_value)@classmethoddeffrom_tensor_mask(cls,tensor:Tensor,mask:Tensor):r\"\"\"Build a `NestedTensor` object from a padded `Tensor` and corresponding mask `Tensor`.Args:tensor: Padded Tensor.mask: Tensor Mask.Returns:(torch.Tensor):Examples:>>> padded_tensor = torch.tensor([[1, 2, 3, 0, 0],...                                [4, 5, 0, 0, 0],...                                [6, 7, 8, 9, 0]])>>> mask_tensor = torch.tensor([[1, 1, 1, 0, 0],...                             [1, 1, 0, 0, 0],...                             [1, 1, 1, 1, 0]])>>> nested_tensor = NestedTensor.from_tensor_mask(padded_tensor, mask_tensor)>>> nested_tensorNestedTensor([[1, 2, 3, 0],[4, 5, 0, 0],[6, 7, 8, 9]])\"\"\"ifmask.ndim==2:returncls(t[slice(0,m.sum())]fort,minzip(tensor,mask))returncls(t[[slice(0, (m.sum(dim=dim)>0).sum().item())fordiminreversed(range(m.dim()))]]fort,minzip(tensor,mask)\n",
      "        )defnested_like(self,other:Tensor,unsafe:bool=False)->NestedTensor:r\"\"\"Create a new `NestedTensor` from a `Tensor`.The newly created `NestedTensor` will have the same shape as current `NestedTensor`.Args:other: The `Tensor` to be nested.unsafe: Whether to check the shape of `other` and current `NestedTensor`.Returns:(NestedTensor):Examples:>>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])>>> (nested_tensor == nested_tensor.nested_like(nested_tensor)).all()tensor(True)>>> tensor = nested_tensor.tensor>>> (nested_tensor == nested_tensor.nested_like(tensor)).all()tensor(True)>>> f = nested_tensor.nested_like(torch.randn(2, 2))Traceback (most recent call last):ValueError: The shape of NestedTensor and input tensor does not match, torch.Size([2, 3]) != torch.Size([2, 2])>>> p = nested_tensor.nested_like(torch.randn(2, 2), True)>>> p = nested_tensor.nested_like(torch.randn(3, 3), True)Traceback (most recent call last):ValueError: The batch size of NestedTensor and input tensor does not match, 2 != 3\"\"\"# noqa: E501ifisinstance(other,NestedTensor):returnother.clone()ifnotunsafeandself.shape!=other.shape:raiseValueError(f\"The shape of NestedTensor and input tensor does not match,{self.shape}!={other.shape}\")ifself.size(0)!=other.size(0):raiseValueError(f\"The batch size of NestedTensor and input tensor does not match,{self.size(0)}!={other.size(0)}\")returnNestedTensor([o[tuple(slice(0,dim)fordimint.shape)]fort,oinzip(self._storage,other)])@propertydefdevice(self)->torch.device:r\"\"\"Device of the NestedTensor.Returns:(torch.Tensor):Examples:>>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])>>> nested_tensor.devicedevice(type='cpu')\"\"\"returnself._device(tuple(self._storage))@propertydefshape(self)->torch.Size|int:r\"\"\"Alias for `size()`.\"\"\"returnself.size()@propertydefndim(self)->int:r\"\"\"Alias for `dim()`.\"\"\"returnself.dim()defsize(self,dim:int|None=None)->torch.Size|int:r\"\"\"Returns the size of the self `NestedTensor`.Args:dim: If not specified, the returned value is a `torch.Size`, a subclass of `tuple`.If specified, returns an `int` holding the size of that dimension.Defaults to `None`.Returns:(torch.Size | int):Examples:>>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])>>> nested_tensor.size()torch.Size([2, 3])>>> nested_tensor.size(0)2>>> nested_tensor.storage()[1] = torch.tensor([4, 5, 6, 7])>>> nested_tensor.shapetorch.Size([2, 4])>>> nested_tensor.size(1)4\"\"\"returnself._size(tuple(self._storage),dim,self.batch_first)defdim(self)->int:r\"\"\"Number of dimension of the NestedTensor.Returns:(int):Examples:>>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])>>> nested_tensor.dim()2>>> nested_tensor.storage().append(torch.tensor([6, 7, 8, 9]))>>> nested_tensor.ndim2\"\"\"returnself._dim(tuple(self._storage))deftolist(self)->list:r\"\"\"Convert a NestedTensor to a list of lists of values.Returns:(list):Examples:>>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])>>> nested_tensor.tolist()[[1, 2, 3], [4, 5]]\"\"\"return[t.tolist()fortinself._storage]defall(self,dim:int|None=None,keepdim:bool=False)->bool|Tensor|NestedTensor:r\"\"\"Tests if all elements in NestedTensor evaluate to True.Returns:(bool | Tensor):Examples:>>> nested_tensor = NestedTensor([torch.ones(2, 4, dtype=torch.bool), torch.ones(3, 5, dtype=torch.bool)])>>> nested_tensor.all()tensor(True)>>> nested_tensor.all(dim=0)tensor([True, True])>>> nested_tensor.all(dim=0, keepdim=True)tensor([[True, True]])>>> nested_tensor.all(dim=1)NestedTensor([[ True,  True,  True,  True, False],[ True,  True,  True,  True,  True]])>>> nested_tensor.all(dim=1, keepdim=True)NestedTensor([[[ True,  True,  True,  True, False]],<BLANKLINE>[[ True,  True,  True,  True,  True]]])>>> nested_tensor.batch_first = False>>> nested_tensor.all(dim=1)tensor([True, True])>>> nested_tensor.batch_first = False>>> nested_tensor.all(dim=0)NestedTensor([[ True,  True,  True,  True, False],[ True,  True,  True,  True,  True]])>>> nested_tensor.all(dim=1)tensor([True, True])\"\"\"ifdimisNone:returntorch.tensor(all(i.all()foriinself._storage))if(self.batch_firstanddim==0)or(notself.batch_firstanddim==1):ifkeepdim:returntorch.tensor([i.all()foriinself._storage]).unsqueeze(0ifself.batch_firstelse1)returntorch.tensor([i.all()foriinself._storage])ifself.batch_firstordim!=0:dim-=1returnNestedTensor([i.all(dim=dim,keepdim=keepdim)foriinself._storage])defwhere(self,condition,other)->NestedTensor:r\"\"\"Return a NestedTensor of elements selected from either self or other, depending on condition.Returns:(NestedTensor):Examples:>>> nested_tensor = NestedTensor([torch.tensor([1, 2, 3]), torch.tensor([4, 5])])>>> nested_tensor.where(nested_tensor > 2, torch.tensor([[6, 5, 4], [3, 2, 1]]))NestedTensor([[6, 5, 3],[4, 5, 0]])>>> nested_tensor.where(nested_tensor > 2, NestedTensor([[6, 5, 4], [3, 2]]))NestedTensor([[6, 5, 3],[4, 5, 0]])>>> nested_tensor.where(torch.tensor(True), NestedTensor([[6, 5, 4], [3, 2]]))NestedTensor([[1, 2, 3],[4, 5, 0]])\"\"\"ifisinstance(condition,Tensor)andself.shape==condition.shape:condition=self.nested_like(condition)ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(condition,NestedTensor)andisinstance(other,NestedTensor):returnNestedTensor(\n",
      "                [x.where(c,y)forx,c,yinzip(self._storage,condition._storage,other._storage)],**self._state)ifisinstance(condition,NestedTensor):returnNestedTensor([x.where(c,other)forx,cinzip(self._storage,condition._storage)],**self._state)ifisinstance(other,NestedTensor):returnNestedTensor([x.where(condition,y)forx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor(x.where(condition,other)forxinself._storage)@classmethoddef__torch_function__(cls,func,types,args=(),kwargs=None):ifkwargsisNone:kwargs={}iffuncnotinNestedTensorFuncornotall(issubclass(t, (torch.Tensor,NestedTensor))fortintypes):args=[a.tensorifhasattr(a,\"tensor\")elseaforainargs]returnfunc(*args,**kwargs)returnNestedTensorFunc[func](*args,**kwargs)def__getitem__(self,index:int|slice|tuple)->tuple[Tensor,Tensor]|NestedTensor:ifisinstance(index,tuple):returnNestedTensor([t[index[0]][index[1:]]fortinself._storage])ifisinstance(index, (int,slice)):ret=self._storage[index]ifisinstance(ret,Tensor):returnret,torch.ones_like(ret,dtype=torch.bool)returnself.tensor,self.maskraiseValueError(f\"Unsupported index type{type(index)}\")def__getattr__(self,name)->Any:ifnotself._storage:raiseValueError(f\"Unable to get{name}from an empty{self.__class__.__name__}\")ret=[getattr(i,name)foriinself._storage]elem=ret[0]ifisinstance(elem,Tensor):returnNestedTensor(ret,**self._state)ifcallable(elem):returnNestedTensorFuncWrapper(ret,state=self._state)ifelem.__hash__isnotNoneandlen(set(ret))==1:returnelemreturnret@propertydef_state(self)->Mapping:return{k:vfork,vinself.__dict__.items()ifnot(k.startswith(\"_\")ork.endswith(\"_\"))}def__state__(self)->Mapping:returnself.__dict__def__setstate__(self,state:Mapping)->None:self.__dict__.update(state)def__len__(self)->int:returnlen(self._storage)def__repr__(self):returnself.__class__.__name__+repr(self.tensor)[len(self.tensor.__class__.__name__) :]# noqa: E203def__bool__(self)->int:returnall(bool(x)forxinself._storage)def__gt__(# type: ignore[override]self,other:Tensor|NestedTensor|SupportsFloat)->bool|Tensor|NestedTensor:ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor(i>jfori,jinzip(self._storage,other._storage))ifisinstance(other, (int,float,Tensor)):returnNestedTensor([x>otherforxinself._storage],**self._state)raiseTypeError(f\"> not supported between instances of '{type(self)}' and '{type(other)}'\")def__ge__(# type: ignore[override]self,other:Tensor|NestedTensor|SupportsFloat)->bool|Tensor|NestedTensor:ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor(i>=jfori,jinzip(self._storage,other._storage))ifisinstance(other, (int,float,Tensor)):returnNestedTensor([x>=otherforxinself._storage],**self._state)raiseTypeError(f\">= not supported between instances of '{type(self)}' and '{type(other)}'\")def__eq__(# type: ignore[override]self,other:Tensor|NestedTensor|SupportsFloat)->bool|Tensor|NestedTensor:ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor(i==jfori,jinzip(self._storage,other._storage))ifisinstance(other, (int,float,Tensor)):returnNestedTensor([x==otherforxinself._storage],**self._state)returnFalsedef__le__(# type: ignore[override]self,other:Tensor|NestedTensor|SupportsFloat)->bool|Tensor|NestedTensor:ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor(i<=jfori,jinzip(self._storage,other._storage))ifisinstance(other, (int,float,Tensor)):returnNestedTensor([x<=otherforxinself._storage],**self._state)raiseTypeError(f\"<= not supported between instances of '{type(self)}' and '{type(other)}'\")def__lt__(# type: ignore[override]self,other:Tensor|NestedTensor|SupportsFloat)->bool|Tensor|NestedTensor:ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor(i<jfori,jinzip(self._storage,other._storage))ifisinstance(other, (int,float,Tensor)):returnNestedTensor([x<otherforxinself._storage],**self._state)raiseTypeError(f\"< not supported between instances of '{type(self)}' and '{type(other)}'\")def__abs__(self):returnNestedTensor([abs(value)forvalueinself._storage],**self._state)def__add__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x+yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value+otherforvalueinself._storage],**self._state)def__radd__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y+xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other+valueforvalueinself._storage],**self._state)def__iadd__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x+=yelse:forvalueinself._storage:value+=otherreturnselfdef__sub__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x-yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value-otherforvalueinself._storage],**self._state)def__rsub__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y-xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other-valueforvalueinself._storage],**self._state)def__isub__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x-=yelse:forvalueinself._storage:value-=otherreturnselfdef__pos__(self):returnNestedTensor([+xforxinself._storage])def__neg__(self):returnNestedTensor([-xforxinself._storage])def__invert__(self):returnNestedTensor([~xforxinself._storage])def__mul__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x*yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value*otherforvalueinself._storage],**self._state)def__rmul__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y*xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other*valueforvalueinself._storage],**self._state)def__imul__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x*=yelse:forvalueinself._storage:value*=otherreturnselfdef__pow__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x**yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value**otherforvalueinself._storage],**self._state)def__rpow__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y**xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other**valueforvalueinself._storage],**self._state)def__ipow__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x**=yelse:forvalueinself._storage:value**=otherreturnselfdef__matmul__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x@yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value@otherforvalueinself._storage],**self._state)def__rmatmul__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y@xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other@valueforvalueinself._storage],**self._state)def__imatmul__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x@=yelse:forvalueinself._storage:value@=otherreturnselfdef__truediv__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x/yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value/otherforvalueinself._storage],**self._state)def__rtruediv__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y/xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other/valueforvalueinself._storage],**self._state)def__itruediv__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x/=yelse:forvalueinself._storage:value/=otherreturnselfdef__floordiv__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x//yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value//otherforvalueinself._storage],**self._state)def__rfloordiv__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y//xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other//valueforvalueinself._storage],**self._state)def__ifloordiv__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x//=yelse:forvalueinself._storage:value//=otherreturnselfdef__mod__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x%yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value%otherforvalueinself._storage],**self._state)def__rmod__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y%xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other%valueforvalueinself._storage],**self._state)def__imod__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x%=yelse:forvalueinself._storage:value%=otherreturnselfdef__and__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x&yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value&otherforvalueinself._storage],**self._state)def__rand__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y&xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other&valueforvalueinself._storage],**self._state)def__iand__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x&=yelse:forvalueinself._storage:value&=otherreturnselfdef__or__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x|yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value|otherforvalueinself._storage],**self._state)def__ror__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y|xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other|valueforvalueinself._storage],**self._state)def__ior__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x|=yelse:forvalueinself._storage:value|=otherreturnselfdef__xor__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x^yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value^otherforvalueinself._storage],**self._state)def__rxor__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y^xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other^valueforvalueinself._storage],**self._state)def__ixor__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x^=yelse:forvalueinself._storage:value^=otherreturnselfdef__lshift__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x<<yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value<<otherforvalueinself._storage],**self._state)def__rlshift__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y<<xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other<<valueforvalueinself._storage],**self._state)def__ilshift__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x<<=yelse:forvalueinself._storage:value<<=otherreturnselfdef__rshift__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([x>>yforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([value>>otherforvalueinself._storage],**self._state)def__rrshift__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifisinstance(other,NestedTensor):returnNestedTensor([y>>xforx,yinzip(self._storage,other._storage)],**self._state)returnNestedTensor([other>>valueforvalueinself._storage],**self._state)def__irshift__(self,other):ifisinstance(other,Tensor)andself.shape==other.shape:other=self.nested_like(other)ifhasattr(other,\"to\"):other=other.to(self.dtype)ifisinstance(other,NestedTensor):forx,yinzip(self._storage,other._storage):x>>=yelse:forvalueinself._storage:value>>=otherreturnself@method_cache(maxsize=1)def_tensor(self,storage:tuple,batch_first:bool,padding_value:SupportsFloat)->Tensor:ifstorage[0].dim()==0:returntorch.stack(storage,dim=0)returnpad_tensor(storage,size=self.size(),batch_first=batch_first,padding_value=float(padding_value))@method_cache(maxsize=1)def_mask(self,storage:tuple,batch_first:bool,mask_value:bool)->Tensor:ifstorage[0].dim()==0:returntorch.full((len(storage),),notmask_value,dtype=torch.bool,device=self.device)size=self.size()# ignore channel dimensionifstorage[0].dim()>1andlen({t.size(-1)fortinstorage})==1:size=size[:-1]# type: ignorereturnmask_tensor(storage,size=size,batch_first=batch_first,mask_value=mask_value)@method_cache(maxsize=1)def_device(self,storage)->torch.device:returnstorage[0].device@method_cache(maxsize=1)def_size(self,storage,dim:int|None=None,batch_first:bool=True)->torch.Size|int:ifdimisnotNone:ifdim==0:returnlen(storage)returnmax(t.size(dim-1)fortinstorage)ifmax(t.dim()fortinstorage)==0:returntorch.Size((len(storage),))ndim=max(t.dim()fortinstorage)size=[max(t.shape[i]ifi<len(t.shape)else0fortinstorage)foriinrange(ndim)]size.insert(0ifbatch_firstelse1,len(storage))returntorch.Size(size)@method_cache(maxsize=1)def_dim(self,storage)->torch.Size:returnmax(t.dim()fortinstorage)+1NestedTensorFunc=TorchFuncRegistry()@NestedTensorFunc.implement(torch.cat)defcat(tensors,dim:int=0):ifdim!=0:raiseNotImplementedError(f\"NestedTensor only supports cat when dim=0, but got{dim}\")returnNestedTensor([tfortensorintensorsfortintensor._storage],tensors[0]._state)@NestedTensorFunc.implement(torch.isin)defisin(elements,test_elements,*,assume_unique:bool=False,invert:bool=False):ifisinstance(elements,NestedTensor):elements=elements.tensorifisinstance(test_elements,NestedTensor):test_elements=test_elements.tensorreturntorch.isin(elements,test_elements,assume_unique=assume_unique,invert=invert)@NestedTensorFunc.implement(torch.log)deflog(tensor):returnNestedTensor([torch.log(t)fortintensor._storage])@NestedTensorFunc.implement(torch.mean)defmean(input,dim:int|None=None,keepdim:bool=False,*,dtype:torch.dtype|None=None,\n",
      "):returninput.mean(dim=dim,keepdim=keepdim,dtype=dtype)@NestedTensorFunc.implement(torch.sqrt)defsqrt(tensor):returnNestedTensor([torch.sqrt(t)fortintensor._storage])@NestedTensorFunc.implement(torch.stack)defstack(*args,**kwargs):raiseNotImplementedError(\"NestedTensor does not support stack as of now\")classNestedTensorFuncWrapper:# pylint: disable=R0903r\"\"\"Function Wrapper to handle NestedTensor as input.\"\"\"__storage:Sequence[Callable]=[]state:Mapping={}def__init__(self,*callables:Iterable[Callable],state:Mapping|None=None)->None:iflen(callables)==1andisinstance(callables,Sequence):callables=callables[0]# type: ignoreself._storage=callables# type: ignoreifstateisNone:state={}self.state=state@propertydef_storage(self):returnself.__storage@_storage.setterdef_storage(self,callables:Sequence):ifnotisinstance(callables,Sequence):raiseValueError(f\"callables must be a Sequence, bug got{type(callables)}\")iflen(callables)==0:raiseValueError(\"callables must be a non-empty Sequence.\")ifnotcallable(callables[0]):raiseValueError(f\"callables must be a Sequence of Callable, bug got{type(callables[0])}\")self.__storage=callablesdef__call__(self,*args,**kwargs)->NestedTensor|Sequence[Tensor]:ret=[call(*args,**kwargs)forcallinself._storage]elem=ret[0]ifisinstance(elem,Tensor):returnNestedTensor(ret,**self.state)ifelem.__hash__isnotNoneandlen(set(ret))==1:returnelemreturnretdefcollate_pn_tensor_fn(batch,*,collate_fn_map:dict[type|tuple[type, ...],Callable]|None=None):returnNestedTensor(batch)default_collate_fn_map[PNTensor]=collate_pn_tensor_fn\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_330.txt:\n",
      "Title: will this be a model or just a tokenizer/embeddings ? technically a sentence trancformer ?\n",
      "URL: https://github.com/huggingface/transformers/issues/31497\n",
      "Body:\n",
      "Feature requestfor the text component : will this config be abe to take a languge model as a pretrained config ?ie when creating a vision/text decoder model you can specify the tet model component ( ie the mistral model ) ?as the mistral is a decoder only model .... maybe it would need corss attention to connect ?will the text componant be able to be used with the standard AutoModelForCausalLM ? <<< If so it would be prudent to be able to intialize a model with the pretrained components ... ie a lnaguge model and a whisper and a clip model and a xclip ? or to the effect of ... enableing for the genration of modelif these (bodys) ... ie collection of tensors whcih can be read by the imagebind library  transfromer... instead of a simple tokenizer .... it would then be able to utilize the tokenizer from the llm or combine the tokenizers into a single model by adding the speicla tokens to the languge model base ?in truth this would enable the generation of even larger models and give the pretrained model a chance to find thier homes in the new body of a (imagebind) as it is the wrapper which makes the model ...(hence perhaps also including a feature to execute code on the fly and return the executing back to the model directly if enabled or ok byu the user...as this is the method as the model could generate its on code to execute in opython and return the result to the transformer to be utilized in the response it is generating .... so if the model is given functions as an input (it would also be able to run them in the back ground pre retuning the response) ... also given code by the user in chat it should be able to run the code in ipython behind the scenes and return the response (not on the system directoy(hence havign hiddon ipython repl) ..(it would still have to make the calls)Motivationrags internally !usablity ! of the model wrapper to understand that each component could be trained seperate or even within the model itself !as well as fitting with anything in and wnything out principle (media wise) (code as an input (function))Your contributioni really would like to see this project get forwards as its the same thing i was working towards but i have fiath you will succeed ... if not i will stillc arry on my path !\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_293.txt:\n",
      "Title: Loading HuBERT models with DeepSpeed ZeRO-3 causes program to hang\n",
      "URL: https://github.com/huggingface/transformers/issues/31797\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.3Platform: Linux-5.14.0-362.24.1.el9_3.x86_64-x86_64-with-glibc2.34Python version: 3.10.14Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: yesUsing GPU in script?: noGPU type: NVIDIA A100-SXM4-40GBWho can help?@sanchit-gandhi@muellerzrInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionhubert_mre.py:fromtransformersimportAutoConfig,HubertModel,TrainingArguments,HfArgumentParserdefmain():parser=HfArgumentParser(TrainingArguments)training_args=parser.parse_args_into_dataclasses()[0]config=AutoConfig.from_pretrained(\"facebook/hubert-large-ls960-ft\")model=HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\",config=config)if__name__==\"__main__\":main()hubert_mre.sh:exportNCCL_DEBUG=INFOexportNCCL_DEBUG_SUBSYS=ALLexportCUDA_LAUNCH_BLOCKING=1\n",
      "\n",
      "OUTPUT_DIR=$HOME/hubert_mre\n",
      "\n",
      "deepspeed \\\n",
      "    --num_gpus 2 \\\n",
      "    --master_port 60000 \\\n",
      "    ./hubert_mre.py \\\n",
      "    --output_dir$OUTPUT_DIR\\\n",
      "    --deepspeed zero3.jsonzero3.json:{\"fp16\": {\"enabled\":\"auto\",\"loss_scale\":0,\"loss_scale_window\":1000,\"initial_scale_power\":16,\"hysteresis\":2,\"min_loss_scale\":1},\"bf16\": {\"enabled\":\"auto\"},\"train_micro_batch_size_per_gpu\":\"auto\",\"train_batch_size\":\"auto\",\"gradient_accumulation_steps\":\"auto\",\"zero_optimization\": {\"stage\":3,\"overlap_comm\":true,\"contiguous_gradients\":true,\"sub_group_size\":1e9,\"reduce_bucket_size\":\"auto\",\"stage3_prefetch_bucket_size\":\"auto\",\"stage3_param_persistence_threshold\":\"auto\",\"stage3_max_live_parameters\":1e9,\"stage3_max_reuse_distance\":1e9,\"stage3_gather_16bit_weights_on_model_save\":true}\n",
      "}Runhubert_mre.shand watch the script hang indefinitely.The curious thing is that this seems to happen only with HuBERT models. If, for example, you replaceHubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")withWav2Vec2BertModel.from_pretrained(\"facebook/w2v-bert-2.0\"), the script runs just fine.Also, this works fine if you pass--num_gpus 1.Expected behaviorThe script runs to completion without hanging indefinitely.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_287.txt:\n",
      "Title: DirectML storage compatibility issues\n",
      "URL: https://github.com/huggingface/transformers/issues/31823\n",
      "Body:\n",
      "Feature requestI'm trying to run transformers models using DirectML, and it works fine in most cases. However, since Microsoft's DirectML does not support storage, this causes an error whenever an untyped_storage method is involved.Package versions installed:torch~=2.3.1torch_directml=0.2.2.dev240614transformers=4.42.3Motivationcompatibility issuesYour contributionI hope there is a way to provide a custom device context method without modifying the transformers source code. In my current use case, I can write compatible code for storage and skip this part.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_278.txt:\n",
      "Title: [BUG] GPT-2 tokenizer is NOT invertible\n",
      "URL: https://github.com/huggingface/transformers/issues/31884\n",
      "Body:\n",
      "System InfoHello,It is my understanding that the gpt-2 tokenizer, obtained withAutoTokenizer.from_pretrained(\"gpt2\"), should be invertible. That is, given a sentencetext, we should have thattext == tokenizer.decode(tokenizer(text, add_special_tokens=False)[\"input_ids\"])However, it is not the case, unlike thetiktokenreference implementation, which is correctly invertible.For example, given the sentenceIs this restaurant family-friendly ? Yes No Unsure ? This is a follow-up sentence ., encoding + decoding removes the space before punctuations, yielding a different sentence.I have tried instantiating the tokenizer usingGPT2Tokenizer.from_pretrained(\"openai-community/gpt2\"), and using the optionsadd_prefix_space=Trueoris_split_into_words=True, but the problem persists.Hence, it looks like a bug to me, since BPE tokenizers should be invertible, as far as I understand.Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun this code, and you should see the bug. I am usingtransformers==4.38.2#gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)gpt2_tokenizer=GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")oai_tokenizer=tiktoken.get_encoding(\"gpt2\")orig=\"Is this restaurant family-friendly ? Yes No Unsure ? This is an other sentence .\"hf_enc=gpt2_tokenizer(orig)[\"input_ids\"]hf_dec=gpt2_tokenizer.decode(hf_enc)oai_enc=oai_tokenizer.encode(orig)oai_dec=oai_tokenizer.decode(oai_enc)print(hf_dec)print(oai_dec)Expected behaviorThe two decoded sentence should be equal, yet they are not.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_536.txt:\n",
      "Title: Misleading ImportError when using JAX tensors without Flax installed\n",
      "URL: https://github.com/huggingface/transformers/issues/28959\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.2Platform: Linux-5.19.0-1027-gcp-x86_64-with-glibc2.35Python version: 3.11.4Huggingface_hub version: 0.20.1Safetensors version: 0.4.1Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.3.0.dev20231228+cpu (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installed  # But I have installedJaxLib version: not installed  # But I have installedUsing GPU in script?: N/AUsing distributed or parallel set-up in script?: N/AWho can help?@sanchit-gandhi@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWhile attempting to convert tokenizer outputs to JAX tensors using the following code:fromtransformersimportAutoTokenizertokenizer=AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')sentences=['hello world']inputs=tokenizer(sentences,padding=True,return_tensors='jax')I received the following ImportError:ImportError: Unable to convert output to JAX tensors format, JAX is not installed.However, JAX is indeed installed in my environment.Expected behaviorUpon further investigation, it seems the error arises because the library checks for Flax's availability (is_flax_available()) rather than JAX's direct presence. Here is the snippet from thesource codethat led to this conclusion:ifnotis_flax_available():raiseImportError(\"Unable to convert output to JAX tensors format, JAX is not installed.\")This can be somewhat misleading, as the error message suggests a lack of JAX installation, while the actual requirement is for Flax. Not all JAX users utilize Flax, and this might cause confusion.Would it be possible to update the error message to more accurately reflect the requirement for Flax when attempting to use JAX tensor formats? Such a clarification would greatly assist users in diagnosing setup issues.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_522.txt:\n",
      "Title: Generate: support passing position_ids\n",
      "URL: https://github.com/huggingface/transformers/issues/29149\n",
      "Body:\n",
      "Thank you@tengomucho, for uncovering this bug.The problemIn a nutshell, passing the correctposition_idstogenerateshould result in exactly the same results as not passing them. In other words, the following test should pass on all models, if added toGenerationTesterMixin. We can see that it is failing in general.deftest_passing_position_ids(self):# Check that passing position ids to generate yields the same results as not passing them, if the position ids# are correctly built. If the test fails, it means one of two things:# 1 - the manual position ids are not being piped correctly; OR# 2 - the automated position ids are not being correctly built.formodel_classinself.all_generative_model_classes:config,input_ids,attention_mask,_=self._get_input_ids_and_config(batch_size=1)ifconfig.is_encoder_decoder:self.skipTest(\"This model does not support position_ids\")# To truly test this property, let's create a batch where the second row corresponds to the test input with# left padding of 1.pad_token=torch.tensor([[config.pad_token_idor0]],device=input_ids.device,dtype=input_ids.dtype)input_ids=torch.cat((input_ids,torch.cat((pad_token,input_ids[:,1:]),dim=1)),dim=0)pad_mask=torch.zeros((1,1),dtype=attention_mask.dtype,device=attention_mask.device)attention_mask=torch.cat((attention_mask,torch.cat((pad_mask,attention_mask[:,1:]),dim=1)),dim=0)position_ids=torch.clamp(torch.cumsum(attention_mask,dim=-1)-1,min=0)config.use_cache=Trueconfig.is_decoder=Truemodel=model_class(config).to(torch_device).eval()try:output_position_ids=model.generate(input_ids,attention_mask=attention_mask,position_ids=position_ids,max_new_tokens=10)exceptValueErrorasexc:if\"The following `model_kwargs` are not used by the model: ['position_ids']\"instr(exc):self.skipTest(\"This model does not support position_ids\")else:raiseoutput_no_position_ids=model.generate(input_ids,attention_mask=attention_mask,max_new_tokens=10)self.assertListEqual(output_no_position_ids.tolist(),output_position_ids.tolist())The fixThere are two root causes for this:position_idsis rejected in some models when it is passed (e.g. seehere). These models often assume no padding whenposition_idsis rejected.position_idsis never updated, so it is only correct when created from scratch (=not passed).As such, a fix to this problem should consist in updatingposition_idsingenerate, withprepare_inputs_for_generationonly creating newposition_idswhen they don't exist.The test pasted above should be part of our tests after fixing the issue.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_907.txt:\n",
      "Title: Support on Mixture of expert models\n",
      "URL: https://github.com/huggingface/transformers/issues/14814\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_734.txt:\n",
      "Title: Time Series Transformer - Dynamic Categorical Features\n",
      "URL: https://github.com/huggingface/transformers/issues/24695\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_735.txt:\n",
      "Title: Is there any plan to add kosmos-2 to the transformers.\n",
      "URL: https://github.com/huggingface/transformers/issues/24671\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_906.txt:\n",
      "Title: Fine-tuning GPT-J-6B in colab: 8-bit weights with low-rank adaptors\n",
      "URL: https://github.com/huggingface/transformers/issues/14839\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1.txt:\n",
      "Title: Track progress for VLMs refactoring\n",
      "URL: https://github.com/huggingface/transformers/issues/33374\n",
      "Body:\n",
      "This issue tracks the progress on improving the handling and testing of Vision-Language Models. The main goals are to enhance/enable generation tests, handle other generation techniques like assisted decoding and ensure all models pass CI checks.I already started working on it and merged/opened some PRs. This issue should help us track how much is left until VLMs are standardized from modeling code perspective.Enable Generation Tests for VLMsMerged a PR to calculate and expand text with \"image\" tokens in processing. VLMs currently add only one placeholder per visual. During the modeling phase, we expand the inputs to match the actual length of image embeddings. This approach limits the functionality ofgenerate(), especially in enabling other cache formats and torch.compile and introduces hidden bugs. (Expand inputs in processors for VLMs#30962)Verify that the addition ofpreprocessor_config.jsonon the hub does not break existing functionality. Related discussion on slack:https://huggingface.slack.com/archives/C01N44FJDHT/p171957701917237). TL;DR: we can't avoid breaking BC but we still want the feature as it has so many benefits. So we'll just try again and hope that users don't use the old version anymoreFix Failing Edge Cases in Current VLMsIdentified edge cases involving multi-image inputs and cache position preparation after merging the above PR (VLM: fixes after refactor#32907)Introducenum_image_tokensattribute for specifying image sequence length. It ensures text expansion to the correct length based on the image backbone, otherwise we can't currently use the same processing class for different image backbones. (draft available locally)Add Generation Tests to VLM ClassesAlready added in LLaVA-Onevision and Qwen2-VL (Llava Onevision: add model#32673,Qwen2-VL: clean-up and add more tests#33354)ImplementGenerationTesterMixinto include tests with both image and text inputs. Current tests accept only text as input. Enable for all models except BLIP (draft available locally)Special Case for BLIPCreate a PR to adapt testing suite for BLIP'smain_input_namewhich is notinput_idslike in other model, but ispixel_values. Check that we don't cause red CI if we rely on model'smain_input_namefor testsRemove BLIP's custom generation logic and enable generation tests, that should also help us get rid of extra hacks for handling maximum length orBOStoken in modeling codeFinalizing CI for VLMsResolveattention_Implementationrelated failures to make CI fully happy for VLMs (Attn implementation for composite models#32238)Ensure all VLMs pass all CI checks, including slow tests. Identify the reason and fix if there are failures (most probably failure is related to torch version, but need double check)Motivation,Your contribution.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_523.txt:\n",
      "Title: Flash attention implementation with BERT base model\n",
      "URL: https://github.com/huggingface/transformers/issues/29129\n",
      "Body:\n",
      "Model descriptionhello and thanks community.I am trying to replace standard attention by flash attention in the BERT base Model. Anyone please help not able to find any tutorial or any discussions.or just give some directions how to do that ..I have got the idea of making attention prob drop prob = 0 . it makes sense but not sure how it is going to work.@tridao@Arthur@jamaliki@sorenmc@LysandreJik@ArthurZuckerOpen source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationNo response\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_245.txt:\n",
      "Title: TimmBackbone.from_pretrained out_indices not working correctly (out_indices is sorted using strings blocks.11 before blocks.3)\n",
      "URL: https://github.com/huggingface/transformers/issues/32133\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.4Platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35Python version: 3.10.14Huggingface_hub version: 0.23.5Safetensors version: 0.4.3Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.3.1+cu121 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noWho can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import TimmBackbone\n",
      "backbone = TimmBackbone.from_pretrained(\"eva02_base_patch14_224.mim_in22k\", out_indices=[3, 5, 7, 11], num_channels=1)---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "Cell In[20], [line 1](vscode-notebook-cell:?execution_count=20&line=1)\n",
      "----> [1](vscode-notebook-cell:?execution_count=20&line=1) backbone = TimmBackbone.from_pretrained(\n",
      "      [2](vscode-notebook-cell:?execution_count=20&line=2)     \"eva02_base_patch14_224.mim_in22k\",\n",
      "      [3](vscode-notebook-cell:?execution_count=20&line=3)     out_indices=[3, 5, 7, 11],\n",
      "      [4](vscode-notebook-cell:?execution_count=20&line=4)     num_channels=1,\n",
      "      [5](vscode-notebook-cell:?execution_count=20&line=5)     # out_features=[[\"blocks.3\", \"blocks.5\", \"blocks.7\", \"blocks.11\"]],\n",
      "      [6](vscode-notebook-cell:?execution_count=20&line=6) )\n",
      "\n",
      "File /mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:113, in TimmBackbone.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n",
      "    [105](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:105) out_indices = kwargs.pop(\"out_indices\", config.out_indices)\n",
      "    [106](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:106) config = TimmBackboneConfig(\n",
      "    [107](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:107)     backbone=pretrained_model_name_or_path,\n",
      "    [108](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:108)     num_channels=num_channels,\n",
      "   (...)\n",
      "    [111](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:111)     out_indices=out_indices,\n",
      "    [112](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:112) )\n",
      "--> [113](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:113) return super()._from_config(config, **kwargs)\n",
      "\n",
      "File /mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1428, in PreTrainedModel._from_config(cls, config, **kwargs)\n",
      "   [1426](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1426)         model = cls(config, **kwargs)\n",
      "   [1427](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1427) else:\n",
      "-> [1428](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1428)     model = cls(config, **kwargs)\n",
      "   [1430](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1430) # restore default dtype if it was modified\n",
      "   [1431](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1431) if dtype_orig is not None:\n",
      "\n",
      "File /mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:89, in TimmBackbone.__init__(self, config, **kwargs)\n",
      "     [85](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:85) self._return_layers = {\n",
      "     [86](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:86)     layer[\"module\"]: str(layer[\"index\"]) for layer in self._backbone.feature_info.get_dicts()\n",
      "     [87](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:87) }\n",
      "     [88](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:88) self._all_layers = {layer[\"module\"]: str(i) for i, layer in enumerate(self._backbone.feature_info.info)}\n",
      "---> [89](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/models/timm_backbone/modeling_timm_backbone.py:89) super()._init_backbone(config)\n",
      "\n",
      "File /mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:190, in BackboneMixin._init_backbone(self, config)\n",
      "    [187](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:187) self.backbone_type = BackboneType.TIMM if self.use_timm_backbone else BackboneType.TRANSFORMERS\n",
      "    [189](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:189) if self.backbone_type == BackboneType.TIMM:\n",
      "--> [190](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:190)     self._init_timm_backbone(config)\n",
      "    [191](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:191) elif self.backbone_type == BackboneType.TRANSFORMERS:\n",
      "    [192](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:192)     self._init_transformers_backbone(config)\n",
      "\n",
      "File /mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:162, in BackboneMixin._init_timm_backbone(self, config)\n",
      "    [159](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:159) out_features = self._backbone.feature_info.module_name()\n",
      "    [161](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:161) # We verify the out indices and out features are valid\n",
      "--> [162](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:162) verify_out_features_out_indices(\n",
      "    [163](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:163)     out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n",
      "    [164](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:164) )\n",
      "    [165](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:165) self._out_features, self._out_indices = out_features, out_indices\n",
      "\n",
      "File /mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:49, in verify_out_features_out_indices(out_features, out_indices, stage_names)\n",
      "     [47](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:47)         raise ValueError(f\"out_features must not contain any duplicates, got {out_features}\")\n",
      "     [48](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:48)     if out_features != (sorted_feats := [feat for feat in stage_names if feat in out_features]):\n",
      "---> [49](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:49)         raise ValueError(\n",
      "     [50](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:50)             f\"out_features must be in the same order as stage_names, expected {sorted_feats} got {out_features}\"\n",
      "     [51](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:51)         )\n",
      "     [53](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:53) if out_indices is not None:\n",
      "     [54](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/mnt/c/Users/fuchsfa/Desktop/Git/foundation-models/.venv/lib/python3.10/site-packages/transformers/utils/backbone_utils.py:54)     if not isinstance(out_indices, list):\n",
      "\n",
      "ValueError: out_features must be in the same order as stage_names, expected ['blocks.3', 'blocks.5', 'blocks.7', 'blocks.11'] got ['blocks.11', 'blocks.3', 'blocks.5', 'blocks.7']Expected behaviorThe could should load the model correctly.I think this lineif positive_indices != tuple(sorted(positive_indices)):(https://github.com/huggingface/transformers/blame/main/src/transformers/utils/backbone_utils.py#L64) is the problem andsortedshould be replaced by human/natural sorting.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_251.txt:\n",
      "Title: gemma2 + flash atten Error: RuntimeError: linalg.vector_norm: Expected a floating point or complex tensor as input. Got Long\n",
      "URL: https://github.com/huggingface/transformers/issues/32103\n",
      "Body:\n",
      "System Infotransformers: 4.43.0.dev0torch: 2.3.0deepspeed: 0.14.0flash atten: 2.6.1Due to the latest flash attention supporting logits soft capping, I have added the extension of the PI length of ROPE for gemma2 by referring to llama_model.py. I used the latest transformers version 4.43.0.dev0 + flash attention 2.6.1 to fine-tune gemma2, as I had previously fine-tuned gemma2 using transformers 4.42.3 (which used eager attention) and the inference effect was good. I thought the process would be smooth this time, but many problems have arisen during the use of this new package:Tip: My sample lengths are generally around 11k - 30k.After loading gemma2, if the number of processes >= 2 when tokenizing the datasets, the progress will get stuck, while if gemma2 is not loaded, the datasets can execute normally, and even with a single process it can execute normally.Using deepspeed zero2 with flash attention enabled, it will report the error: RuntimeError: linalg.vector_norm: Expected a floating point or complex tensor as input. Got Long. After checking the official explanation, it seems that there are abnormal gradients, which is consistent with the third point below.Using deepspeed zero3 with flash attention enabled, the program will not report an error, but the initial loss value is over 2000.With all the above parameters unchanged, but only disabling flash attention and using eager attention, the code executes normally, and the initial loss is similar to my previous fine-tuning, starting to decrease from around 2.In summary, it seems that there are some issues with the current integration of flash attention and Hugging Face. Personally, I think the extension of the PI length of ROPE is already stored in the query and key before the attention, so it should not be the cause of the impact. I hope Hugging Face can review whether these errors exist, and I greatly appreciate your work.Who can help?@ArthurZucker@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRuntimeError: linalg.vector_norm: Expected a floating point or complex tensor as input. Got Long[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\", line 1938,intrain\n",
      "[rank0]:returninner_training_loop(\n",
      "[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\", line 2274,in_inner_training_loop\n",
      "[rank0]:     tr_loss_step = self.training_step(model, inputs)\n",
      "[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/trainer.py\", line 3344,intraining_step\n",
      "[rank0]:     self.accelerator.backward(loss,**kwargs)\n",
      "[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2126,inbackward\n",
      "[rank0]:     self.deepspeed_engine_wrapped.backward(loss,**kwargs)\n",
      "[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 175,inbackward\n",
      "[rank0]:self.engine.step()\n",
      "[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2169,instep\n",
      "[rank0]:     self._take_model_step(lr_kwargs)\n",
      "[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2075,in_take_model_step\n",
      "[rank0]:self.optimizer.step()\n",
      "[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1842,instep\n",
      "[rank0]:     scaled_global_grad_norm =self.scaled_global_norm()\n",
      "[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1789,inscaled_global_norm\n",
      "[rank0]:returntorch.norm(torch.stack(norm_groups), p=norm_type)\n",
      "[rank0]:   File\"/root/miniconda3/envs/llm/lib/python3.10/site-packages/torch/functional.py\", line 1631,innorm\n",
      "[rank0]:returntorch.linalg.vector_norm(input, _p, _dim, keepdim, dtype=dtype)\n",
      "[rank0]: RuntimeError: linalg.vector_norm: Expected a floating point or complex tensor as input. Got LongExpected behavior\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_279.txt:\n",
      "Title: Add support for XTR\n",
      "URL: https://github.com/huggingface/transformers/issues/31873\n",
      "Body:\n",
      "Model descriptionXTR (ConteXtualized Token Retriever) is a multi-vector retrieval model that improves efficiency by focusing on retrieving and ranking the most important document tokens. Details are described athttps://arxiv.org/abs/2304.01982.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationImplementation for PyTorch:https://github.com/mjeensung/xtr-pytorchWeight:https://huggingface.co/google/xtr-base-enAuthor:@jhyuklee\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_286.txt:\n",
      "Title: Add support for GGUF Phi-3\n",
      "URL: https://github.com/huggingface/transformers/issues/31826\n",
      "Body:\n",
      "Model descriptionModel is Phi-3 saved as a GGUF.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationIt would be the analog of this recent PR but for Phi 3:e462843#diff-55e3debce4586d986257d1b1acb48c8708504f4b3b63e413e89f27ac2cc738b3\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_292.txt:\n",
      "Title: TinyModel addition\n",
      "URL: https://github.com/huggingface/transformers/issues/31804\n",
      "Body:\n",
      "Model descriptionhttps://github.com/noanabeshima/tiny_modelIt's a small language model trained on TinyStories for interpretability with sparse autoencoders and transcoders added. It has no layernorms (this helps with interpretability) which makes it not fit with any existing model architecture in the transformers library. Its architecture is essentially GPT-2's except that it doesn't have layernorms and it has untied embed/deembed.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationThe implementation is here:https://github.com/noanabeshima/tiny_model/blob/main/tiny_model/lm.pyThe weights are here:https://huggingface.co/noanabeshima/tiny_model/blob/main/tiny_model.ptThe default config corresponding to the weights is:d_model=768,\n",
      "    n_layers=4,\n",
      "    n_heads=16,\n",
      "    max_seq_len=256,\n",
      "    vocab_size=10_000I am the author.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_331.txt:\n",
      "Title: How do I replace a spare tokens?\n",
      "URL: https://github.com/huggingface/transformers/issues/31475\n",
      "Body:\n",
      "System InfoI want to SFT Mistral-v0.3 with my own chat template.So I followedthis commentand replaced some [controal_n] tokens with special tokens for the chat template.However, the new vocabulary was actually added and the size of the vocabulary increased.Is there any way to replace the vocabulary?Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductiontokenizer.json{\n",
      "  \"version\": \"1.0\",\n",
      "  \"truncation\": null,\n",
      "  \"padding\": null,\n",
      "  \"added_tokens\": [\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "{\n",
      "      \"id\": 10,\n",
      "      \"content\": \"<|system|>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 11,\n",
      "      \"content\": \"<|user|>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 12,\n",
      "      \"content\": \"<|assistant|>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "    {\n",
      "      \"id\": 13,\n",
      "      \"content\": \"<|eot|>\",\n",
      "      \"single_word\": false,\n",
      "      \"lstrip\": false,\n",
      "      \"rstrip\": false,\n",
      "      \"normalized\": false,\n",
      "      \"special\": true\n",
      "    },\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~tokenizer_config.json{\n",
      "  \"add_bos_token\": true,\n",
      "  \"add_eos_token\": false,\n",
      "  \"add_prefix_space\": true,\n",
      "  \"added_tokens_decoder\": {\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    \"10\": {\n",
      "          \"content\": \"<|system|>\",\n",
      "          \"lstrip\": false,\n",
      "          \"normalized\": false,\n",
      "          \"rstrip\": false,\n",
      "          \"single_word\": false,\n",
      "          \"special\": true\n",
      "        },\n",
      "        \"11\": {\n",
      "          \"content\": \"<|user|>\",\n",
      "          \"lstrip\": false,\n",
      "          \"normalized\": false,\n",
      "          \"rstrip\": false,\n",
      "          \"single_word\": false,\n",
      "          \"special\": true\n",
      "        },\n",
      "        \"12\": {\n",
      "          \"content\": \"<|assistant|>\",\n",
      "          \"lstrip\": false,\n",
      "          \"normalized\": false,\n",
      "          \"rstrip\": false,\n",
      "          \"single_word\": false,\n",
      "          \"special\": true\n",
      "        },\n",
      "        \"13\": {\n",
      "          \"content\": \"<|eot|>\",\n",
      "          \"lstrip\": false,\n",
      "          \"normalized\": false,\n",
      "          \"rstrip\": false,\n",
      "          \"single_word\": false,\n",
      "          \"special\": true\n",
      "        },\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "}test codetokenizer =  AutoTokenizer.from_pretrained(model_dir)\n",
      "pprint(tokenizer.added_tokens_decoder)output~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      " 768: AddedToken(\"[control_766]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      " 769: AddedToken(\"[control_767]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      " 770: AddedToken(\"[control_768]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      " 32768: AddedToken(\"<|system|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      " 32769: AddedToken(\"<|user|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      " 32770: AddedToken(\"<|assistant|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      " 32771: AddedToken(\"<|eot|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}Expected behavior[control_n] Tokens can be replaced with any token.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_325.txt:\n",
      "Title: NotImplementedError: Cannot copy out of meta tensor; no data when embedding to meta\n",
      "URL: https://github.com/huggingface/transformers/issues/31560\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.0Platform: Linux-5.4.0-81-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.23.4Safetensors version: 0.4.2Accelerate version: 0.31.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.1.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionTo sendquery_position_embeddingsto meta, modify it as follows:max_memory = {0: max_size // 2, \"cpu\": max_size * 2}https://github.com/huggingface/transformers/blob/main/tests/test_modeling_common.py#L30852Runpython3 -m pytest tests/models/deformable_detr/test_modeling_deformable_detr.py::DeformableDetrModelTest::test_disk_offload_safetensorsExpected behaviorFAILED tests/models/deformable_detr/test_modeling_deformable_detr.py::DeformableDetrModelTest::test_disk_offload_safetensors - NotImplementedError: Cannot copy out of meta tensor; no data!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_94.txt:\n",
      "Title: SupportStaticCachein assisted generation\n",
      "URL: https://github.com/huggingface/transformers/issues/32946\n",
      "Body:\n",
      "Looking for contributions!Assisted generation (or speculative decoding) is a strategy to speed up generation. UsingStaticCacheandtorch.compileis another strategy to speed up generation. Currently, the two are not compatible. It would be nice to be able to use both at the same time, for maximum speed 😎In a nutshell, assisted generation has to clear the cache of the models for the tokens that were rejected.StaticCachedoesn't have the functions to do it implemented.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_319.txt:\n",
      "Title: Please reopen issue #30361\n",
      "URL: https://github.com/huggingface/transformers/issues/31635\n",
      "Body:\n",
      "System Infotransformers: 4.39.3python: 3.12system: linuxWho can help?@muellerz@SunMarc@ganteInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSteps outlinehereDid some further investigation, created custom trainer from subclassingSeq2SeqTrainerand directly calledmodel.generatecustom_generation_kwargs = {\n",
      "    \"max_new_tokens\": 150,\n",
      "}\n",
      "\n",
      "from transformers import Seq2SeqTrainer\n",
      "\n",
      "def compute_metrics(eval_preds):\n",
      "    predictions, labels = eval_preds\n",
      "    print(f\"Predictions.shape = {predictions.shape}, Labels.shape = {labels.shape}\")\n",
      "    if isinstance(predictions, tuple):\n",
      "        predictions = predictions[0]\n",
      "    if predictions.shape[1] != labels.shape[1]:\n",
      "        predictions = predictions[..., 1:]\n",
      "        #labels  = labels[..., :-1]\n",
      "    predictions = np.where(predictions == -100, tokenizer.pad_token_id, predictions)\n",
      "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
      "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
      "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
      "    acc = utils.accuracy(preds=predictions, labels=labels)\n",
      "    return {\"accuracy\": acc}\n",
      "\n",
      "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
      "    def __init__(self, *args, custom_generation_kwargs=None, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "        self.custom_generation_kwargs = custom_generation_kwargs or {}\n",
      "    \n",
      "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
      "        if not self.args.predict_with_generate or prediction_loss_only:\n",
      "            return super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n",
      "\n",
      "        # Generate predictions\n",
      "        print(f\"Name of self.model.main_input_name = {self.model.main_input_name}\")\n",
      "        generation_inputs = inputs[self.model.main_input_name]\n",
      "        generated_tokens = self.model.generate(\n",
      "            generation_inputs,\n",
      "            **custom_generation_kwargs\n",
      "        )\n",
      "        print(f\"Generated tokens {generated_tokens.shape}\")\n",
      "\n",
      "        # If the model has past_key_values, it will also return them, but we don't need them here\n",
      "        if self.args.prediction_loss_only:\n",
      "            return (None, generated_tokens, None)\n",
      "\n",
      "        # Compute loss\n",
      "        with torch.no_grad():\n",
      "            outputs = model(**inputs)\n",
      "            print(f\"Logits.shape = {outputs.logits.shape}\")\n",
      "            if self.label_smoother is not None:\n",
      "                loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n",
      "            else:\n",
      "                loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
      "\n",
      "        return (loss, generated_tokens, inputs[\"labels\"])\n",
      "\n",
      "# Initialize the trainer with custom generation parameters\n",
      "trainer = CustomSeq2SeqTrainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=tokenized_datasets.select(range(100)),\n",
      "    eval_dataset=tokenized_datasets.select(range(1000)),\n",
      "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
      "    tokenizer=tokenizer,\n",
      "    compute_metrics=compute_metrics\n",
      ")\n",
      "\n",
      "trainer.train()\n",
      "eval_results = trainer.evaluate()\n",
      "print(eval_results)What we observe is that the number of generated tokens fluctuate on every batch:Generated tokens torch.Size([32, 104])\n",
      "Logits.shape = torch.Size([32, 150, 50265])\n",
      "Generated tokens torch.Size([32, 124])\n",
      "Logits.shape = torch.Size([32, 150, 50265])\n",
      "Generated tokens torch.Size([32, 115])\n",
      "Logits.shape = torch.Size([32, 150, 50265])\n",
      "Generated tokens torch.Size([32, 129])\n",
      "Logits.shape = torch.Size([32, 150, 50265])\n",
      "Generated tokens torch.Size([32, 122])\n",
      "Logits.shape = torch.Size([32, 150, 50265])Expected behaviorTo work as expected and generated tokens to adhere to eithermax_new_tokensormax_length.EditThis only works if you callmodel.generate(inputs, generation_config=gen_conf_obj)withGenerationConfigobject but seems tonot workif you callmodel.generate(inputs, max_new_tokens=128), it defaults to generating tokens of context_size=20, which contradicts the examples shown intext-generation.Stated as:Customize text generation\n",
      "You can override any generation_config by passing the parameters and their values directly to the generate method:\n",
      "\n",
      "model.generate(**inputs, num_beams=4, do_sample=True)\n",
      "outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)Finally, this seems to be working withSeq2SeqTrainingArgumentswhen passing ageneration_configobject but how exactly do we get the default values frommodel.generation_config. For instance, we have X,Y,Z models each with differentmodel.generation_configI want to get those defaults and update the ones I'm interested?Doingmodel.generate_config.to_dict()brings everything and it's not working.Also, getting those params that are only set to some value still not working as intended.generation_config = model.generation_config\n",
      "set_params = {k: getattr(generation_config, k) for k in dir(generation_config)\n",
      "              if not k.startswith('_') and getattr(generation_config, k) is not None}\n",
      "\n",
      "generation_config = GenerationConfig(**set_params)Themodel.generation_configshould have an easy to access method that returns the default values per model?GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 1\n",
      "}Is there a way to access those default values, e..g,model.generation_config.get_defaults().to_dict()so that I can instantiate aGenerationConfigfrom those and add or update the ones I need based on the specific task. Did it by simply parsing the output but should have a proper way to access those?Edit 2Passing aGenerationConfigobject in theSeq2SeqTrainingArgumentsiscompletely ignoredfor some models (e.g., bart-base, flan-t5-small, etc), interesting is the fact that passinggen_conf = GenerationConfig(max_new_tokens=128)directly tomodel.generate(inputs, generation_config=gen_conf)(for CustomSeq2SeqTrainer) is also being ignored, same formodel.generate(max_length=128)as well asmodel.generate(max_new_tokens=128), in the case ofbart-base.Please provide a work around in this case. We need a modular and robust solution across different models.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_480.txt:\n",
      "Title: Supporting Selective Activation Checkpointing and CPU Offloading Option.\n",
      "URL: https://github.com/huggingface/transformers/issues/29648\n",
      "Body:\n",
      "Feature requestreferencesdeepspeed docsMotivationHi, first of all, thank you for your great contribution for open-source community, HF team !I really enjoy training with HF transformers but found that activation checkpointing (a.k.a gradient checkpointing) is applied for every layers and checkpoints activation in GPU memory where it can easily leads to GPU OOM.However, there are well-known techniques to checkpoint activations selectively and even in CPU.Implementations of these features can be found in Megatron and DeepSpeed, and torch team also releasethe blogabout this technique today.So i tested selective activation checkpointing with CPU offloading by modifying HF model (in my case, gpt-2 xl).i just changed two things,model._gradient_checkpointing_funcand forward pass like below.fromtransformersimportAutoConfig,AutoModelForCausalLMfromdeepspeed.runtime.activation_checkpointingimportcheckpointing# init model and activate grad ckptmodel=AutoModelForCausalLM(**kwargs)model.gradient_checkpointing_enable()# change grad ckpt functioncheckpointing.configure(mpu_=None,deepspeed_config=None,partition_activations=False,contiguous_checkpointing=False,num_checkpoints=num_checkpoints,# only use these for nowcheckpoint_in_cpu=True,# only use these for nowsynchronize=False,profile=True,\n",
      ")model._gradient_checkpointing_func=checkpointing.checkpoint# use deepspeed activation checkpointing functiondefforward(**inputs):\n",
      "    ...defcustom(start,end):defcustom_forward(*inputs):layers_=self.h[start:end]x_=inputs[0]# hidden_stateshead_mask_=inputs[3]# hidden_statesfori,layerinenumerate(layers_):x_=layer(x_,None,inputs[2],head_mask_[i],inputs[4],inputs[5],False,None)[0]# tuple to tensor# Tra()returnx_returncustom_forwardifself.gradient_checkpointingandself.training:# print('use_deepspeed_activation_checkpointing is True !')# Tra()l=0total_num_layers=len(self.h)chunk_length=num_layers## how many checkpoint do you want?# print(f'chunk_length : {chunk_length}')whilel<total_num_layers:ifself.model_parallel:raiseNotImplementedError(\"Idc model parallel :)\")ifoutput_hidden_states:raiseNotImplementedError(\"Idc layerwise output :)\")hidden_states=self._gradient_checkpointing_func(custom(l,l+chunk_length),hidden_states,None,attention_mask,head_mask[l:l+chunk_length],encoder_hidden_states,encoder_attention_mask,False,None,\n",
      "            )ifoutput_attentions:raiseNotImplementedError(\"We use efficient SDPA and it does not support to output attention score map\")ifself.model_parallel:raiseNotImplementedError(\"Idc model parallel :)\")l+=chunk_lengthand i could see significant improvement in GPU memory (trading off throughput).The quantitative results (CPU/GPU memory and elapsed time for 3 times forwarding) are recorded by torch profiler and figure was generated by GPT-4.My setting was like this.2x A100 80GBgpt2-xl (near 1.5B)fixed batch size with same random seed ([128, 512] # B, T)zero-3 without CPU offloading (using accelerate)flash attn 2 is applied (monkey patched with xformers)Results show that it could save about 10GB for each GPU device.The reason why reserved GPU memory for checkpointing every 2, 3, 6 layers are same is that peak memory is dominated by computing loss i guess.here is my detailed result for checkpointing every 3 layers with CPU offloading,and HF default version (using torch.utils.checkpoint for every layers)i didn't add profiling result for larger model but there were more improvement and i can find sweet spot for GPU memory VS throughput trade off.Your contributionDidnt PR yet because in HF transformers, every model has its own forward method and applying it for all would be a huge update.i just add profiling results and naive code snippet for implementation.just want to discuss with you guys !\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_43.txt:\n",
      "Title: RobertaTokenizer has incorrect character offsets\n",
      "URL: https://github.com/huggingface/transformers/issues/33237\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.1Platform: macOS-14.6.1-arm64-arm-64bitPython version: 3.9.19Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.3.1 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: NoUsing distributed or parallel set-up in script?: NoWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI have byte offsets for a tagging task which I am trying to train a transformer with. Using theSalesforce/codet5-basetokenizer (which uses RobertaTokenizer) I tokenize the input and then callchar_to_tokento find out which tokens my byte offset lands in so I can build the label tensor. For some inputschar_to_tokenreturnsNone, even when the character range in question appears in a token.Running the following code:test_string=\"\"\"(\"    \" +\"\"\"tok=AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\",local_files_only=True)enc=tok(text=test_string,truncation=False,add_special_tokens=False)char_mapping=[]foriinrange(len(enc[\"input_ids\"])):char_mapping.append((i,enc.token_to_chars(i),tok.decode(enc[\"input_ids\"][i])))print(f\"\\nchar_mapping ={char_mapping}\")tok_mapping=[]foriinrange(len(test_string)):tok_mapping.append((i,enc.char_to_token(i)))print(f\"\\ntok_mapping ={tok_mapping}\")produces this output:char_mapping=[(0,CharSpan(start=0,end=2),'(\"'), \n",
      "                (1,CharSpan(start=5,end=5),'   '), \n",
      "                (2,CharSpan(start=6,end=7),' \"'), \n",
      "                (3,CharSpan(start=8,end=9),' +')]tok_mapping=[(0,0), (1,0), (2,None), (3,None), (4,None), (5,None), (6,2), (7,None), (8,3)]Note that token 1 has a zero length character span despite it containing 3 space characters. The correspondingchar_to_tokencall for those characters producesNone. Also tokens 2 and 3 each contain 2 characters, but have single character spans, and again the correspondingchar_to_tokencall for the start characters of those tokens returnsNone.Part of my tagging task involves tagging string literals with a \"string literal\" label, but whenever the string contains whitespace I hit theseNonevalues and I can't find the correct token to tag.Expected behaviorchar_to_tokenshould return the correct token id when the token contains the specified character offset.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_133.txt:\n",
      "Title: class Cache must not be a subclass oftorch.nn.Module\n",
      "URL: https://github.com/huggingface/transformers/issues/32681\n",
      "Body:\n",
      "System InfoI'm usingtransformers==4.44.0.The script that I used for collecting my system info is as follows:$ curl -OL https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/collect_env.py\n",
      "$ python3 collect_env.pyThe collected system info is as follows:Collecting environment information...\n",
      "PyTorch version: 2.4.0\n",
      "Is debug build: False\n",
      "CUDA used to build PyTorch: 12.1\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 20.04.6 LTS (x86_64)\n",
      "GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "Clang version: Could not collect\n",
      "CMake version: version 3.26.4\n",
      "Libc version: glibc-2.31\n",
      "\n",
      "Python version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\n",
      "Python platform: Linux-5.15.0-102-generic-x86_64-with-glibc2.31\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: 10.1.243\n",
      "CUDA_MODULE_LOADING set to: LAZY\n",
      "GPU models and configuration: \n",
      "GPU 0: NVIDIA RTX A6000\n",
      "GPU 1: NVIDIA RTX A6000\n",
      "\n",
      "Nvidia driver version: 535.171.04\n",
      "cuDNN version: Could not collect\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "Is XNNPACK available: True\n",
      "\n",
      "CPU:\n",
      "Architecture:                       x86_64\n",
      "CPU op-mode(s):                     32-bit, 64-bit\n",
      "Byte Order:                         Little Endian\n",
      "Address sizes:                      43 bits physical, 48 bits virtual\n",
      "CPU(s):                             32\n",
      "On-line CPU(s) list:                0-31\n",
      "Thread(s) per core:                 2\n",
      "Core(s) per socket:                 16\n",
      "Socket(s):                          1\n",
      "NUMA node(s):                       1\n",
      "Vendor ID:                          AuthenticAMD\n",
      "CPU family:                         23\n",
      "Model:                              49\n",
      "Model name:                         AMD Ryzen Threadripper PRO 3955WX 16-Cores\n",
      "Stepping:                           0\n",
      "Frequency boost:                    enabled\n",
      "CPU MHz:                            2082.992\n",
      "CPU max MHz:                        4402.7339\n",
      "CPU min MHz:                        2200.0000\n",
      "BogoMIPS:                           7785.71\n",
      "Virtualization:                     AMD-V\n",
      "L1d cache:                          512 KiB\n",
      "L1i cache:                          512 KiB\n",
      "L2 cache:                           8 MiB\n",
      "L3 cache:                           64 MiB\n",
      "NUMA node0 CPU(s):                  0-31\n",
      "Vulnerability Gather data sampling: Not affected\n",
      "Vulnerability Itlb multihit:        Not affected\n",
      "Vulnerability L1tf:                 Not affected\n",
      "Vulnerability Mds:                  Not affected\n",
      "Vulnerability Meltdown:             Not affected\n",
      "Vulnerability Mmio stale data:      Not affected\n",
      "Vulnerability Retbleed:             Mitigation; untrained return thunk; SMT enabled with STIBP protection\n",
      "Vulnerability Spec rstack overflow: Mitigation; safe RET\n",
      "Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\n",
      "Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
      "Vulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\n",
      "Vulnerability Srbds:                Not affected\n",
      "Vulnerability Tsx async abort:      Not affected\n",
      "Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] mypy==1.11.1\n",
      "[pip3] mypy-extensions==1.0.0\n",
      "[pip3] numpy==1.26.4\n",
      "[pip3] pytorchvideo==0.1.5\n",
      "[pip3] torch==2.4.0\n",
      "[pip3] torchvision==0.19.0\n",
      "[pip3] triton==3.0.0\n",
      "[conda] Could not collectWho can help?People who have been involved in#32168@gante@guangy10@amyeroberts@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSteps to reproduceCreate an environment with python 3.10 with pytorch-2.4.0 and transformers-4.44.0 installed.For example, if you don't mind using conda:conda create -n repro python=3.10\n",
      "conda activate repro\n",
      "conda install pytorch pytorch-cuda=12.4 -c pytorch -c nvidia\n",
      "pip install transformers==4.44.0Copy and paste the example code under theTorch export for static cachesection inthe transformers v4.44.0 release page. (Say this code is saved asexample.pyin your working directory.) The code is as follows:importos,torch,copyfromtransformersimportAutoModelForCausalLM,AutoTokenizer,DynamicCachedevice=\"cuda\"ckpt=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"INITIAL_PROMPT=\"From now on, you are going to answer all my questions with historical details. Make sure to always add a bit of french here and there, for style.\"model=AutoModelForCausalLM.from_pretrained(ckpt,torch_dtype=torch.float16)model.to(device)tokenizer=AutoTokenizer.from_pretrained(ckpt)prompt_cache=DynamicCache()inputs=tokenizer(INITIAL_PROMPT,return_tensors=\"pt\").to(\"cuda\")prompt_cache=model(**inputs,past_key_values=prompt_cache).past_key_valuesprompt=\"Why are french people obsessed with french?\"new_inputs=tokenizer(INITIAL_PROMPT+prompt,return_tensors=\"pt\").to(\"cuda\")past_key_values=copy.deepcopy(prompt_cache)outputs=model.generate(**new_inputs,past_key_values=past_key_values,max_new_tokens=20)response=tokenizer.batch_decode(outputs)[0]print(response)prompt=\"What is the best city to swim in?\"new_inputs=tokenizer(INITIAL_PROMPT+prompt,return_tensors=\"pt\").to(\"cuda\")outputs=model.generate(**new_inputs,past_key_values=copy.deepcopy(prompt_cache),max_new_tokens=20)response=tokenizer.batch_decode(outputs)[0]Run the code:python example.pyThe error messageYou'll be able to see the error message, basically complaining about that the example code is callingcopy.deepcopyon thetorch.nn.Moduleinstanceprompt_cache.Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 12.45it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/path/to/your/working/directory/example.py\", line 18, in <module>\n",
      "    past_key_values = copy.deepcopy(prompt_cache)\n",
      "  File \"/path/to/your/repro/lib/python3.10/copy.py\", line 172, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"/path/to/your/repro/lib/python3.10/copy.py\", line 271, in _reconstruct\n",
      "    state = deepcopy(state, memo)\n",
      "  File \"/path/to/your/repro/lib/python3.10/copy.py\", line 146, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"/path/to/your/repro/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"/path/to/your/repro/lib/python3.10/copy.py\", line 146, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"/path/to/your/repro/lib/python3.10/copy.py\", line 206, in _deepcopy_list\n",
      "    append(deepcopy(a, memo))\n",
      "  File \"/path/to/your/repro/lib/python3.10/copy.py\", line 153, in deepcopy\n",
      "    y = copier(memo)\n",
      "  File \"/path/to/your/repro/lib/python3.10/site-packages/torch/_tensor.py\", line 87, in __deepcopy__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Only Tensors created explicitly by the user (graph leaves) support the deepcopy protocol at the moment.  If you were attempting to deepcopy a module, this may be because of a torch.nn.utils.weight_norm usage, see https:/github.com/pytorch/pytorch/pull/103001CommentsThis is due to the change made in#32168, where the classCachehas become a subclass oftorch.nn.Module. - See thecommentthat I wrote in this PR.Considering that the classCache(and its subclasses, such asDynamicCache) represents KV cache generated by a model (which is atorch.nn.Moduleobject itself), it is not natural to defineCacheas a subclass oftorch.nn.Module.It looks like the purpose was to enablecopy.deepcopyforCacheobjects, but apparently, PyTorch 2.4 won't allow it.Expected behaviorThe example code from the release page runs without the error, demonstrating support for prompt reuse introduced in transformers-4.44.0.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_655.txt:\n",
      "Title: Support reverse_prompt\n",
      "URL: https://github.com/huggingface/transformers/issues/26862\n",
      "Body:\n",
      "Feature requestAdd a simple option to provide a stopping criteria in the generate method. This option could also be a stopping criteria.MotivationLlama.cpp supports an option--reverse-prompt. It is very useful for chat models to stop at words like \"USER:\".Your contributionThis is how I implemented it, but it is not ideal:def count_stop_words(text, stop_words):\n",
      "    return Counter({expr: text.count(expr) for expr in stop_words})\n",
      "\n",
      "\n",
      "class MyStoppingCriteria(StoppingCriteria):\n",
      "    def __init__(self, stop_counter):\n",
      "        self.stop_counter = stop_counter\n",
      "\n",
      "    def __call__(self, input_ids, scores, **kwargs):\n",
      "        # Get the generated text as a string\n",
      "        generated_text = tokenizer.decode(input_ids[0])\n",
      "        counter = count_stop_words(generated_text, self.stop_counter)\n",
      "        if counter - self.stop_counter:\n",
      "            return True\n",
      "        return FalseThen you can use this parameter:stopping_criteria=StoppingCriteriaList(\n",
      "                [\n",
      "                    MyStoppingCriteria(\n",
      "                        count_stop_words(PROMPT, [\"ASSISTANT:\", \"USER:\", \"SYSTEM:\"])\n",
      "                    )\n",
      "                ]\n",
      "            ),\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_899.txt:\n",
      "Title: [activations] pytorch-1.11+ Tanh Gelu Approximation\n",
      "URL: https://github.com/huggingface/transformers/issues/15397\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_694.txt:\n",
      "Title: Batch Decoding of LMs will cause different outputs with different batch size\n",
      "URL: https://github.com/huggingface/transformers/issues/25921\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_680.txt:\n",
      "Title: Registering Models in MLflow Callback\n",
      "URL: https://github.com/huggingface/transformers/issues/26319\n",
      "Body:\n",
      "Feature requestWe can add registering models functionality to MLflow callback so that we can use MLflow model registry with 🤗 models. To do that we can introduce an optional _registered_model_name field to the callback, and can register the model in case these field is not None.class MLflowCallback(TrainerCallback):\n",
      "    #...\n",
      "    def setup(self, args, state, model):\n",
      "        #...\n",
      "        self._log_artifacts = os.getenv(\"HF_MLFLOW_LOG_ARTIFACTS\", \"FALSE\").upper() in ENV_VARS_TRUE_VALUES\n",
      "        self._registered_model_name = os.getenv(\"HF_REGISTERED_MODEL_NAME\") # Suggested Change\n",
      "        #...\n",
      "        \n",
      "    def on_save(self, args, state, control, **kwargs):\n",
      "        if self._initialized and state.is_world_process_zero and self._log_artifacts:\n",
      "          ckpt_dir = f\"checkpoint-{state.global_step}\"\n",
      "          artifact_path = os.path.join(args.output_dir, ckpt_dir)\n",
      "          logger.info(f\"Logging checkpoint artifacts in {ckpt_dir}. This may take time.\")\n",
      "          self._ml_flow.pyfunc.log_model(\n",
      "              ckpt_dir,\n",
      "              artifacts={\"model_path\": artifact_path},\n",
      "              python_model=self._ml_flow.pyfunc.PythonModel(),\n",
      "              registered_model_name=self._registered_model_name or None  # Suggested Change\n",
      "          )MotivationModel registry is one of the most useful features of MLflow, but current callback doesn't support it. It forces users to make a custom implementation to use this functionality. Instead, we can extend 🤗 MLflow callback to provide this feature.Your contributionI can implement this extension and create a PR.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_858.txt:\n",
      "Title: An example for finetuning FLAVA or any VLP multimodel using trainer (for example for classification)\n",
      "URL: https://github.com/huggingface/transformers/issues/18066\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_131.txt:\n",
      "Title: tracker: moveprepare_inputs_for_generationinto the generation mixin 🧹\n",
      "URL: https://github.com/huggingface/transformers/issues/32685\n",
      "Body:\n",
      "🧹 This is a tracker regarding the move ofprepare_inputs_for_generationinto the generation mixin 🧹Why?prepare_inputs_for_generationis not part of the core modeling, but rather a utility forgenerateit should greatly reduce the need to touch modeling code, ongeneratechanges. Fewer modeling changes -> improved model stabilitygreatly reduced number of lines of code 🙏TrackerOrdered list of tasks:Fix related slow tests before we start — allllama,generate, andcache_utils[except sink cache, broken atm] slow tests should be passing to ensure we don’t break anything (Llama: make slow tests green 🟢#33138)PreTrainedModeldoesn't inherit fromGenerationMixin, so thatcan_generate()becomes independent ofprepare_inputs_for_generationbeing overwritten or not (Generation: deprecatePreTrainedModelinheriting fromGenerationMixin#33203)Move llama’sprepare_inputs_for_generationto the generation mixin. This implies moving one function that prepares the 4D mask too (the one that is called there)Add tests for the generalistprepare_inputs_for_generation— currently we don’t test it directly, and we shouldAddress the case ofsynced_gpusingenerate: whensynced_gpusandcache_positionsis out of bounds, take the latest availableinput_idsfor dummy computations (seeFix synced GPUs#33252; should fixMulti GPU generate with llama shape error#32885,Shape mismatch when generating with multiple processes#32603, andBugfix for generation with an early-stopping process#32641)Deleteprepare_inputs_for_generationfrom as many models as possible. There may be merge conflicts here, due to the 4D mask function. Try to iron out as many trivial cases as possibleChangeprepare_inputs_for_generationto forward**kwargsfrom its input to its output. With minimal changes, this should enable most VLMs to use the shared function (they forwardpixel_valuesfrom the input to the output)By this point most cases ofprepare_inputs_for_generationshould be removed 🤗  We would need to check the others individually, there may be further simplification patterns available!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_125.txt:\n",
      "Title: Difference in embedding weight initialization for randomly initialized T5 model\n",
      "URL: https://github.com/huggingface/transformers/issues/32854\n",
      "Body:\n",
      "System InfotransformersWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe problem is technical, so I will describe it here. I believe the idea is to keep the weight initialization the same for pytorch or tf models initialized from scratch. However, this is different.Inhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py#L821the embedding weights are initialized with a variance of 1. However, in tf, this is done by initializing with a standard deviation of 0.05.https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1635https://www.tensorflow.org/api_docs/python/tf/random_normal_initializerAccording to the docs, it's default initialized with these arguments:tf.random_normal_initializer(\n",
      "    mean=0.0, stddev=0.05, seed=None\n",
      ")PyTorch initialization:# Mesh TensorFlow embeddings initialization\n",
      "            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n",
      "            module.shared.weight.data.normal_(mean=0.0, std=factor * 1.0)TF initialization:def embedding_weights(mesh,\n",
      "                      vocab_dim,\n",
      "                      output_dim,\n",
      "                      variable_dtype,\n",
      "                      name=\"embedding\",\n",
      "                      ensemble_dim=None,\n",
      "                      initializer=None):\n",
      "  \"\"\"Embedding weights.\"\"\"\n",
      "  shape = mtf.Shape(\n",
      "      [ensemble_dim] if ensemble_dim else []) + [vocab_dim, output_dim]\n",
      "  if initializer is None:\n",
      "    initializer = tf.random_normal_initializer()\n",
      "  ret = mtf.get_variable(\n",
      "      mesh, name, shape, dtype=variable_dtype, initializer=initializer)\n",
      "  return retThis is already mentioned in#16749but since#16749mentions 2 issues this first one seems to have gone unnoticed, so I am opening a separate issue for this one.Expected behaviorI expect the initialization to be the same across TF and PyTorch T5 models.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_643.txt:\n",
      "Title: transformers.utils.fx feature support for passes.shape_prop.ShapeProp(graph)\n",
      "URL: https://github.com/huggingface/transformers/issues/27169\n",
      "Body:\n",
      "System InfoUbuntu x86_64.Hi Dev community,I want to get intermediate tensor shape llm for given input using fx graph.In torch fx, we haveout = fx.passes.shape_prop.ShapeProp(graph)out.propogate(sample_input).shapeIs there similar feature capability for transformers.utils.fx ?Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI'm getting error:Using symbolic_trace graph with fx.passes.passes.shape_prop.ShapeProp I'm gettingraise RuntimeError(RuntimeError: ShapeProp error for: node=%decoder_input_ids : torch.Tensor [num_users=2] = placeholder[target=decoder_input_ids] with meta={}Expected behaviorRun smoothly for given fx graph , input.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_55.txt:\n",
      "Title: Model saving (via.save_pretrainedor.push_to_hub) produces inconsistent shard sizes when some weights are offloaded\n",
      "URL: https://github.com/huggingface/transformers/issues/33209\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Linux-6.1.85+-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.23.5Safetensors version: 0.4.4Accelerate version: 0.32.1Accelerate config: \tnot foundPyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): 2.17.0 (True)Flax version (CPU?/GPU?/TPU?): 0.8.4 (gpu)Jax version: 0.4.26JaxLib version: 0.4.26Using distributed or parallel set-up in script?: noUsing GPU in script?: yesGPU type: NVIDIA A100-SXM4-40GBWho can help?@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionNote: Tested on a A100 GPU w/ 40GB VRAMAfter loading a large model (e.g., via.from_pretrained) withdevice_map='auto'such that certain parts need to be offloaded to CPU, any following calls to serialize the model (e.g.,.save_pretrainedor.push_to_hub) result in a model with result inn-1correct shards, followed by 1 shard of the remaining weights.Takehttps://huggingface.co/google/gemma-2-27b-itfor example, if running# pip install acceleratefromtransformersimportAutoModelForCausalLMimporttorchmodel=AutoModelForCausalLM.from_pretrained(\"google/gemma-2-27b-it\",device_map=\"auto\",torch_dtype=torch.bfloat16,\n",
      ")model.save_pretrained('output')it will produce 8 shards instead of the expected 12: The first 7 are of size ~5GB and the last is ~20GB. Also note that 7 * 5 = 35 < 40 (VRAM), meaning the first few were on the GPU when the model was serialized.Expected behaviorAll shards should be < MAX_SHARD_SIZE (defaults to 5GB)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_41.txt:\n",
      "Title: when l use the model generate method within @tf.function, it encounterd a mistake\n",
      "URL: https://github.com/huggingface/transformers/issues/33241\n",
      "Body:\n",
      "System Infotransformers version: 4.43.3Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.17Python version: 3.8.19Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): not installed (NA)Tensorflow version (GPU?): 2.7.0 (True)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: Yes, using TensorFlow MirroredStrategy for distributed training.Who can help?@ArthurZuckerthe original issues is #33329, thanks a lotInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction''def train_generator_step(self, input_ids, attention_mask, labels, styles, max_len, step, accumulation_steps=4,lambda_rec=1.0, lambda_lm=1.0, lambda_adv=1.0, lambda_kl=1.0, gamma=1.0):max_len = tf.constant(max_len, dtype=tf.int32)\n",
      "max_len_value = max_len\n",
      "seq_len = input_ids.shape[2]\n",
      "max_new_tokens = tf.maximum(max_len_value - seq_len - 10, 1)\n",
      "max_new_tokens = tf.cast(max_new_tokens, tf.int32)\n",
      "max_new_tokens = tf.maximum(max_new_tokens, 1)\n",
      "max_new_tokens = tf.constant(max_new_tokens, dtype=tf.int32)\n",
      "\n",
      "@tf.function\n",
      "def step_fn(input_ids=input_ids, attention_mask=attention_mask, labels=labels, styles=styles, accumulation_steps=accumulation_steps, \n",
      "            lambda_rec=lambda_rec, lambda_lm=lambda_lm, lambda_adv=lambda_adv, lambda_kl=lambda_kl, gamma=gamma):\n",
      "    with tf.GradientTape() as tape:\n",
      "        tf.debugging.enable_check_numerics()\n",
      "        \n",
      "        accumulation_steps, lambda_rec, lambda_lm, lambda_adv, lambda_kl, gamma = pr.conv_tensor_to_float(accumulation_steps, lambda_rec, lambda_lm, lambda_adv, lambda_kl, gamma)\n",
      "\n",
      "        epsilon = 1e-6 # 快速修复\n",
      "\n",
      "        \"\"\"\n",
      "        we firstly to reshape the input\n",
      "        \"\"\"\n",
      "        actual_shape = tf.shape(input_ids)\n",
      "        input_ids = tf.reshape(input_ids, (actual_shape[0] * actual_shape[1], actual_shape[2]))\n",
      "        attention_mask = tf.reshape(attention_mask, (actual_shape[0] * actual_shape[1], actual_shape[2]))\n",
      "\n",
      "        \"\"\"\n",
      "        then, we repeat styles and labels\n",
      "        \"\"\"\n",
      "        styles = tf.repeat(styles, repeats=actual_shape[0])\n",
      "        labels = tf.repeat(labels, repeats=actual_shape[0], axis=0)\n",
      "\n",
      "        # 嵌入风格标签\n",
      "        style_embeddings = self.embedding(styles) # [num_devices * batch_size, n_embd]\n",
      "        print(\"Style embeddings shape:\", style_embeddings.shape)  # Debug info\n",
      "        \n",
      "        # 将输入 ID 嵌入到相同的嵌入空间\n",
      "        input_embeddings = self.gen.transformer.wte(input_ids) # [num_devices * batch_size, seq_len, n_embd]\n",
      "        print(\"Input embeddings shape:\", input_embeddings.shape)  # Debug info\n",
      "        \n",
      "        extended_input_embeddings = input_embeddings + tf.expand_dims(style_embeddings, axis=1)\n",
      "        print(\"Extended embeddings shape:\", extended_input_embeddings.shape)  # Debug info\n",
      "\n",
      "        input_ids, attention_mask, labels, styles = dis.convert_tensor(input_ids, attention_mask, labels, styles)\n",
      "\n",
      "        outputs = self.gen(input_ids=input_ids, attention_mask=attention_mask, training=True)\n",
      "        logits = outputs.logits\n",
      "        print(\"Logits shape:\", logits.shape)  # Debug info\n",
      "        print(\"Logits dtype:\", logits.dtype)  # Debug info\n",
      "        print(\"labels shape:\", labels.shape)  # Debug info\n",
      "\n",
      "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
      "        mask = tf.cast(labels != -100, logits.dtype)\n",
      "        print(\"Mask shape:\", mask.shape)  # Debug info\n",
      "        print(\"Mask dtype:\", mask.dtype)  # Debug info\n",
      "\n",
      "        # Check for NaN or Inf in logits\n",
      "        tf.debugging.check_numerics(logits, \"Logits contain NaN or Inf\")\n",
      "\n",
      "        # 各个损失\n",
      "        rec_loss = loss_fn(tf.where(labels == -100, tf.zeros_like(labels, dtype=logits.dtype), tf.cast(labels, logits.dtype)), logits)\n",
      "        rec_loss = tf.reduce_sum(rec_loss * mask) / (tf.reduce_sum(mask) + epsilon)\n",
      "        rec_loss = tf.cast(rec_loss, tf.float32)\n",
      "        print(\"Reconstruction loss:\", rec_loss)  # Debug info\n",
      "\n",
      "        for var in self.gen.trainable_variables:\n",
      "            tf.debugging.check_numerics(var, message=\"Model weight check\")\n",
      "\n",
      "        print(\"Input shapes:\", \n",
      "                \"input_ids:\", input_ids.shape, \n",
      "                \"input_ids dtype:\", input_ids.dtype,\n",
      "                \"attention_mask:\", attention_mask.shape, \n",
      "                \"labels:\", labels.shape, \n",
      "                \"styles:\", styles.shape,\n",
      "                \"max_len_value:\", max_len_value)\n",
      "\n",
      "        new_shape = tf.shape(input_ids)\n",
      "        print(\"New shape:\", new_shape)  # Debug info\n",
      "        print(\"Seq len:\", seq_len)  # Debug info\n",
      "        \n",
      "        # max_new_tokens = tf.maximum(max_new_tokens, 1)\n",
      "        print(\"Max length:\", max_len_value)  # Debug info\n",
      "        print(\"Max new tokens:\", max_new_tokens)  # Debug info\n",
      "        if isinstance(max_new_tokens, tf.Tensor):\n",
      "            print(\"Max new tokens:\", tf.get_static_value(max_new_tokens))  # Debug info\n",
      "        batch_size = new_shape[0]\n",
      "\n",
      "        # 扩展 input_ids\n",
      "        \"\"\"\n",
      "        at here, we need to padding to -> [batch_size, max_new_tokens]\n",
      "        \"\"\"\n",
      "        padding = tf.zeros((batch_size, max_new_tokens), dtype=input_ids.dtype)\n",
      "        print(\"Padding shape:\", padding.shape)  # Debug info\n",
      "        \n",
      "        extended_input_ids = tf.concat([input_ids, padding], axis=1)\n",
      "        extended_attention_mask1 = tf.concat([attention_mask, tf.zeros((tf.shape(attention_mask)[0],\n",
      "                                    max_new_tokens), dtype=attention_mask.dtype)], axis=1)\n",
      "        \n",
      "        extended_input_ids = tf.cast(extended_input_ids, tf.int32)\n",
      "        extended_attention_mask1 = tf.cast(extended_attention_mask1, tf.float32)\n",
      "        \n",
      "        print(f\"Extended input_ids shape: {extended_input_ids.shape}\")\n",
      "        print(f\"Extended attention_mask shape: {extended_attention_mask1.shape}\")\n",
      "\n",
      "        # 确保最大长度大于最小长度\n",
      "        max_length = max_len_value + max_new_tokens\n",
      "        min_length = 1\n",
      "        # tf.print(\"max_length:\", max_length)\n",
      "        # tf.print(\"min_length:\", min_length)\n",
      "\n",
      "        tf.debugging.assert_greater(max_length, min_length, message=f\"max_length ({max_length}) must be greater than min_length ({min_length})\")\n",
      "        \n",
      "        pad_token_id = int(self.tokenizer.pad_token_id)\n",
      "        eos_token_id = int(self.tokenizer.eos_token_id)\n",
      "        bos_token_id = int(self.tokenizer.bos_token_id)\n",
      "\n",
      "        try:                    \n",
      "            generated_ids = self.gen.generate(\n",
      "                extended_input_ids, \n",
      "                attention_mask=extended_attention_mask1, \n",
      "                max_new_tokens=max_new_tokens,\n",
      "                pad_token_id=pad_token_id,\n",
      "                eos_token_id=eos_token_id,\n",
      "                bos_token_id=bos_token_id,\n",
      "                # use_cache=True,\n",
      "                # num_beams=1,  # 使用贪婪搜索\n",
      "                do_sample=False,  # 不使用采样\n",
      "                # temperature=1.0,  # 降低随机性\n",
      "            )\n",
      "            print(\"Generation successful. Generated IDs shape:\", generated_ids.shape)\n",
      "\n",
      "        except Exception as e:\n",
      "            print(f\"Error during generation: {e}\")\n",
      "            print(f\"input_ids shape: {input_ids.shape}\")\n",
      "            print(f\"attention_mask shape: {attention_mask.shape}\")\n",
      "            print(f\"max_len_value: {max_len_value}\")\n",
      "            raise'''\"\"\"Relevant Message\"\"\"Training genStyle embeddings shape: (4, 768)Input embeddings shape: (4, 72, 768)Extended embeddings shape: (4, 72, 768)Logits shape: (4, 72, 21128)Logits dtype: <dtype: 'float16'>labels shape: (4, 72)Mask shape: (4, 72)Mask dtype: <dtype: 'float16'>Reconstruction loss: Tensor(\"Cast_3:0\", shape=(), dtype=float32)Input shapes: input_ids: (4, 72) input_ids dtype: <dtype: 'int32'> attention_mask: (4, 72) labels: (4, 72) styles: (4,) max_len_value: tf.Tensor(125, shape=(), dtype=int32)New shape: Tensor(\"Shape_1:0\", shape=(2,), dtype=int32)Seq len: 72Max length: tf.Tensor(125, shape=(), dtype=int32)Max new tokens: tf.Tensor(43, shape=(), dtype=int32)Max new tokens: 43Padding shape: (4, 43)Extended input_ids shape: (4, 115)Extended attention_mask shape: (4, 115)/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:377: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (seehttps://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration)return py_builtins.overload_of(f)(*args)Error during generation: max_new_tokens must be greater than 0, but is 43.input_ids shape: (4, 72)attention_mask shape: (4, 72)max_len_value: 125Traceback (most recent call last):File \"train.py\", line 530, intrain_model.train(train_tf_dataset_X, train_tf_dataset_Y, valid_tf_dataset_X, valid_tf_dataset_Y, trainconfig.epochs)File \"train.py\", line 306, in trainrec_loss, lm_loss, adv_loss, kl_loss, current_lr, accuracy, total_gen_loss = self.distributed_train_generator_step(File \"train.py\", line 138, in distributed_train_generator_steploss, rec_loss, lm_loss, adv_loss, kl_loss, current_lr, accuracy = self.strategy.run(File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1316, in runreturn self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2892, in call_for_each_replicareturn self._call_for_each_replica(fn, args, kwargs)File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 677, in _call_for_each_replicareturn mirrored_run.call_for_each_replica(File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 104, in call_for_each_replicareturn _call_for_each_replica(strategy, fn, args, kwargs)File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 246, in _call_for_each_replicacoord.join(threads)File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/training/coordinator.py\", line 389, in joinsix.reraise(*self._exc_info_to_raise)File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/six.py\", line 719, in reraiseraise valueFile \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exceptionyieldFile \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in runself.main_result = self.main_fn(*self.main_args, **self.main_kwargs)File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 601, in wrapperreturn func(*args, **kwargs)File \"train.py\", line 133, in generator_steploss, rec_loss, lm_loss, adv_loss, kl_loss, current_lr, accuracy, gradients = self.model.train_generator_step(*args, **kwargs)File \"/root/autodl-tmp/model/model.py\", line 302, in train_generator_stepstep_total_loss, step_rec_loss, step_lm_loss, step_adv_loss, step_kl_loss, step_gradients, step_accuracy = step_fn(File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handlerraise e.with_traceback(filtered_tb) from NoneFile \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1129, in autograph_handlerraise e.ag_error_metadata.to_exception(e)ValueError: in user code:File \"/root/autodl-tmp/model/model.py\", line 220, in step_fn  *generated_ids = self.gen.generate(File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/transformers/generation/tf_utils.py\", line 738, in generate  *model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargsFile \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py\", line 1207, in update  *self.validate()File \"/root/miniconda3/envs/gpt2-env/lib/python3.8/site-packages/transformers/generation/configuration_utils.py\", line 544, in validate  *raise ValueError(f\"max_new_tokensmust be greater than 0, but is {self.max_new_tokens}.\")ValueError:max_new_tokensmust be greater than 0, but is 43.\"\"\"Definetely, l occured this mistake within @tf.function, and there is no logical mistake when l dubug my code under eager-excution model, similarly, when i use max_length and min_length paramters, it would be occured to \"ValueError: max_length must be greater than min_length, 1 is larger than 128.\", like this. But, when l set the paramter\"max_new_tokens\" as a constant value like 50, it would be fine, l donno what leads this, and debug this for at least 20 times.\"\"\"Expected behaviorOf course, the value of my variable is dynamic, but I have already defined it outside the graph and used it as a parameter. My expected behavior should be 43 as max_new_token, but it reported an error.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_69.txt:\n",
      "Title: Multi-GPU setup: indices should be either on cpu or on the same device as the indexed tensor (cuda:1)\n",
      "URL: https://github.com/huggingface/transformers/issues/33147\n",
      "Body:\n",
      "System Infopython version: 3.11.9transformers version: 4.44.2accelerate version: 0.33.0torch version: 2.4.0+cu121Who can help?@ganteInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionHello!I have a setup with 8xH100 and I need to run really large models. To get started I went through your official example, it ishttps://huggingface.co/docs/accelerate/en/concept_guides/big_model_inferenceFirst of all, there is a typo in there:frommingpt.bpeimportBPETokenizertokenizer=BPETokenizer()inputs=tokenizer(\"Hello, my name is\").to(0)outputs=model.generate(x1,max_new_tokens=10,do_sample=False)[0]tokenizer.decode(outputs.cpu().squeeze())There is no x1 variable. Example from this guide works well.However, I tried to do the same using Mistral models, so I adapted code form an example to run it with Mistral model:fromhuggingface_hubimportsnapshot_downloadfromaccelerateimportinit_empty_weightsfromtransformersimportAutoModelForCausalLM,AutoTokenizerfromaccelerateimportload_checkpoint_and_dispatchcheckpoint='mistralai/Mixtral-8x22B-Instruct-v0.1'weights_location=snapshot_download(repo_id=checkpoint,cache_dir='./cache')model=AutoModelForCausalLM.from_pretrained(checkpoint)fromaccelerateimportload_checkpoint_and_dispatchmodel=load_checkpoint_and_dispatch(model,checkpoint=weights_location,device_map=\"auto\",no_split_module_classes=['Block']\n",
      ")tokenizer=AutoTokenizer.from_pretrained(checkpoint)inputs=tokenizer(\"Hello, my name is\",return_tensors=\"pt\").to(0)outputs=model.generate(inputs['input_ids'],max_new_tokens=10,do_sample=False)[0]This code fails on the last line giving the following exception:RuntimeError                              Traceback (most recent call last)\n",
      "Cell In[37], line 1\n",
      "----> 1 outputs = model.generate(inputs['input_ids'], max_new_tokens=10, do_sample=False)[0]\n",
      "      2 tokenizer.decode(outputs.cpu().squeeze())\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)\n",
      "    113 @functools.wraps(func)\n",
      "    114 def decorate_context(*args, **kwargs):\n",
      "    115     with ctx_factory():\n",
      "--> 116         return func(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/generation/utils.py:2024, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n",
      "   2016     input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "   2017         input_ids=input_ids,\n",
      "   2018         expand_size=generation_config.num_return_sequences,\n",
      "   2019         is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "   2020         **model_kwargs,\n",
      "   2021     )\n",
      "   2023     # 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n",
      "-> 2024     result = self._sample(\n",
      "   2025         input_ids,\n",
      "   2026         logits_processor=prepared_logits_processor,\n",
      "   2027         logits_warper=prepared_logits_warper,\n",
      "   2028         stopping_criteria=prepared_stopping_criteria,\n",
      "   2029         generation_config=generation_config,\n",
      "   2030         synced_gpus=synced_gpus,\n",
      "   2031         streamer=streamer,\n",
      "   2032         **model_kwargs,\n",
      "   2033     )\n",
      "   2035 elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n",
      "   2036     # 11. prepare logits warper\n",
      "   2037     prepared_logits_warper = (\n",
      "   2038         self._get_logits_warper(generation_config, device=input_ids.device)\n",
      "   2039         if generation_config.do_sample\n",
      "   2040         else None\n",
      "   2041     )\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/generation/utils.py:2982, in GenerationMixin._sample(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\n",
      "   2979 model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n",
      "   2981 # forward pass to get next token\n",
      "-> 2982 outputs = self(**model_inputs, return_dict=True)\n",
      "   2984 if synced_gpus and this_peer_finished:\n",
      "   2985     continue  # don't waste resources running the code we don't need\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
      "   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
      "   1552 else:\n",
      "-> 1553     return self._call_impl(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n",
      "   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1558 # this function, and just call forward.\n",
      "   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
      "   1560         or _global_backward_pre_hooks or _global_backward_hooks\n",
      "   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1562     return forward_call(*args, **kwargs)\n",
      "   1564 try:\n",
      "   1565     result = None\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/accelerate/hooks.py:169, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)\n",
      "    167         output = module._old_forward(*args, **kwargs)\n",
      "    168 else:\n",
      "--> 169     output = module._old_forward(*args, **kwargs)\n",
      "    170 return module._hf_hook.post_forward(module, output)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:1274, in MixtralForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position)\n",
      "   1271 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "   1273 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "-> 1274 outputs = self.model(\n",
      "   1275     input_ids=input_ids,\n",
      "   1276     attention_mask=attention_mask,\n",
      "   1277     position_ids=position_ids,\n",
      "   1278     past_key_values=past_key_values,\n",
      "   1279     inputs_embeds=inputs_embeds,\n",
      "   1280     use_cache=use_cache,\n",
      "   1281     output_attentions=output_attentions,\n",
      "   1282     output_hidden_states=output_hidden_states,\n",
      "   1283     output_router_logits=output_router_logits,\n",
      "   1284     return_dict=return_dict,\n",
      "   1285     cache_position=cache_position,\n",
      "   1286 )\n",
      "   1288 hidden_states = outputs[0]\n",
      "   1289 logits = self.lm_head(hidden_states)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
      "   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
      "   1552 else:\n",
      "-> 1553     return self._call_impl(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n",
      "   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1558 # this function, and just call forward.\n",
      "   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
      "   1560         or _global_backward_pre_hooks or _global_backward_hooks\n",
      "   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1562     return forward_call(*args, **kwargs)\n",
      "   1564 try:\n",
      "   1565     result = None\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:1068, in MixtralModel.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict, cache_position)\n",
      "   1056     layer_outputs = self._gradient_checkpointing_func(\n",
      "   1057         decoder_layer.__call__,\n",
      "   1058         hidden_states,\n",
      "   (...)\n",
      "   1065         cache_position,\n",
      "   1066     )\n",
      "   1067 else:\n",
      "-> 1068     layer_outputs = decoder_layer(\n",
      "   1069         hidden_states,\n",
      "   1070         attention_mask=causal_mask,\n",
      "   1071         position_ids=position_ids,\n",
      "   1072         past_key_value=past_key_values,\n",
      "   1073         output_attentions=output_attentions,\n",
      "   1074         output_router_logits=output_router_logits,\n",
      "   1075         use_cache=use_cache,\n",
      "   1076         cache_position=cache_position,\n",
      "   1077     )\n",
      "   1079 hidden_states = layer_outputs[0]\n",
      "   1081 if use_cache:\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
      "   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
      "   1552 else:\n",
      "-> 1553     return self._call_impl(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n",
      "   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1558 # this function, and just call forward.\n",
      "   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
      "   1560         or _global_backward_pre_hooks or _global_backward_hooks\n",
      "   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1562     return forward_call(*args, **kwargs)\n",
      "   1564 try:\n",
      "   1565     result = None\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:812, in MixtralDecoderLayer.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache, cache_position, **kwargs)\n",
      "    810 residual = hidden_states\n",
      "    811 hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "--> 812 hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n",
      "    813 hidden_states = residual + hidden_states\n",
      "    815 outputs = (hidden_states,)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
      "   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
      "   1552 else:\n",
      "-> 1553     return self._call_impl(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n",
      "   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1558 # this function, and just call forward.\n",
      "   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
      "   1560         or _global_backward_pre_hooks or _global_backward_hooks\n",
      "   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1562     return forward_call(*args, **kwargs)\n",
      "   1564 try:\n",
      "   1565     result = None\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/mixtral/modeling_mixtral.py:738, in MixtralSparseMoeBlock.forward(self, hidden_states)\n",
      "    733 idx, top_x = torch.where(expert_mask[expert_idx])\n",
      "    735 # Index the correct hidden states and compute the expert hidden state for\n",
      "    736 # the current expert. We need to make sure to multiply the output hidden\n",
      "    737 # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n",
      "--> 738 current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n",
      "    739 current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
      "    741 # However `index_add_` only support torch tensors for indexing so we'll use\n",
      "    742 # the `top_x` tensor here.\n",
      "\n",
      "RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cuda:1)I think the most important is:RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cuda:1)There are few additions to that:I am running this code using Jupyter notebook.Model seems like properly distributed across all GPUs:+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   32C    P0            111W /  700W |   67576MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          Off |   00000000:2D:00.0 Off |                    0 |\n",
      "| N/A   37C    P0            115W /  700W |   68344MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          Off |   00000000:44:00.0 Off |                    0 |\n",
      "| N/A   31C    P0            110W /  700W |   68344MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          Off |   00000000:5B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0            116W /  700W |   68552MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          Off |   00000000:89:00.0 Off |                    0 |\n",
      "| N/A   32C    P0            111W /  700W |   68192MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          Off |   00000000:A8:00.0 Off |                    0 |\n",
      "| N/A   35C    P0            119W /  700W |   68344MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          Off |   00000000:C0:00.0 Off |                    0 |\n",
      "| N/A   40C    P0            116W /  700W |   68360MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          Off |   00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   32C    P0            110W /  700W |   67568MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      67566MiB |\n",
      "|    1   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68334MiB |\n",
      "|    2   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68334MiB |\n",
      "|    3   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68542MiB |\n",
      "|    4   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68182MiB |\n",
      "|    5   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68334MiB |\n",
      "|    6   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      68350MiB |\n",
      "|    7   N/A  N/A    701575      C   ...yenv/versions/3.11.9/bin/python3.11      67558MiB |\n",
      "+-----------------------------------------------------------------------------------------+However, I found easier step of reproduction with smaller model, it uses a bit different approach, but the result is the same:fromtransformersimportAutoModelForCausalLM,AutoTokenizerimporttorchfromaccelerateimportinfer_auto_device_map,dispatch_model# tried with smaller mistral, result is the samemodel_name='meta-llama/Meta-Llama-Guard-2-8B'model_name='meta-llama/Meta-Llama-Guard-2-8B'token='my-hf-token'tokenizer=AutoTokenizer.from_pretrained(model_name,token=token)model=AutoModelForCausalLM.from_pretrained(model_name,token=token)device_map=infer_auto_device_map(model,max_memory={i:\"8GiB\"foriinrange(torch.cuda.device_count())}\n",
      ")model2=dispatch_model(model,device_map=device_map)# Prepare input messagesmessages=[\n",
      "    {\"role\":\"user\",\"content\":\"Hello, how are you?\"},\n",
      "    {\"role\":\"assistant\",\"content\":\"I'm doing well, thank you! How can I assist you today?\"},\n",
      "    {\"role\":\"user\",\"content\":\"Can you tell me about the weather today?\"}\n",
      "]# Concatenate the messages into a single stringconversation=\"\\n\".join([f\"{msg['role']}:{msg['content']}\"formsginmessages])inputs=tokenizer(conversation,return_tensors=\"pt\")# I tried also# inputs = tokenizer(conversation, return_tensors=\"pt\").to('cuda:0')# and# inputs = tokenizer(conversation, return_tensors=\"pt\").to('cuda:1')# It is the same# and even like this:# first_device = list(device_map.values())[0]# inputs = inputs.to(f'cuda:{first_device}')# still the sameoutput=model.generate(inputs[\"input_ids\"],max_length=150,# Adjust according to your needsnum_return_sequences=1,no_repeat_ngram_size=2,do_sample=True,top_k=50,top_p=0.95,\n",
      ")The result is the same:RuntimeError                              Traceback (most recent call last)\n",
      "Cell In[21], line 2\n",
      "      1 # Generate text with the model\n",
      "----> 2 output = model.generate(\n",
      "      3     inputs[\"input_ids\"],\n",
      "      4     max_length=150,  # Adjust according to your needs\n",
      "      5     num_return_sequences=1,\n",
      "      6     no_repeat_ngram_size=2,\n",
      "      7     do_sample=True,\n",
      "      8     top_k=50,\n",
      "      9     top_p=0.95,\n",
      "     10 )\n",
      "     12 # Decode the output to readable text\n",
      "     13 generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)\n",
      "    113 @functools.wraps(func)\n",
      "    114 def decorate_context(*args, **kwargs):\n",
      "    115     with ctx_factory():\n",
      "--> 116         return func(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/generation/utils.py:2024, in GenerationMixin.generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\n",
      "   2016     input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "   2017         input_ids=input_ids,\n",
      "   2018         expand_size=generation_config.num_return_sequences,\n",
      "   2019         is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "   2020         **model_kwargs,\n",
      "   2021     )\n",
      "   2023     # 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n",
      "-> 2024     result = self._sample(\n",
      "   2025         input_ids,\n",
      "   2026         logits_processor=prepared_logits_processor,\n",
      "   2027         logits_warper=prepared_logits_warper,\n",
      "   2028         stopping_criteria=prepared_stopping_criteria,\n",
      "   2029         generation_config=generation_config,\n",
      "   2030         synced_gpus=synced_gpus,\n",
      "   2031         streamer=streamer,\n",
      "   2032         **model_kwargs,\n",
      "   2033     )\n",
      "   2035 elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n",
      "   2036     # 11. prepare logits warper\n",
      "   2037     prepared_logits_warper = (\n",
      "   2038         self._get_logits_warper(generation_config, device=input_ids.device)\n",
      "   2039         if generation_config.do_sample\n",
      "   2040         else None\n",
      "   2041     )\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/generation/utils.py:2982, in GenerationMixin._sample(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\n",
      "   2979 model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n",
      "   2981 # forward pass to get next token\n",
      "-> 2982 outputs = self(**model_inputs, return_dict=True)\n",
      "   2984 if synced_gpus and this_peer_finished:\n",
      "   2985     continue  # don't waste resources running the code we don't need\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
      "   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
      "   1552 else:\n",
      "-> 1553     return self._call_impl(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n",
      "   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1558 # this function, and just call forward.\n",
      "   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
      "   1560         or _global_backward_pre_hooks or _global_backward_hooks\n",
      "   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1562     return forward_call(*args, **kwargs)\n",
      "   1564 try:\n",
      "   1565     result = None\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/accelerate/hooks.py:169, in add_hook_to_module.<locals>.new_forward(module, *args, **kwargs)\n",
      "    167         output = module._old_forward(*args, **kwargs)\n",
      "    168 else:\n",
      "--> 169     output = module._old_forward(*args, **kwargs)\n",
      "    170 return module._hf_hook.post_forward(module, output)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189, in LlamaForCausalLM.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\n",
      "   1186 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "   1188 # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
      "-> 1189 outputs = self.model(\n",
      "   1190     input_ids=input_ids,\n",
      "   1191     attention_mask=attention_mask,\n",
      "   1192     position_ids=position_ids,\n",
      "   1193     past_key_values=past_key_values,\n",
      "   1194     inputs_embeds=inputs_embeds,\n",
      "   1195     use_cache=use_cache,\n",
      "   1196     output_attentions=output_attentions,\n",
      "   1197     output_hidden_states=output_hidden_states,\n",
      "   1198     return_dict=return_dict,\n",
      "   1199     cache_position=cache_position,\n",
      "   1200 )\n",
      "   1202 hidden_states = outputs[0]\n",
      "   1203 if self.config.pretraining_tp > 1:\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
      "   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
      "   1552 else:\n",
      "-> 1553     return self._call_impl(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n",
      "   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1558 # this function, and just call forward.\n",
      "   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
      "   1560         or _global_backward_pre_hooks or _global_backward_hooks\n",
      "   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1562     return forward_call(*args, **kwargs)\n",
      "   1564 try:\n",
      "   1565     result = None\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1001, in LlamaModel.forward(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\n",
      "    989     layer_outputs = self._gradient_checkpointing_func(\n",
      "    990         decoder_layer.__call__,\n",
      "    991         hidden_states,\n",
      "   (...)\n",
      "    998         position_embeddings,\n",
      "    999     )\n",
      "   1000 else:\n",
      "-> 1001     layer_outputs = decoder_layer(\n",
      "   1002         hidden_states,\n",
      "   1003         attention_mask=causal_mask,\n",
      "   1004         position_ids=position_ids,\n",
      "   1005         past_key_value=past_key_values,\n",
      "   1006         output_attentions=output_attentions,\n",
      "   1007         use_cache=use_cache,\n",
      "   1008         cache_position=cache_position,\n",
      "   1009         position_embeddings=position_embeddings,\n",
      "   1010     )\n",
      "   1012 hidden_states = layer_outputs[0]\n",
      "   1014 if use_cache:\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
      "   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
      "   1552 else:\n",
      "-> 1553     return self._call_impl(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n",
      "   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1558 # this function, and just call forward.\n",
      "   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
      "   1560         or _global_backward_pre_hooks or _global_backward_hooks\n",
      "   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1562     return forward_call(*args, **kwargs)\n",
      "   1564 try:\n",
      "   1565     result = None\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:750, in LlamaDecoderLayer.forward(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\n",
      "    748 residual = hidden_states\n",
      "    749 hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "--> 750 hidden_states = self.mlp(hidden_states)\n",
      "    751 hidden_states = residual + hidden_states\n",
      "    753 outputs = (hidden_states,)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\n",
      "   1551     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n",
      "   1552 else:\n",
      "-> 1553     return self._call_impl(*args, **kwargs)\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\n",
      "   1557 # If we don't have any hooks, we want to skip the rest of the logic in\n",
      "   1558 # this function, and just call forward.\n",
      "   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n",
      "   1560         or _global_backward_pre_hooks or _global_backward_hooks\n",
      "   1561         or _global_forward_hooks or _global_forward_pre_hooks):\n",
      "-> 1562     return forward_call(*args, **kwargs)\n",
      "   1564 try:\n",
      "   1565     result = None\n",
      "\n",
      "File ~/.pyenv/versions/3.11.9/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:309, in LlamaMLP.forward(self, x)\n",
      "    307     down_proj = sum(down_proj)\n",
      "    308 else:\n",
      "--> 309     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "    311 return down_proj\n",
      "\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cuda:1!Since it happens not only on Mistral models, I believe it is either something wrong in my code or there is a bug in the library, please help me to find out. Thanks.Expected behaviorInference should work on multiple GPU devices.I can provide any additional information and try some adjustments.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_496.txt:\n",
      "Title: Whisper Inference in BF16 precision.\n",
      "URL: https://github.com/huggingface/transformers/issues/29475\n",
      "Body:\n",
      "System InfoA google colab instance.Who can help?@sanchit-gandhiInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionMy sample code:importtorchfromtransformersimportpipelinedevice=\"cpu\"pipe=pipeline(\"automatic-speech-recognition\",model=\"openai/whisper-base\",device=device,torch_dtype=torch.bfloat16,\n",
      ")sample=\"/content/sample.mp3\"prediction=pipe(sample)[\"text\"]print(prediction)Expected behaviorI used the sample pipeline code from the huggingface whisper samples and in the dtype argument for the pipeline i wanted the inference to run in BF16 precision. Below is the error i encountered.RuntimeError: Input type (torch.FloatTensor) and weight type (CPUBFloat16Type) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_482.txt:\n",
      "Title: Whisper no_speech_threshold not applied when chunking input\n",
      "URL: https://github.com/huggingface/transformers/issues/29595\n",
      "Body:\n",
      "Feature requestThe Whisper pipeline accepts achunk_length_sparameter, which chunks the input so it can be used for batch inference. There is also ano_speech_thresholdparam, which can be used to filter out silence, which helps with reducing halucinations. The problem is, forno_speech_thresholdto be applied theoutput must be \"long\". And in the case of chunked input, every segment is considered short, even though the full input is long. This means theno_speech_thresholdcant be applied for chunked input.When attempting, It gives this error for each batch:Audio input consists of only 3000. Short-form transcription is activated.no_speech_threshold is set to 0.2, but will be ignored.It should be possible to keep the no speech threshold enabled for long chunked inputs.MotivationChunking brings large performance improvements for long-form transcription, but this benefit is negated if there is no way to suppress the silence segments, which most long-form audio will no doubt contain.Your contributionI could implement a change which removes theis_shortformcheck when the input is chunked, though I'm unsure if and why this will break things.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_96.txt:\n",
      "Title: clarify the label shifting behavior of llama models whenlabelsis given.\n",
      "URL: https://github.com/huggingface/transformers/issues/32944\n",
      "Body:\n",
      "Feature requesti believelabelsin the training of causal LMs means the value to predict at timen, i.e., the next token. in other words, i'd assume, iflabelsis given, it should be already shifted by one in the data loader w.r.t. theinput_ids.however, inLlamaForCausalLM.forward(), i found the labels are always shifted, silently.transformers/src/transformers/models/llama/modeling_llama.pyLines 1205 to 1210\n",
      "      inf1d822biflabelsisnotNone:# Shift so that tokens < n predict nshift_logits=logits[..., :-1, :].contiguous()shift_labels=labels[...,1:].contiguous()# Flatten the tokensloss_fct=CrossEntropyLoss()Args:labels(`torch.LongTensor`ofshape`(batch_size, sequence_length)`,*optional*):Labelsforcomputingthemaskedlanguagemodelingloss.Indicesshouldeitherbein`[0, ...,config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100`areignored(masked),thelossisonlycomputedforthetokenswithlabelsin`[0, ..., config.vocab_size]`....iflabelsisnotNone:# Shift so that tokens < n predict nshift_logits=logits[..., :-1, :].contiguous()shift_labels=labels[...,1:].contiguous()# Flatten the tokensloss_fct=CrossEntropyLoss()shift_logits=shift_logits.view(-1,self.config.vocab_size)shift_labels=shift_labels.view(-1)# Enable model parallelismshift_labels=shift_labels.to(shift_logits.device)loss=loss_fct(shift_logits,shift_labels)i found it quite unexpected hence calling it \"silently\". as this is for a causal LM, shouldn't it be not shifting the labels by default? in modeling GPT2, this is at least documented explicitly.https://github.com/huggingface/transformers/blob/f1d822ba337499d429f832855622b97d90ac1406/src/transformers/models/gpt2/modeling_gpt2.py#L1309-1314in gemma2, it has the same behavior and no explicit mentioning in the docstring.transformers/src/transformers/models/gemma2/modeling_gemma2.pyLines 978 to 982\n",
      "      inf1d822bArgs:labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored(masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.i think at least we should force the docstring to mention this, if making a change is too dangerous at this point.Motivationi didn't expect this behavior and used my data loader, which does the shifting already, as i believe that is whatlabelsshould mean. as a result, i ended up finetuning a model to predict the next next token, which outputted gibberish.Your contributionhopefully leaving this issue helps communication across usersi can make a one line change in the docstring.not sure how exactly, but if this potential misunderstanding could be checked, it'd be great. technically, we can check if the labels are already shifted. though i don't know where is the best place for this.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_82.txt:\n",
      "Title: Using multi GPU fails with AutoModelForCausalLM quantization_config=quantization_config\n",
      "URL: https://github.com/huggingface/transformers/issues/33112\n",
      "Body:\n",
      "I am developing an very advanced multi-GPU batch captioning APPThe below code works when I dont use quantization_config=quantization_config because i am able to set.to(device)but whenquantization_config=quantization_configis used it doesn't allow me to set.to(device)Any ideas ?Whenquantization_config=quantization_configis set the error i got isYou shouldn't move a model that is dispatched using accelerate hooks.Error processing image R:\\Joy_Caption_v1\\outputs\\1_3x_Ultimate_Fidelity_Standard_Texture_2_Creativity_0.png on GPU 0:.tois not supported for4-bitor8-bitbitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correctdtype.If I remove.topart then the multi GPU part fails :/The entire code is below it is not very big 460 linesI want to be able to run 8 different captionioning on 8 different GPUsimport gradio as gr\n",
      "from huggingface_hub import InferenceClient\n",
      "from torch import nn\n",
      "from transformers import AutoModel, AutoProcessor, AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast, AutoModelForCausalLM\n",
      "from pathlib import Path\n",
      "import torch\n",
      "import torch.amp.autocast_mode\n",
      "from PIL import Image, ImageOps\n",
      "import numpy as np\n",
      "import io\n",
      "import os\n",
      "import argparse\n",
      "import time\n",
      "import glob\n",
      "import platform\n",
      "from transformers import BitsAndBytesConfig\n",
      "import re\n",
      "import threading\n",
      "from concurrent.futures import ThreadPoolExecutor\n",
      "import sys\n",
      "\n",
      "CLIP_PATH = \"google/siglip-so400m-patch14-384\"\n",
      "VLM_PROMPT = \"A descriptive caption for this image:\\n\"\n",
      "MODEL_PATH = \"rombodawg/Meta-Llama-3.1-8B-Instruct-reuploaded\"\n",
      "CHECKPOINT_PATH = Path(\"model_files\")\n",
      "TITLE = \"<h1><center>SECourses JoyCaption Image Captioning App V11</center></h1>\\n<h2><center>Official Link and Latest Version : <a href='https://www.patreon.com/posts/110613301'>https://www.patreon.com/posts/110613301</a></center></h2>\\n\"\n",
      "\n",
      "HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n",
      "\n",
      "class ImageAdapter(nn.Module):\n",
      "    def __init__(self, input_features: int, output_features: int):\n",
      "        super().__init__()\n",
      "        self.linear1 = nn.Linear(input_features, output_features)\n",
      "        self.activation = nn.GELU()\n",
      "        self.linear2 = nn.Linear(output_features, output_features)\n",
      "    \n",
      "    def forward(self, vision_outputs: torch.Tensor):\n",
      "        x = self.linear1(vision_outputs)\n",
      "        x = self.activation(x)\n",
      "        x = self.linear2(x)\n",
      "        return x\n",
      "\n",
      "# Load CLIP\n",
      "print(\"Loading CLIP\")\n",
      "clip_processor = AutoProcessor.from_pretrained(CLIP_PATH)\n",
      "clip_model = AutoModel.from_pretrained(CLIP_PATH)\n",
      "clip_model = clip_model.vision_model\n",
      "clip_model.eval()\n",
      "clip_model.requires_grad_(False)\n",
      "\n",
      "# Tokenizer\n",
      "print(\"Loading tokenizer\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
      "assert isinstance(tokenizer, PreTrainedTokenizer) or isinstance(tokenizer, PreTrainedTokenizerFast), f\"Tokenizer is of type {type(tokenizer)}\"\n",
      "\n",
      "class ModelManager:\n",
      "    def __init__(self, use_4bit):\n",
      "        self.use_4bit = use_4bit\n",
      "        self.models = {}\n",
      "        self.image_adapters = {}\n",
      "        self.clip_models = {}\n",
      "        self.default_gpu = 0\n",
      "\n",
      "    def get_models(self, gpu_id):\n",
      "        device = f\"cuda:{gpu_id}\"\n",
      "        if gpu_id not in self.models:\n",
      "            print(f\"Loading model for GPU {gpu_id}\")\n",
      "            text_model = self.load_model(device)\n",
      "            text_model.eval()\n",
      "            \n",
      "            image_adapter = ImageAdapter(clip_model.config.hidden_size, text_model.config.hidden_size).to(device)\n",
      "            image_adapter.load_state_dict(torch.load(CHECKPOINT_PATH / \"image_adapter.pt\", map_location=device))\n",
      "            image_adapter.eval()\n",
      "            \n",
      "            clip_model_gpu = clip_model.to(device)\n",
      "            \n",
      "            self.models[gpu_id] = text_model\n",
      "            self.image_adapters[gpu_id] = image_adapter\n",
      "            self.clip_models[gpu_id] = clip_model_gpu\n",
      "        \n",
      "        return self.models[gpu_id], self.image_adapters[gpu_id], self.clip_models[gpu_id]\n",
      "\n",
      "    def load_model(self, device):\n",
      "        if self.use_4bit:\n",
      "            quantization_config = BitsAndBytesConfig(\n",
      "                load_in_4bit=True,\n",
      "                bnb_4bit_compute_dtype=torch.bfloat16\n",
      "            )\n",
      "        else:\n",
      "            quantization_config = BitsAndBytesConfig(\n",
      "                load_in_4bit=False\n",
      "            )\n",
      "\n",
      "        model = AutoModelForCausalLM.from_pretrained(\n",
      "            MODEL_PATH,\n",
      "            device_map=device,\n",
      "            quantization_config=quantization_config,\n",
      "            torch_dtype=torch.bfloat16 if not self.use_4bit else None\n",
      "        )\n",
      "        return model.to(device)\n",
      "\n",
      "def open_folder(folder_path):\n",
      "    if platform.system() == \"Windows\":\n",
      "        os.startfile(folder_path)\n",
      "    elif platform.system() == \"Linux\":\n",
      "        os.system(f'xdg-open \"{folder_path}\"')\n",
      "    elif platform.system() == \"Darwin\":  # macOS\n",
      "        os.system(f'open \"{folder_path}\"')\n",
      "\n",
      "def convert_to_png(image_path):\n",
      "    try:\n",
      "        with Image.open(image_path) as img:\n",
      "            if img.mode not in ('RGB', 'RGBA'):\n",
      "                img = img.convert('RGB')\n",
      "            elif img.mode == 'RGBA':\n",
      "                bg = Image.new('RGB', img.size, (255, 255, 255))\n",
      "                bg.paste(img, mask=img.split()[3])\n",
      "                img = bg\n",
      "            \n",
      "            img.thumbnail((1024, 1024))\n",
      "            img_array = np.array(img)\n",
      "            \n",
      "            return img_array\n",
      "    except Exception as e:\n",
      "        print(f\"Error converting image {image_path}: {str(e)}\")\n",
      "        return None\n",
      "\n",
      "def cut_off_last_sentence(caption):\n",
      "    sentence_endings = re.findall(r'[.!?]', caption)\n",
      "    if sentence_endings:\n",
      "        last_ending_index = caption.rfind(sentence_endings[-1])\n",
      "        return caption[:last_ending_index + 1].strip()\n",
      "    return caption\n",
      "\n",
      "def remove_this_is(caption):\n",
      "    caption = caption.replace(\"|.\", \"\")  # Remove all \"|.\"\n",
      "    caption = \" \".join(caption.split())  # Trim multiple spaces to single space\n",
      "    caption = caption.strip()  # Trim leading and trailing spaces\n",
      "    if caption.lower().startswith(\"this is \"):\n",
      "        caption = caption[8:]  # Remove \"this is \" from the beginning\n",
      "    return caption\n",
      "\n",
      "@torch.no_grad()\n",
      "def generate_caption(input_image: np.ndarray, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, device, text_model, image_adapter, clip_model, use_4bit):\n",
      "    torch.cuda.empty_cache()\n",
      "\n",
      "    image = clip_processor(images=input_image, return_tensors='pt').pixel_values.to(device)\n",
      "\n",
      "    prompt = tokenizer.encode(VLM_PROMPT, return_tensors='pt', padding=padding, truncation=truncation, add_special_tokens=add_special_tokens).to(device)\n",
      "\n",
      "    with torch.amp.autocast_mode.autocast(device_type='cuda', enabled=True):\n",
      "        vision_outputs = clip_model(pixel_values=image, output_hidden_states=True)\n",
      "        image_features = vision_outputs.hidden_states[-2]\n",
      "        embedded_images = image_adapter(image_features)\n",
      "    \n",
      "    prompt_embeds = text_model.model.embed_tokens(prompt)\n",
      "    assert prompt_embeds.shape == (1, prompt.shape[1], text_model.config.hidden_size), f\"Prompt shape is {prompt_embeds.shape}, expected {(1, prompt.shape[1], text_model.config.hidden_size)}\"\n",
      "    embedded_bos = text_model.model.embed_tokens(torch.tensor([[tokenizer.bos_token_id]], device=device, dtype=torch.int64))\n",
      "\n",
      "    # Determine the target dtype based on 4-bit quantization setting\n",
      "    target_dtype = torch.float16 if use_4bit else torch.bfloat16\n",
      "\n",
      "    # Ensure all tensors are on the same device and have the same dtype before concatenation\n",
      "    embedded_bos = embedded_bos.to(device).to(target_dtype)\n",
      "    embedded_images = embedded_images.to(device).to(target_dtype)\n",
      "    prompt_embeds = prompt_embeds.to(device).to(target_dtype)\n",
      "\n",
      "    inputs_embeds = torch.cat([\n",
      "        embedded_bos.expand(embedded_images.shape[0], -1, -1),\n",
      "        embedded_images,\n",
      "        prompt_embeds.expand(embedded_images.shape[0], -1, -1),\n",
      "    ], dim=1)\n",
      "\n",
      "    input_ids = torch.cat([\n",
      "        torch.tensor([[tokenizer.bos_token_id]], dtype=torch.long, device=device),\n",
      "        torch.zeros((1, embedded_images.shape[1]), dtype=torch.long, device=device),\n",
      "        prompt,\n",
      "    ], dim=1)\n",
      "    attention_mask = torch.ones_like(input_ids)\n",
      "\n",
      "    # Ensure inputs_embeds is in the correct dtype\n",
      "    inputs_embeds = inputs_embeds.to(target_dtype)\n",
      "\n",
      "    # Use autocast for mixed precision handling\n",
      "    with torch.cuda.amp.autocast(enabled=use_4bit):\n",
      "        generate_ids = text_model.generate(\n",
      "            input_ids,\n",
      "            inputs_embeds=inputs_embeds,\n",
      "            attention_mask=attention_mask,\n",
      "            max_new_tokens=max_new_tokens,\n",
      "            do_sample=do_sample,\n",
      "            top_k=top_k,\n",
      "            temperature=temperature,\n",
      "            suppress_tokens=None\n",
      "        )\n",
      "\n",
      "    generate_ids = generate_ids[:, input_ids.shape[1]:]\n",
      "    if generate_ids[0][-1] == tokenizer.eos_token_id:\n",
      "        generate_ids = generate_ids[:, :-1]\n",
      "\n",
      "    caption = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "    \n",
      "    caption = remove_this_is(caption.strip())\n",
      "    \n",
      "    if cut_off_sentence:\n",
      "        caption = cut_off_last_sentence(caption)\n",
      "    \n",
      "    return caption\n",
      "\n",
      "def save_caption(caption_path, caption, overwrite, append, remove_newlines):\n",
      "    if remove_newlines:\n",
      "        caption = ' '.join(caption.split())\n",
      "\n",
      "    os.makedirs(os.path.dirname(caption_path), exist_ok=True)\n",
      "\n",
      "    if os.path.exists(caption_path):\n",
      "        if overwrite:\n",
      "            mode = 'w'\n",
      "        elif append:\n",
      "            mode = 'a'\n",
      "        else:\n",
      "            return None\n",
      "    else:\n",
      "        mode = 'w'\n",
      "\n",
      "    try:\n",
      "        with open(caption_path, mode, encoding='utf-8') as f:\n",
      "            if mode == 'a':\n",
      "                f.write('\\n')\n",
      "            f.write(caption)\n",
      "        return caption_path\n",
      "    except Exception as e:\n",
      "        print(f\"Error saving caption: {e}\")\n",
      "        return None\n",
      "\n",
      "def process_image(image_path, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, output_folder, gpu_id, model_manager):\n",
      "    os.makedirs(output_folder, exist_ok=True)\n",
      "\n",
      "    output_image_path = os.path.join(output_folder, os.path.basename(image_path))\n",
      "    save_path = os.path.splitext(output_image_path)[0] + '.txt'\n",
      "    \n",
      "    if os.path.exists(save_path) and not overwrite and not append:\n",
      "        print(f\"Skipped {image_path} - caption already exists.\")\n",
      "        return None, None, 0, output_image_path\n",
      "\n",
      "    start_time = time.time()\n",
      "    print(f\"Processing {image_path} on GPU {gpu_id}...\")\n",
      "    try:\n",
      "        image_array = convert_to_png(image_path)\n",
      "        if image_array is None:\n",
      "            raise ValueError(\"Failed to convert image\")\n",
      "        \n",
      "        device = f\"cuda:{gpu_id}\"\n",
      "        text_model, image_adapter, clip_model = model_manager.get_models(gpu_id)\n",
      "        \n",
      "        # Ensure all models are on the correct device\n",
      "        text_model = text_model.to(device)\n",
      "        image_adapter = image_adapter.to(device)\n",
      "        clip_model = clip_model.to(device)\n",
      "        \n",
      "        caption = generate_caption(image_array, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, device, text_model, image_adapter, clip_model, model_manager.use_4bit)\n",
      "        if remove_newlines:\n",
      "            caption = ' '.join(caption.split())\n",
      "    \n",
      "        actual_save_path = save_caption(save_path, caption, overwrite, append, remove_newlines)\n",
      "\n",
      "        Image.fromarray(image_array).save(output_image_path)\n",
      "\n",
      "        process_time = time.time() - start_time\n",
      "        print(f\"Processed {image_path} on GPU {gpu_id} in {process_time:.2f} seconds.\")\n",
      "        return caption, actual_save_path, process_time, output_image_path\n",
      "    except Exception as e:\n",
      "        error_message = f\"Error processing image {image_path} on GPU {gpu_id}: {str(e)}\\n\"\n",
      "        error_message += f\"Image shape: {image_array.shape if image_array is not None else 'Unknown'}\"\n",
      "        print(error_message)\n",
      "        return None, None, 0, image_path\n",
      "\n",
      "def file_exists(file_path):\n",
      "    exists = os.path.isfile(file_path)\n",
      "    size = os.path.getsize(file_path) if exists else 0\n",
      "    return exists, size\n",
      "\n",
      "def prepare_gpu_plan(input_folder, output_folder, overwrite, append, gpu_ids):\n",
      "    image_files = glob.glob(os.path.join(input_folder, '*.*'))\n",
      "    image_files = [f for f in image_files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp'))]\n",
      "    \n",
      "    if not overwrite and not append:\n",
      "        image_files = [f for f in image_files if not os.path.exists(os.path.join(output_folder, os.path.splitext(os.path.basename(f))[0] + '.txt'))]\n",
      "    \n",
      "    gpu_list = [int(gpu.strip()) for gpu in gpu_ids.split(',') if gpu.strip()]\n",
      "    num_gpus = len(gpu_list)\n",
      "    \n",
      "    images_per_gpu = len(image_files) // num_gpus\n",
      "    remainder = len(image_files) % num_gpus\n",
      "    \n",
      "    gpu_plans = []\n",
      "    start = 0\n",
      "    for i, gpu in enumerate(gpu_list):\n",
      "        end = start + images_per_gpu + (1 if i < remainder else 0)\n",
      "        gpu_plans.append((gpu, image_files[start:end]))\n",
      "        start = end\n",
      "    \n",
      "    return gpu_plans\n",
      "\n",
      "def process_gpu_batch(gpu_id, image_files, output_folder, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, model_manager, progress_callback=None):\n",
      "    total_images = len(image_files)\n",
      "    processed = 0\n",
      "    skipped = 0\n",
      "    start_time = time.time()\n",
      "    \n",
      "    print(f\"GPU {gpu_id}: Starting batch processing of {total_images} images.\")\n",
      "    \n",
      "    for image_file in image_files:\n",
      "        result = process_image(image_file, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, output_folder, gpu_id, model_manager)\n",
      "        \n",
      "        if result[0] is None:\n",
      "            skipped += 1\n",
      "        else:\n",
      "            processed += 1\n",
      "        \n",
      "        elapsed = time.time() - start_time\n",
      "        avg_speed = elapsed / (processed + skipped) * 1000  # in milliseconds\n",
      "        remaining = total_images - processed - skipped\n",
      "        estimated_time = remaining * (elapsed / (processed + skipped))\n",
      "        \n",
      "        progress_message = f\"GPU {gpu_id} status: {processed} processed, {skipped} skipped, {remaining} left, average speed {avg_speed:.2f} ms/image, estimated time {estimated_time // 60:.0f} min {estimated_time % 60:.0f} seconds\\n\"\n",
      "        print(progress_message, end='')\n",
      "        sys.stdout.flush()\n",
      "        \n",
      "        if progress_callback:\n",
      "            progress_callback(progress_message)\n",
      "\n",
      "    print(f\"GPU {gpu_id}: Finished processing {processed} images, skipped {skipped} images.\")\n",
      "\n",
      "def multi_gpu_process(gpu_plans, output_folder, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, model_manager, progress_callback=None):\n",
      "    with ThreadPoolExecutor(max_workers=len(gpu_plans)) as executor:\n",
      "        futures = []\n",
      "        for gpu_id, image_files in gpu_plans:\n",
      "            future = executor.submit(process_gpu_batch, gpu_id, image_files, output_folder, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, model_manager, progress_callback)\n",
      "            futures.append(future)\n",
      "        \n",
      "        for future in futures:\n",
      "            future.result()\n",
      "\n",
      "def batch_process(input_folder, output_folder, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, gpu_ids, model_manager, progress_callback=None):\n",
      "    if not output_folder:\n",
      "        output_folder = input_folder\n",
      "    \n",
      "    gpu_plans = prepare_gpu_plan(input_folder, output_folder, overwrite, append, gpu_ids)\n",
      "    total_images = sum(len(plan[1]) for plan in gpu_plans)\n",
      "    \n",
      "    progress = f\"Starting batch processing. Found {total_images} images.\\n\"\n",
      "    progress += f\"Input folder: {input_folder}\\n\"\n",
      "    progress += f\"Output folder: {output_folder}\\n\"\n",
      "    print(progress)\n",
      "    if progress_callback:\n",
      "        progress_callback(progress)\n",
      "    \n",
      "    multi_gpu_process(gpu_plans, output_folder, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, model_manager, progress_callback)\n",
      "    \n",
      "    final_message = f\"Batch processing complete. Processed {total_images} images.\\n\"\n",
      "    print(final_message)\n",
      "    if progress_callback:\n",
      "        progress_callback(final_message)\n",
      "    return progress + final_message\n",
      "\n",
      "def gradio_interface(image_input, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, model_manager):\n",
      "    if isinstance(image_input, dict):\n",
      "        image_path = image_input['path']\n",
      "    else:\n",
      "        image_path = image_input\n",
      "    \n",
      "    gpu_id = model_manager.default_gpu  # Use the default GPU for single image processing\n",
      "    output_folder = \"outputs\"\n",
      "    output_image_path = os.path.join(output_folder, os.path.basename(image_path))\n",
      "    save_path = os.path.splitext(output_image_path)[0] + '.txt'\n",
      "\n",
      "    # Check if the caption file already exists\n",
      "    if os.path.exists(save_path) and not overwrite and not append:\n",
      "        return \"Skipped\", f\"Skipped captioning for {image_path} - caption file already exists and neither overwrite nor append was selected.\\nProcessing time: 0 seconds\"\n",
      "\n",
      "    caption, actual_save_path, process_time, output_image_path = process_image(image_path, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, output_folder, gpu_id, model_manager)\n",
      "\n",
      "    if caption is None:\n",
      "        save_info = f\"Failed to generate caption for {image_path}.\"\n",
      "        processing_info = f\"Processing time: {process_time:.2f} seconds\"\n",
      "    else:\n",
      "        if actual_save_path:\n",
      "            save_info = f\"Caption saved to: {actual_save_path}\"\n",
      "        else:\n",
      "            save_info = f\"Caption could not be saved. Attempted path: {os.path.splitext(output_image_path)[0] + '.txt'}\"\n",
      "        \n",
      "        processing_info = f\"Processing time: {process_time:.2f} seconds\"\n",
      "    \n",
      "    return caption or \"Failed\", f\"{save_info}\\n{processing_info}\"\n",
      "\n",
      "def main():\n",
      "    parser = argparse.ArgumentParser(description=\"SECourses JoyCaption Image Captioning App V11\")\n",
      "    parser.add_argument(\"--share\", action=\"store_true\", help=\"Use Gradio's share feature\")\n",
      "    parser.add_argument(\"--use_4bit\", action=\"store_true\", help=\"Use 4-bit quantization\")\n",
      "    global args\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    model_manager = ModelManager(args.use_4bit)\n",
      "\n",
      "    with gr.Blocks() as demo:\n",
      "        gr.HTML(TITLE)\n",
      "        with gr.Row():\n",
      "            with gr.Column():\n",
      "                input_image = gr.Image(type=\"filepath\", label=\"Input Image\", height=512)\n",
      "                overwrite = gr.Checkbox(label=\"Overwrite existing caption file\", value=False)\n",
      "                append = gr.Checkbox(label=\"Append new caption to existing caption\", value=False)\n",
      "                remove_newlines = gr.Checkbox(label=\"Remove newlines from generated captions\", value=True)\n",
      "                cut_off_sentence = gr.Checkbox(label=\"Cut off at last complete sentence\", value=True)\n",
      "                run_button = gr.Button(\"Caption\")\n",
      "            \n",
      "            with gr.Column():\n",
      "                output_caption = gr.Textbox(label=\"Caption\")\n",
      "                save_info = gr.Textbox(label=\"Save Information and Processing Time\")\n",
      "                max_new_tokens = gr.Slider(minimum=1, maximum=1000, value=300, step=1, label=\"Max New Tokens\")\n",
      "                do_sample = gr.Checkbox(label=\"Do Sample\", value=False)\n",
      "                top_k = gr.Slider(minimum=1, maximum=100, value=10, step=1, label=\"Top K\")\n",
      "                temperature = gr.Slider(minimum=0.1, maximum=2.0, value=0.5, step=0.1, label=\"Temperature\")\n",
      "                padding = gr.Checkbox(label=\"Padding\", value=False)\n",
      "                truncation = gr.Checkbox(label=\"Truncation\", value=False)\n",
      "                add_special_tokens = gr.Checkbox(label=\"Add Special Tokens\", value=False)\n",
      "        \n",
      "        with gr.Row():\n",
      "            input_folder = gr.Textbox(label=\"Input Folder for Batch Processing\")\n",
      "            output_folder = gr.Textbox(label=\"Output Folder for Batch Processing (Optional)\")\n",
      "            gpu_ids = gr.Textbox(label=\"GPU IDs (comma-separated, e.g., 0,1,2)\", value=\"0\")\n",
      "            batch_button = gr.Button(\"Start Batch Processing\")\n",
      "        \n",
      "        batch_progress = gr.Textbox(label=\"Batch Processing Progress\", lines=20)\n",
      "        \n",
      "        open_outputs_button = gr.Button(\"Open Results Folder\")\n",
      "        \n",
      "        run_button.click(fn=lambda *args: gradio_interface(*args, model_manager), \n",
      "                         inputs=[input_image, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence], \n",
      "                         outputs=[output_caption, save_info])\n",
      "        \n",
      "        def batch_process_with_progress(*args):\n",
      "            progress = \"\"\n",
      "            def update_progress(msg):\n",
      "                nonlocal progress\n",
      "                progress += msg\n",
      "                return progress\n",
      "            \n",
      "            return batch_process(*args, model_manager=model_manager, progress_callback=update_progress)\n",
      "        \n",
      "        batch_button.click(fn=batch_process_with_progress, \n",
      "                           inputs=[input_folder, output_folder, overwrite, append, remove_newlines, max_new_tokens, do_sample, top_k, temperature, padding, truncation, add_special_tokens, cut_off_sentence, gpu_ids],\n",
      "                           outputs=batch_progress)\n",
      "        \n",
      "        open_outputs_button.click(fn=lambda: open_folder(output_folder.value or input_folder.value or \"outputs\"))\n",
      "\n",
      "    demo.launch(share=args.share, inbrowser=True)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()Who can help?@SunMarc@Narsil\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_455.txt:\n",
      "Title: Add support for walltime-based saving/logging/evaluating\n",
      "URL: https://github.com/huggingface/transformers/issues/29984\n",
      "Body:\n",
      "Feature requestWe currently have the save strategiesepochorsteps. It would be useful to add one fortime, too. After every backward pass we check if a given time interval has passed. If it has, save a checkpoint and reset the timer.MotivationMotivation comes from usage on clusters where you have a job time limit. Youcanfirst do a test run and see how long a step takes on average and extrapolate from there, but relying on walltime would probably be easier.Your contributionI can work on this. I think a condition should be added to the defaultflowcallback (https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_callback.py#L432) to also include this new strategy. I am not sure yet how to track the starting time, though. Should it be passed separately and saved in thetrainerinstance? Or added toargs?In terms of implementation, a lot of inspiration can be taken fromhttps://twitter.com/StasBekman/status/1774842972795982160\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_284.txt:\n",
      "Title: Add support for MiniCPM-V-2 and MiniCPM-Llama3-V-2_5\n",
      "URL: https://github.com/huggingface/transformers/issues/31836\n",
      "Body:\n",
      "Model descriptionMiniCPM-V is a series of Openbmb's vision language models.We want to add support for MiniCPM-V-2 and later modelsOpen source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationhttps://huggingface.co/openbmb/MiniCPM-V-2https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_509.txt:\n",
      "Title: Please correct the following DeepSpeed config values that mismatch TrainingArguments values: scheduler.params.total_num_steps=0 vs hf num_training_steps (calculated)= 260\n",
      "URL: https://github.com/huggingface/transformers/issues/29348\n",
      "Body:\n",
      "System Infotransformersversion: 4.36.2Platform: Linux-4.15.0-213-generic-x86_64-with-glibc2.27Python version: 3.9.18Huggingface_hub version: 0.21.1Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config:    not foundPyTorch version (GPU?): 2.0.1+cu117 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionraise ValueError(ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:ds scheduler.params.total_num_steps=0 vs hf num_training_steps (calculated)=260The easiest method is to set these DeepSpeed config values to 'auto'.When I use transformers==4.28.1 + deepspeed==0.13.3 for Llama2 fine-tuning, the code runs normally and training is completed. This error occurs when I upgrade transformers to 4.36.x, 4.37.x or 4.38.1 respectively.And I have not modified the default_offload_opt_param.json file of deepspeed. The contents of the file are as follows:{\"bf16\": {\"enabled\":\"auto\"},\"optimizer\": {\"type\":\"AdamW\",\"params\": {\"lr\":\"auto\",\"betas\":\"auto\",\"eps\":\"auto\",\"weight_decay\":\"auto\"}\n",
      "  },\"scheduler\": {\"type\":\"WarmupDecayLR\",\"params\": {\"total_num_steps\":\"auto\",\"warmup_min_lr\":\"auto\",\"warmup_max_lr\":\"auto\",\"warmup_num_steps\":\"auto\"}\n",
      "  },\"zero_optimization\": {\"stage\":3,\"offload_optimizer\": {\"device\":\"cpu\",\"pin_memory\":true},\"offload_param\": {\"device\":\"cpu\",\"pin_memory\":true},\"overlap_comm\":true,\"contiguous_gradients\":true,\"sub_group_size\":1e9,\"reduce_bucket_size\":\"auto\",\"stage3_prefetch_bucket_size\":\"auto\",\"stage3_param_persistence_threshold\":\"auto\",\"stage3_max_live_parameters\":1e9,\"stage3_max_reuse_distance\":1e9,\"stage3_gather_16bit_weights_on_model_save\":true},\"gradient_accumulation_steps\":\"auto\",\"gradient_clipping\":\"auto\",\"steps_per_print\":5,\"train_batch_size\":\"auto\",\"train_micro_batch_size_per_gpu\":\"auto\",\"wall_clock_breakdown\":false}The value of scheduler.params.total_num_steps is always \"auto\".Expected behaviorplease fix this bug\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_247.txt:\n",
      "Title: TF Lite model created from TFWhisperForConditionalGeneration.from_pretrained craches\n",
      "URL: https://github.com/huggingface/transformers/issues/32125\n",
      "Body:\n",
      "System InfoThe main branch still does not include the fix:#19691 (comment)namely#new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float(\"inf\")\n",
      "    new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float(1)which causes TFLite Whisper model to crash when used as:interpreter = tf.lite.Interpreter(tflite_model_path)\n",
      "    tflite_generate = interpreter.get_signature_runner()\n",
      "    output = tflite_generate(input_features=input_features)Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionclass GenerateModel(tf.Module):\n",
      "    def __init__(self, model, forced_decoder_ids):\n",
      "        super(GenerateModel, self).__init__()\n",
      "        self.model = model\n",
      "        self.forced_decoder_ids = forced_decoder_ids\n",
      "\n",
      "    @tf.function(\n",
      "        # shouldn't need static batch size, but throws exception without it (needs to be fixed)\n",
      "        input_signature=[\n",
      "            tf.TensorSpec((1, 80, 3000), tf.float32, name=\"input_features\"),\n",
      "        ],\n",
      "    )\n",
      "    def serving(self, input_features):\n",
      "        outputs = self.model.generate(\n",
      "            input_features,\n",
      "            forced_decoder_ids=self.forced_decoder_ids,\n",
      "            #max_new_tokens=223,  # change as needed\n",
      "            return_dict_in_generate=True,\n",
      "        )\n",
      "        return {\"sequences\": outputs[\"sequences\"]}\n",
      "\n",
      "model = TFWhisperForConditionalGeneration.from_pretrained(model_name)\n",
      "generate_model = GenerateModel(model=model, forced_decoder_ids=forced_decoder_ids)\n",
      "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})\n",
      "\n",
      "# Convert the model\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
      "converter.target_spec.supported_ops = [\n",
      "        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
      "        tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
      "]\n",
      "\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
      "tflite_model = converter.convert()\n",
      "\n",
      "# Save the model\n",
      "with open(tflite_model_path, 'wb') as f:\n",
      "        f.write(tflite_model)\n",
      "\n",
      "....\n",
      "    interpreter = tf.lite.Interpreter(tflite_model_path)\n",
      "    tflite_generate = interpreter.get_signature_runner()\n",
      "    output = tflite_generate(input_features=input_features)causes crache on tflite_generate.Applying the patch#19691 (comment)solves the problemExpected behaviorThe code below should work without patchclass GenerateModel(tf.Module):\n",
      "    def __init__(self, model, forced_decoder_ids):\n",
      "        super(GenerateModel, self).__init__()\n",
      "        self.model = model\n",
      "        self.forced_decoder_ids = forced_decoder_ids\n",
      "\n",
      "    @tf.function(\n",
      "        # shouldn't need static batch size, but throws exception without it (needs to be fixed)\n",
      "        input_signature=[\n",
      "            tf.TensorSpec((1, 80, 3000), tf.float32, name=\"input_features\"),\n",
      "        ],\n",
      "    )\n",
      "    def serving(self, input_features):\n",
      "        outputs = self.model.generate(\n",
      "            input_features,\n",
      "            forced_decoder_ids=self.forced_decoder_ids,\n",
      "            #max_new_tokens=223,  # change as needed\n",
      "            return_dict_in_generate=True,\n",
      "        )\n",
      "        return {\"sequences\": outputs[\"sequences\"]}\n",
      "\n",
      "model = TFWhisperForConditionalGeneration.from_pretrained(model_name)\n",
      "generate_model = GenerateModel(model=model, forced_decoder_ids=forced_decoder_ids)\n",
      "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})\n",
      "\n",
      "# Convert the model\n",
      "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
      "converter.target_spec.supported_ops = [\n",
      "        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
      "        tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
      "]\n",
      "\n",
      "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
      "tflite_model = converter.convert()\n",
      "\n",
      "# Save the model\n",
      "with open(tflite_model_path, 'wb') as f:\n",
      "        f.write(tflite_model)\n",
      "\n",
      "....\n",
      "    interpreter = tf.lite.Interpreter(tflite_model_path)\n",
      "    tflite_generate = interpreter.get_signature_runner()\n",
      "    output = tflite_generate(input_features=input_features)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_521.txt:\n",
      "Title: [tokenizer] Inconsistent behavior in slow tokenizer and fast tokenizer\n",
      "URL: https://github.com/huggingface/transformers/issues/29159\n",
      "Body:\n",
      "System Infotransformersversion: 4.35.2Platform: Linux-5.4.0-163-generic-x86_64-with-glibc2.10Python version: 3.8.18Huggingface_hub version: 0.19.4Safetensors version: 0.4.1Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.1.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: no needUsing distributed or parallel set-up in script?: no needWho can help?@ArthurZuckerand@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionfromtransformersimportAutoTokenizerdefanswer_or_exception(tokenizer,id):print(f'<<<<<<{tokenizer.__class__}>>>>>>')try:print(f'\"{tokenizer.decode([id])}\"')exceptExceptionase:print(e)tokenizer=AutoTokenizer.from_pretrained(\"/mnt/data01/shichao/models/phi-2\",trust_remote_code=True,use_fast=False)# vocab size: 50294answer_or_exception(tokenizer,50294)# correctanswer_or_exception(tokenizer,50295)# wrongtokenizer=AutoTokenizer.from_pretrained(\"/mnt/data01/shichao/models/phi-2\",trust_remote_code=True,use_fast=True)# vocab size: 50294answer_or_exception(tokenizer,50294)# correctanswer_or_exception(tokenizer,50295)# correcttokenizer=AutoTokenizer.from_pretrained(\"/mnt/data01/shichao/models/Llama-2-7b-chat-hf\",trust_remote_code=True,use_fast=False)# vocab size: 31999answer_or_exception(tokenizer,31999)# correctanswer_or_exception(tokenizer,32000)# wrongtokenizer=AutoTokenizer.from_pretrained(\"/mnt/data01/shichao/models/Llama-2-7b-chat-hf\",trust_remote_code=True,use_fast=True)# vocab size: 31999answer_or_exception(tokenizer,31999)# correctanswer_or_exception(tokenizer,32000)# correctOutput:Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<<<<<<<class 'transformers.models.codegen.tokenization_codegen.CodeGenTokenizer'>>>>>>>\n",
      "\"               \"\n",
      "<<<<<<<class 'transformers.models.codegen.tokenization_codegen.CodeGenTokenizer'>>>>>>>\n",
      "sequence item 0: expected str instance, NoneType found\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "<<<<<<<class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>>>>>>>\n",
      "\"               \"\n",
      "<<<<<<<class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>>>>>>>\n",
      "\"\"\n",
      "<<<<<<<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>>>>>>>\n",
      "\"给\"\n",
      "<<<<<<<class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>>>>>>>\n",
      "piece id is out of range.\n",
      "<<<<<<<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>>>>>>>\n",
      "\"给\"\n",
      "<<<<<<<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>>>>>>>\n",
      "\"\"Expected behaviorConsistentdecodebehavior in slow tokenizer and fast tokenizer when id exceeds vocab size. For example, instead of raise exceptions, the slow tokenizer output empty strings like the fast tokenizer does.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_253.txt:\n",
      "Title: Unrecognized configuration class ChameleonConfig\n",
      "URL: https://github.com/huggingface/transformers/issues/32098\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.0.dev0Platform: Linux-5.19.0-0_fbk10_hardened_11465_g45ce90b62da2-x86_64-with-glibc2.34Python version: 3.10.12Huggingface_hub version: 0.24.0Safetensors version: 0.4.3Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.3.1+cu121 (False)Tensorflow version (GPU?): 2.13.0 (False)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: NOWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun the following code:from transformers import AutoProcessor, AutoModelForCausalLM\n",
      "processor = AutoProcessor.from_pretrained(\"facebook/chameleon-7b\", token=\"blah\")\n",
      "model = AutoModelForCausalLM.from_pretrained(\"facebook/chameleon-7b\", token=\"blah\")Expected behaviorShould run without errors, instead, we get this:2024-07-18 17:28:52.471308: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-18 17:28:52.536932: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 17:28:53.558868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "preprocessor_config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 460/460 [00:00<00:00, 3.03MB/s]\n",
      "Some kwargs in processor config are unused and will not have any effect: image_token, image_seq_length.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sviyer/test_download.py\", line 4, in <module>\n",
      "    model = AutoModelForCausalLM.from_pretrained(\"facebook/chameleon-7b\", token=\"blah\")\n",
      "  File \"/home/sviyer/anaconda3/envs/p310/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 567, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: Unrecognized configuration class <class 'transformers.models.chameleon.configuration_chameleon.ChameleonConfig'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "Model type should be one of BartConfig, ......\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_3.txt:\n",
      "Title: Fast vs. slow tokenizer mismatch for special tokens with lstrip/rstrip set to true\n",
      "URL: https://github.com/huggingface/transformers/issues/33371\n",
      "Body:\n",
      "System Infotransformersversion: 4.45.0.dev0Platform: Linux-6.5.0-1025-azure-x86_64-with-glibc2.31Python version: 3.10.13Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0+cu121 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionCode to reproduce:fromtransformersimportAutoTokenizerslow=AutoTokenizer.from_pretrained(\"distil-whisper/distil-small.en\",use_fast=False)fast=AutoTokenizer.from_pretrained(\"distil-whisper/distil-small.en\",use_fast=True)text=\"   <|startoftranscript|> <|en|>   \"print(slow.tokenize(text))# ['<|startoftranscript|>', '<|en|>']print(fast.tokenize(text))# ['   <|startoftranscript|> ', '<|en|>   ']print(slow.encode(text))# [50257, 50362, 50257, 50258, 50256]print(fast.encode(text))# [50257, 50362, 50257, 50258, 50256]Although the token ids match (.encode), the tokenized strings (.tokenize) are different.Expected behaviorIntuitively, I would expect.tokenizeto have the lstrip and rstrip applied (i.e., the same as the slow tokenizer), so this may be an issue withtokenizers(cc@Narsil)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_938.txt:\n",
      "Title: Long-Short Transformer\n",
      "URL: https://github.com/huggingface/transformers/issues/12635\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_737.txt:\n",
      "Title: Issue Loading 4-bit and 8-bit language models: ValueError:.tois not supported for4-bitor8-bitmodels. Please use the model as it is, since the model has already been set to the correct devices and casted to the correctdtype.\n",
      "URL: https://github.com/huggingface/transformers/issues/24540\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_736.txt:\n",
      "Title: Add HyenaDNA model\n",
      "URL: https://github.com/huggingface/transformers/issues/24659\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_2.txt:\n",
      "Title: Any plans on adding Flash Attention 3?\n",
      "URL: https://github.com/huggingface/transformers/issues/33373\n",
      "Body:\n",
      "As title\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_534.txt:\n",
      "Title: Whisper Sequential long-form decoding doesn't work with timestamps per token\n",
      "URL: https://github.com/huggingface/transformers/issues/28977\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.2Platform: Linux-4.15.0-142-generic-x86_64-with-glibc2.23Python version: 3.10.11Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.0.1 (True)Tensorflow version (GPU?): 2.12.0 (True)Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionFollowing[Whisper] Add sequential longform decoding, it seems that there is an issue when asking for token timestamps when dealing with the new way of handling long-form transcriptions.If usingmodel.generate()method, passingreturn_token_timestamps=Truecauses the issue. Occurs also with the pipeline object if settingreturn_timestamps=\"word\".Here is a simple example to reproduce the issue:fromtransformersimportWhisperForConditionalGeneration,WhisperProcessor,pipelineimportlibrosaSR=16000model=WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")processor=WhisperProcessor.from_pretrained(\"openai/whisper-medium\")file_path=\"path_to_more_than_30_sec_audio\"audio,_=librosa.load(file_path,sr=SR)# Long-form transcription with model.generate()input_features=processor(audio,sampling_rate=SR,return_tensors=\"pt\",truncation=False,# False so the audio isn't truncated and whole audio is sent to the modelreturn_attention_mask=True,padding=\"longest\")predicted_ids=model.generate(**input_features,return_token_timestamps=True)# With pipelinepipe=pipeline(\"automatic-speech-recognition\",model=model,tokenizer=processor.tokenizer,feature_extractor=processor.feature_extractor,return_timestamps=\"word\",return_language=True)pipe(audio)Traceback:AttributeError                            Traceback (most recent call last)\n",
      "Cell In[26], line 19\n",
      "     11#Long-form generation12 input_features = processor(audio, \n",
      "     13                            sampling_rate=16000, \n",
      "     14                            return_tensors=\"pt\", \n",
      "     15                            truncation=False, \n",
      "     16                            return_attention_mask=True, \n",
      "     17                            padding=\"longest\")\n",
      "--->19 predicted_ids = model.generate(**input_features,\n",
      "     20                                return_token_timestamps=True)\n",
      "\n",
      "File~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:641,inWhisperGenerationMixin.generate(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate,**kwargs)\n",
      "    638         proc.set_begin_index(decoder_input_ids.shape[-1])\n",
      "    640#6.8 Run generate with fallback-->641 seek_sequences, seek_outputs, should_skip, do_condition_on_prev_tokens = self.generate_with_fallback(\n",
      "    642     segment_input=segment_input,\n",
      "    643     decoder_input_ids=decoder_input_ids,\n",
      "    644     cur_bsz=cur_bsz,\n",
      "    645     batch_idx_map=batch_idx_map,\n",
      "    646     seek=seek,\n",
      "    647     num_segment_frames=num_segment_frames,\n",
      "    648     max_frames=max_frames,\n",
      "    649     temperatures=temperatures,\n",
      "    650     generation_config=generation_config,\n",
      "    651     logits_processor=logits_processor,\n",
      "    652     stopping_criteria=stopping_criteria,\n",
      "    653     prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
      "    654     synced_gpus=synced_gpus,\n",
      "    655     return_token_timestamps=return_token_timestamps,\n",
      "    656     do_condition_on_prev_tokens=do_condition_on_prev_tokens,\n",
      "    657     kwargs=kwargs,\n",
      "    658 )\n",
      "    660#6.9 In every generated sequence, split by timestamp tokens and extract segments661fori, seek_sequenceinenumerate(seek_sequences):\n",
      "\n",
      "File~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:739,inWhisperGenerationMixin.generate_with_fallback(self, segment_input, decoder_input_ids, cur_bsz, batch_idx_map, seek, num_segment_frames, max_frames, temperatures, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_token_timestamps, do_condition_on_prev_tokens, kwargs)\n",
      "    727 seek_outputs =super().generate(\n",
      "    728     segment_input,\n",
      "    729     generation_config,\n",
      "   (...)\n",
      "    735**kwargs,\n",
      "    736 )\n",
      "    738#post-process sequence tokens and outputs to be in list form-->739 sequence_tokens, seek_outputs = self._postprocess_outputs(\n",
      "    740     seek_outputs, return_token_timestamps, generation_config\n",
      "    741 )\n",
      "    743#remove all previously passed decoder input ids744 seek_sequences = sequence_tokens[:, decoder_input_ids.shape[-1] :]\n",
      "\n",
      "File~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:825,inWhisperGenerationMixin._postprocess_outputs(self, seek_outputs, return_token_timestamps, generation_config)\n",
      "    822returnvalues[batch_idx].cpu()\n",
      "    824     sequence_tokens = seek_outputs[\"sequences\"]\n",
      "-->825     seek_outputs = [\n",
      "    826         {k: split_by_batch_index(v, k, i)fork, vinseek_outputs.items()}\n",
      "    827foriinrange(sequence_tokens.shape[0])\n",
      "    828     ]\n",
      "    829 else:\n",
      "    830     sequence_tokens = seek_outputs\n",
      "\n",
      "File~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:826,in<listcomp>(.0)822returnvalues[batch_idx].cpu()\n",
      "    824     sequence_tokens = seek_outputs[\"sequences\"]\n",
      "    825     seek_outputs = [\n",
      "-->826         {k: split_by_batch_index(v, k, i)fork, vinseek_outputs.items()}\n",
      "    827foriinrange(sequence_tokens.shape[0])\n",
      "    828     ]\n",
      "    829 else:\n",
      "    830     sequence_tokens = seek_outputs\n",
      "\n",
      "File~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:826,in<dictcomp>(.0)822returnvalues[batch_idx].cpu()\n",
      "    824     sequence_tokens = seek_outputs[\"sequences\"]\n",
      "    825     seek_outputs = [\n",
      "-->826         {k: split_by_batch_index(v, k, i)fork, vinseek_outputs.items()}\n",
      "    827foriinrange(sequence_tokens.shape[0])\n",
      "    828     ]\n",
      "    829 else:\n",
      "    830     sequence_tokens = seek_outputs\n",
      "\n",
      "File~/miniconda3/envs/py310-fast/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:822,inWhisperGenerationMixin._postprocess_outputs.<locals>.split_by_batch_index(values, key, batch_idx)\n",
      "    819ifkey ==\"past_key_values\":\n",
      "    820#we don't save `past_key_values` as this is too costly821returnNone\n",
      "-->822returnvalues[batch_idx].cpu()\n",
      "\n",
      "AttributeError:'tuple'object has no attribute'cpu'Works fine if you don't ask the timestamps per token.Expected behaviorModel should be able to return the timestamps per token when working with long audio after#27492\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_252.txt:\n",
      "Title: Using Trainer + a pretrained tokenizer + 4D attention mask is extremely slow\n",
      "URL: https://github.com/huggingface/transformers/issues/32101\n",
      "Body:\n",
      "System Infotransformers                  4.41.0Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import LlamaForCausalLM, LlamaConfig, TrainingArguments, Trainer, AutoTokenizer\n",
      "from datasets import IterableDataset\n",
      "import numpy as np\n",
      "\n",
      "model_config = LlamaConfig(\n",
      "    vocab_size=10,\n",
      "    hidden_size=384,\n",
      "    num_hidden_layers=6,\n",
      "    num_attention_heads=6,\n",
      "    intermediate_size=1024,\n",
      "    max_position_embeddings=1024,\n",
      ")\n",
      "model = LlamaForCausalLM(model_config)\n",
      "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
      "\n",
      "def get_data1():\n",
      "    for i in range(10000):\n",
      "        yield {'input_ids': np.zeros(1024, dtype=int), 'labels': np.zeros(1024, dtype=int), 'attention_mask': np.zeros((1, 1024, 1024), dtype=float)}\n",
      "\n",
      "def get_data2():\n",
      "    for i in range(10000):\n",
      "        yield {'input_ids': np.zeros(1024, dtype=int), 'labels': np.zeros(1024, dtype=int), 'attention_mask': np.zeros((1024), dtype=int)}\n",
      "    \n",
      "ds_slow = IterableDataset.from_generator(get_data1).with_format('torch')\n",
      "ds_fast = IterableDataset.from_generator(get_data2).with_format('torch')\n",
      "\n",
      "training_args = TrainingArguments(max_steps=1, output_dir='./out', report_to=None, per_device_train_batch_size=32, gradient_accumulation_steps=32)\n",
      "trainer1 = Trainer(model, training_args, train_dataset=ds_slow, tokenizer=tokenizer)\n",
      "trainer2 = Trainer(model, training_args, train_dataset=ds_fast, tokenizer=tokenizer)\n",
      "\n",
      "import cProfile\n",
      "cProfile.run('trainer1.train()', './test_slow.profile')\n",
      "cProfile.run('trainer2.train()', './test_fast.profile')import pstats\n",
      "\n",
      "# compare the two profiles\n",
      "p1 = pstats.Stats('./test_slow.profile')\n",
      "p2 = pstats.Stats('./test_fast.profile')\n",
      "p1.sort_stats('cumtime').print_stats()1582200 function calls (1401111 primitive calls) in 340.112 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000  340.112  340.112 {built-in method builtins.exec}\n",
      "        1    0.000    0.000  340.112  340.112 <string>:1(<module>)\n",
      "        1    0.000    0.000  340.112  340.112 trainer.py:1784(train)\n",
      "        1    0.017    0.017  340.112  340.112 trainer.py:1892(_inner_training_loop)\n",
      "       33    0.001    0.000  326.171    9.884 data_loader.py:663(__iter__)\n",
      "       33    0.001    0.000  325.473    9.863 data_loader.py:618(_fetch_batches)\n",
      " 2486/265    0.001    0.000  325.428    1.228 {built-in method builtins.next}\n",
      "       33    0.001    0.000  325.088    9.851 dataloader.py:625(__next__)\n",
      "       33    0.725    0.022  325.083    9.851 dataloader.py:672(_next_data)\n",
      "       33    0.002    0.000  323.988    9.818 fetch.py:24(fetch)\n",
      "       33    0.000    0.000  320.979    9.727 trainer_utils.py:807(__call__)\n",
      "       33    0.000    0.000  320.971    9.726 data_collator.py:270(__call__)\n",
      "       33   16.982    0.515  320.971    9.726 data_collator.py:52(pad_without_fast_tokenizer_warning)\n",
      "       33    0.005    0.000  303.989    9.212 tokenization_utils_base.py:3209(pad)\n",
      "     6493  235.747    0.036  235.747    0.036 {built-in method torch.tensor}\n",
      "      197    0.001    0.000  234.735    1.192 tokenization_utils_base.py:204(__init__)\n",
      "      197    0.001    0.000  234.732    1.192 tokenization_utils_base.py:681(convert_to_tensors)\n",
      "       99    0.000    0.000  234.730    2.371 tokenization_utils_base.py:718(as_tensor)p2.sort_stats('cumtime').print_stats()1567440 function calls (1386340 primitive calls) in 16.431 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   16.431   16.431 {built-in method builtins.exec}\n",
      "        1    0.000    0.000   16.431   16.431 <string>:1(<module>)\n",
      "        1    0.000    0.000   16.431   16.431 trainer.py:1784(train)\n",
      "        1    0.018    0.018   16.431   16.431 trainer.py:1892(_inner_training_loop)\n",
      "       32    0.003    0.000   14.327    0.448 trainer.py:3212(training_step)\n",
      "       32    0.001    0.000    8.830    0.276 accelerator.py:2093(backward)\n",
      "       32    0.000    0.000    8.829    0.276 _tensor.py:433(backward)\n",
      "       32    0.000    0.000    8.829    0.276 __init__.py:149(backward)\n",
      "       32    8.827    0.276    8.827    0.276 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "       33    0.000    0.000    4.546    0.138 memory.py:147(empty_cache)\n",
      "       33    4.546    0.138    4.546    0.138 {built-in method torch._C._cuda_emptyCache}\n",
      " 2486/265    0.001    0.000    1.469    0.006 {built-in method builtins.next}\n",
      "       33    0.001    0.000    1.160    0.035 data_loader.py:663(__iter__)\n",
      "       33    0.000    0.000    1.145    0.035 data_loader.py:618(_fetch_batches)\n",
      "       33    0.000    0.000    1.136    0.034 dataloader.py:625(__next__)\n",
      "       33    0.003    0.000    1.134    0.034 dataloader.py:672(_next_data)\n",
      "       33    0.002    0.000    1.124    0.034 fetch.py:24(fetch)\n",
      "       32    0.000    0.000    0.955    0.030 trainer.py:3254(compute_loss)\n",
      "...\n",
      "        1    0.000    0.000    0.000    0.000 modeling_utils.py:903(_\n",
      "...Expected behaviorSince the trace of the profiler is really long I only included the first few lines.I am running a small llama model on some dummy data, the only difference between the two datasets is that the slow version outputs 4D attention masks, which is a feature recently added in#27539. I am running both trainers for 1 iteration.As you can see the slow run is 340s while the fast one runs in 16s.The slow version of the trainer is many times slower than the fast version. The problem probably lies in the default collatorDataCollatorWithPadding(when there is a pretrained tokenizer), which callstokenizer.padon the 4D attention masks. When you takeaway either 1) the pretrained tokenizer or 2) the 4D attention mask, trainer runs much faster.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_246.txt:\n",
      "Title: Is apply_chat_template support function call usage?\n",
      "URL: https://github.com/huggingface/transformers/issues/32130\n",
      "Body:\n",
      "System Infotransformers= 4.42.4python=3.10.13ubuntu=20.04Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI use simple test code to verify the support of the apply_chat_template  interface for the function call mode, but result shows input tools information was not added to the output prompt. Is there a problem with my usage？fromtransformersimportAutoTokenizerget_current_weather={\"type\":\"function\",\"function\": {\"name\":\"get_current_weather\",\"description\":\"Get the current weather in a given location.\",\"parameters\": {\"type\":\"object\",\"properties\": {\"location\": {\"type\":\"string\",\"description\":\"The city and state, e.g. San Francisco, CA\",\n",
      "          },\"unit\": {\"type\":\"string\",\"enum\": [\"celsius\",\"fahrenheit\"]},\n",
      "        },\"required\": [\"location\"],\n",
      "      },\n",
      "  }\n",
      "}messages=[\n",
      "        {\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
      "        {\"role\":\"user\",\"content\":\"help me query currnet weather in San Francisco.\"},\n",
      "    ]tokenizer=AutoTokenizer.from_pretrained(\"xxx/Qwen2-1.5B-Instruct\")prompt=tokenizer.apply_chat_template(messages,tools=[get_current_weather,],tokenize=False,add_generation_prompt=True)print(\"prompt: \",prompt)prompt:  <|im_start|>systemYou are a helpful assistant.<|im_end|><|im_start|>userhelp me query currnet weather in San Francisco.<|im_end|><|im_start|>assistantExpected behaviorNone\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_508.txt:\n",
      "Title: Load transformer models with shared memory\n",
      "URL: https://github.com/huggingface/transformers/issues/29366\n",
      "Body:\n",
      "System Info.Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction.Expected behaviorI want to load transformer models with share memory for example when open multi workers I want to load the model once and all workers use it and not load the model for every worker\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_285.txt:\n",
      "Title: Add MultiStepLR with Warmup Scheduler\n",
      "URL: https://github.com/huggingface/transformers/issues/31831\n",
      "Body:\n",
      "Feature requestI would like to propose the addition of a new learning rate scheduler that combines MultiStepLR with a warmup phase. Currently, the Transformers library does not include a scheduler that uses both MultiStepLR and warmup. This feature can be beneficial for training models where the learning rate needs to be adjusted at specific epochs with an initial warmup phase to stabilise training.MotivationIn many training scenarios, it is beneficial to start with a warmup phase where the learning rate gradually increases, followed by a phase where the learning rate decreases at specific milestones (steps).ContributionI propose adding a new scheduler,get_multistep_schedule_with_warmup, which combines the functionality of MultiStepLR and Warmup. This scheduler will increase the learning rate linearly during the warmup phase and then follow the MultiStepLR schedule. I am more than happy to create a pull request (PR) implementing this feature. Please let me know if this sounds like a valuable addition, and I will proceed with the implementation.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_440.txt:\n",
      "Title: Trainer do not move the model to GPU when doing evaluation with FSDP\n",
      "URL: https://github.com/huggingface/transformers/issues/30239\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.3Platform: Linux-5.4.0-164-generic-x86_64-with-glibc2.31Python version: 3.10.14Huggingface_hub version: 0.19.4Safetensors version: 0.4.2Accelerate version: 0.29.1Accelerate config:    not foundPyTorch version (GPU?): 2.1.1+cu118 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: yesWho can help?@muellerzr@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionevaluation scriptsmy_evaluate.py:fromtransformersimportAutoModelForCausalLM,AutoTokenizer,TrainingArgumentsfromdatasetsimportload_datasetfromtrlimportSFTTrainerdefmain():# argsmodel_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0'dataset_name='HuggingFaceH4/ultrachat_200k'output_dir='./tiny_llama_eval'model=AutoModelForCausalLM.from_pretrained(model_name,trust_remote_code=True,\n",
      "    )tokenizer=AutoTokenizer.from_pretrained(model_name)eval_dataset=load_dataset(dataset_name,split='test_sft')eval_dataset=eval_dataset.map(lambdax: {\"formatted_chat\":tokenizer.apply_chat_template(x[\"messages\"],tokenize=False,add_generation_prompt=False)})training_args=TrainingArguments(output_dir=output_dir,per_device_eval_batch_size=8,\n",
      "    )trainer=SFTTrainer(model=model,tokenizer=tokenizer,args=training_args,eval_dataset=eval_dataset,eval_packing=False,dataset_text_field='formatted_chat',max_seq_length=2048,\n",
      "    )results=trainer.evaluate()print(results)if__name__=='__main__':main()fsdp_config.yaml:compute_environment:LOCAL_MACHINEdebug:truedistributed_type:FSDPdowncast_bf16:'no'fsdp_config:fsdp_auto_wrap_policy:TRANSFORMER_BASED_WRAPfsdp_backward_prefetch:BACKWARD_PREfsdp_cpu_ram_efficient_loading:truefsdp_forward_prefetch:falsefsdp_offload_params:falsefsdp_sharding_strategy:FULL_SHARDfsdp_state_dict_type:SHARDED_STATE_DICTfsdp_sync_module_states:truefsdp_use_orig_params:falsemachine_rank:0main_training_function:mainmixed_precision:'bf16'num_machines:1num_processes:8rdzv_backend:staticsame_network:truetpu_env:[]tpu_use_cluster:falsetpu_use_sudo:falseuse_cpu:falserunaccelerate launch --config_file fsdp_config.yaml my_evaluate.pyget error:RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:6!inthis linebecauseweightis on cpu whileinputis on gpu.Expected behaviorExpected behavior: the model should be on correct device when doing evaluation with FSDP.What makes the problem:Traineruses itsacceleratorto prepare the model (seethis line). Howeveraccelerate.Acceleratorwill not wrap the model with FSDP whenevaluation_mode == True. As a result, the model will stay on cpu. Seethisfor details.I can fix the problem by simply settingevaluation_mode=Falseintransformers/src/transformers/trainer.pyLines 3585 to 3589\n",
      "      inb109257iflen(self.accelerator._models)==0andmodelisself.model:model=(self.accelerator.prepare(model)ifself.is_deepspeed_enabledelseself.accelerator.prepare_model(model,evaluation_mode=True)But I am not sure if this would cause other problems during the above evaluation.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_326.txt:\n",
      "Title: Fail to load model without .safetensors file\n",
      "URL: https://github.com/huggingface/transformers/issues/31552\n",
      "Body:\n",
      "System InfoCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.transformersversion: 4.41.2Platform: Linux-5.4.0-146-generic-x86_64-with-glibc2.31Python version: 3.11.9Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.27.2Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu118 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?Hi I usedhuggingface-clito download bothhaoranxu/ALMA-13B-RandALMA-13Bin the same cache directory (my_cache_dir).I can load ALMA-13B-R successfully with the following command:model = AutoModelForCausalLM.from_pretrained(\n",
      "    \"haoranxu/ALMA-13B-R\", \n",
      "    cache_dir=my_cache_dir, \n",
      "    torch_dtype=torch.float16, \n",
      "    device_map=\"auto\")but failed to load ALMA-13Bmodel = AutoModelForCausalLM.from_pretrained(\n",
      "    \"haoranxu/ALMA-13B\", \n",
      "    cache_dir=my_cache_dir, \n",
      "    torch_dtype=torch.float16, \n",
      "    device_map=\"auto\")The error log show as follows:urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with \n",
      "url: /haoranxu/ALMA-13B/resolve/main/model.safetensors.index.json (Caused by ConnectTimeoutError(<urllib3.connecti\n",
      "on.HTTPSConnection object at 0x7f4ef4913290>, 'Connection to huggingface.co timed out. (connect timeout=10)'))    \n",
      "                                                                                                                  \n",
      "During handling of the above exception, another exception occurred:     \n",
      "\n",
      "requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded wit\n",
      "h url: /haoranxu/ALMA-13B/resolve/main/model.safetensors.index.json (Caused by ConnectTimeoutError(<urllib3.connec\n",
      "tion.HTTPSConnection object at 0x7f4ef4913290>, 'Connection to huggingface.co timed out. (connect timeout=10)'))After downgradingtransformersto4.39.3, ALMA-13B can be loaded with the same command.Since ALMA-13B-R has.safetensorsfiles whereas ALMA-13B only haspytorch.binfiles, I believe some bugs still need to be fixed.InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionimport torchfrom transformers import AutoModelForCausalLMmodel = AutoModelForCausalLM.from_pretrained(\"haoranxu/ALMA-13B\", cache_dir=your_cache_dir, torch_dtype=torch.float16, device_map=\"auto\")Expected behaviorThe model is successfully loaded. The command-line log probably show as follows:Loading checkpoint shards: 100%|███████████████████████████████| 6/6 [00:31<00:00,  5.24s/it]\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_332.txt:\n",
      "Title: Quantization support for heads and embeddings\n",
      "URL: https://github.com/huggingface/transformers/issues/31474\n",
      "Body:\n",
      "Feature requestHi! I’ve been researching LLM quantization recently (this paper), and noticed a potentially improtant issue that arises when using LLMs with 1-2 bit quantization.Problem description 🔍Transformers supports several great ways for quantizing transformer ‘body’, but it seems that there is no built-in wayto quantize embeddings and/or lm head.The reason why this is important is that some of the recent LLMs have very large vocabularies, and as a result,their embeddings and heads can get massive. For instance,Llama 3has 128K token vocabulary,Qwen 2has over 150K,Gemma 2bhas 256KAs a result, if you load NF4 or AQLM quantized models, theirembeddings can take up 50% or more of the model footprint.This is even more critical for lower bitwidth quantization:Feature Request 🚀It would be great if transformers had a flag to quantize embeddings and heads using some of the existing quantization methods. One simple way would be to use LLM.int8 or NF4 by Tim Dettmers since transformers already supports this.I’ve investigated how quantizing embeddings with these methods affects common models. Below is model perplexity forLlama 3 8B using AQLM+PV 2-bit quantization. I measured three configurations: fp16 embeddings, int8 embeddings and NF4 embeddings with the same parameters that transformers uses for linear layers.The values represent perplexity onWikiText-2 test setmeasured with the same protocol used inGPTQ/AQLM/QuIP#papers.The code for these measurements can be foundhere.Overall, 8-bit compression looks nearly lossless, the increase in perplexity does not exceed the error you get whenquantizing the transformer with the same LLM int8 codec. In turn, NF4 introduces some error (within 0.05 for Llama 3),but I would argue that this trade-off makes sense for low memory applications. Also, embeddings appeareasier to quantize than heads.Implementation details ⚙️There are multiple obstacles on the way to implementing this feature:No support for mixed quantizationCurrently, transformers does not support quantizing with multipleHfQuantizers. IMO this is a good behaviour, as interactions between different quantizators can be messy. The problem is that this feature requires for transformers library to use different compression methods for body and heads/embeddings. I think that can be solved by extendingHfQuantizerinterface by adding embedding/head quantization methods and adding new[embed,head]_quantization_configarguments toQuantizationConfigMixinor something in this area.No support for embedding quantization in bitsandbytesAs far as I know, no quantization method supportsnn.Embedding-like interface. I can ask bitsandbytes maintainers if they would accept a PR that fixes that.Also, there is a caveat that some models use tied embeddings/heads, while implementing, one need to be mindful of them.Cool things that this can enable 🏆If we can implement 4-bit embeddings, it will be possible to write a colab notebook that runsLlama 3 70B modelon the free tier T4 GPU without offoading, by combining embedding/heads quantization and the  PV-tuned modelhttps://huggingface.co/ISTA-DASLab/Meta-Llama-3-70B-AQLM-PV-1Bit-1x16.Another use case is running quantized LLMs on smartphones or embedded devices: for instance, thegemma-2bcan fit into 1GB RAM, but only if you quantize embeddings/heads in addition to transformer weights.If you’re interested in making a demo out of this, I’d be excited to implement this with your review / recommendations if you prefer, or wait for you to implement it your way.What do you think?MotivationWe are faced with a new bottleneck in model quantization. I think we can manage to fix itYour contributionI can allocate my time to submitting PR, but we need to figure out what to do first\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_454.txt:\n",
      "Title: Unable to load starcoder2 finetuned version getting quantization errors\n",
      "URL: https://github.com/huggingface/transformers/issues/29990\n",
      "Body:\n",
      "System InfoI am running on A100 with 40 GB GPU memoryWho can help?@SunMarcand@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction1- I have a SFT tuned starcoder2 model2- I am trying to load the same using AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path)model=from_pretrained_wrapper(model_name_or_path,functionexecutor-run-evaluation-d4381aa-3454115740:File“/app/code/evaluation/evaluation_utils.py”,line189,infrom_pretrained_wrapperfunctionexecutor-run-evaluation-d4381aa-3454115740:AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path)functionexecutor-run-evaluation-d4381aa-3454115740:File“/usr/local/lib/python3.8/dist-packages/transformers/models/auto/auto_factory.py”,line563,infrom_pretrainedfunctionexecutor-run-evaluation-d4381aa-3454115740:returnmodel_class.from_pretrained(functionexecutor-run-evaluation-d4381aa-3454115740:File“/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py”,line3039,infrom_pretrainedfunctionexecutor-run-evaluation-d4381aa-3454115740:config.quantization_config=AutoHfQuantizer.merge_quantization_configs(functionexecutor-run-evaluation-d4381aa-3454115740:File“/usr/local/lib/python3.8/dist-packages/transformers/quantizers/auto.py”,line149,inmerge_quantization_configsfunctionexecutor-run-evaluation-d4381aa-3454115740:quantization_config=AutoQuantizationConfig.from_dict(quantization_config)functionexecutor-run-evaluation-d4381aa-3454115740:File“/usr/local/lib/python3.8/dist-packages/transformers/quantizers/auto.py”,line73,infrom_dictfunctionexecutor-run-evaluation-d4381aa-3454115740:raiseValueError(functionexecutor-run-evaluation-d4381aa-3454115740:ValueError:Unknownquantizationtype,gotbitsandbytes-supportedtypesare: [‘awq’, ‘bitsandbytes_4bit’, ‘bitsandbytes_8bit’, ‘gptq’, ‘aqlm’, ‘quanto’]Expected behaviorIt should be able to load the model properly.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_483.txt:\n",
      "Title: [flax_llama] Why is the return value of thecreate_sinusoidal_positionstruncated bynum_pos?\n",
      "URL: https://github.com/huggingface/transformers/issues/29590\n",
      "Body:\n",
      "System Infotransformersversion: 4.38.2Platform: Linux-3.10.0-1160.el7.x86_64-x86_64-with-glibc2.35Python version: 3.12.2Huggingface_hub version: 0.21.3Safetensors version: 0.4.2Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): not installed (NA)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): 0.8.1 (gpu)Jax version: 0.4.23JaxLib version: 0.4.23Using GPU in script?: YesUsing distributed or parallel set-up in script?: YesWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI noticed that in the implementation offlax-llama, the return value ofcreate_sinusoidal_positionswas truncated bynum_pos.defcreate_sinusoidal_positions(num_pos,dim):inv_freq=1.0/(10000**(np.arange(0,dim,2)/dim))freqs=np.einsum(\"i , j -> i j\",np.arange(num_pos),inv_freq).astype(\"float32\")emb=np.concatenate((freqs,freqs),axis=-1)out=np.concatenate((np.sin(emb)[:,None, :],np.cos(emb)[:,None, :]),axis=-1)returnjnp.array(out[:, :, :num_pos])# Why?Why is this? Is it to prevent the sequence length from being too short?Expected behaviorSee above.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_68.txt:\n",
      "Title: Obtaining an Exception \"KeyError: 'labels'\" while fine-tuning Whisper\n",
      "URL: https://github.com/huggingface/transformers/issues/33153\n",
      "Body:\n",
      "System InfoWLS 2.0 Ubuntu 22.04transformers 4.44.2python3.10Who can help?@sanchit-gandhiInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionUse example fromhttps://huggingface.co/blog/fine-tune-whisperas is. Wthout modify.Get exception:---------------------------------------------------------------------------KeyError                                  Traceback (most recent call last)Cell In[22], line 1----> 1 trainer.train()File [~/.local/lib/python3.10/site-packages/transformers/trainer.py:1929](http://127.0.0.1:8888/home/artyom/.local/lib/python3.10/site-packages/transformers/trainer.py#line=1928), in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)1926 try:1927     # Disable progress bars when uploading models during checkpoints to avoid polluting stdout1928     hf_hub_utils.disable_progress_bars()-> 1929     return inner_training_loop(1930         args=args,1931         resume_from_checkpoint=resume_from_checkpoint,1932         trial=trial,1933         ignore_keys_for_eval=ignore_keys_for_eval,1934     )1935 finally:1936     hf_hub_utils.enable_progress_bars()File [~/.local/lib/python3.10/site-packages/transformers/trainer.py:2236](http://127.0.0.1:8888/home/artyom/.local/lib/python3.10/site-packages/transformers/trainer.py#line=2235), in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)2233     rng_to_sync = True2235 step = -1-> 2236 for step, inputs in enumerate(epoch_iterator):2237     total_batched_samples += 12239     if self.args.include_num_input_tokens_seen:File [~/.local/lib/python3.10/site-packages/accelerate/data_loader.py:454](http://127.0.0.1:8888/home/artyom/.local/lib/python3.10/site-packages/accelerate/data_loader.py#line=453), in DataLoaderShard.__iter__(self)452 # We iterate one batch ahead to check when we are at the end453 try:--> 454     current_batch = next(dataloader_iter)455 except StopIteration:456     yieldFile [~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631](http://127.0.0.1:8888/home/artyom/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py#line=630), in _BaseDataLoaderIter.__next__(self)628 if self._sampler_iter is None:629     # TODO(https://github.com/pytorch/pytorch/issues/76750)630     self._reset()  # type: ignore[call-arg]--> 631 data = self._next_data()632 self._num_yielded += 1633 if self._dataset_kind == _DatasetKind.Iterable and \\634         self._IterableDataset_len_called is not None and \\635         self._num_yielded > self._IterableDataset_len_called:File ~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675, in _SingleProcessDataLoaderIter._next_data(self)673 def _next_data(self):674     index = self._next_index()  # may raise StopIteration--> 675     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration676     if self._pin_memory:677         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)File [~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54](http://127.0.0.1:8888/home/artyom/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py#line=53), in _MapDatasetFetcher.fetch(self, possibly_batched_index)52 else:53     data = self.dataset[possibly_batched_index]---> 54 return self.collate_fn(data)Cell In[17], line 18, in DataCollatorSpeechSeq2SeqWithPadding.__call__(self, features)15 batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")17 # get the tokenized label sequences---> 18 label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]19 # pad the labels to max length20 labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")Cell In[17], line 18, in <listcomp>(.0)15 batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")17 # get the tokenized label sequences---> 18 label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]19 # pad the labels to max length20 labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")KeyError: 'labels'Expected behaviorStarting model training.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_40.txt:\n",
      "Title: cannot import name 'ShardedDDPOption' from 'transformers.trainer'\n",
      "URL: https://github.com/huggingface/transformers/issues/33242\n",
      "Body:\n",
      "System InfoI am getting the following error, but this error should not be there -cannot import name 'ShardedDDPOption' from 'transformers.trainer'I have the following versions installed -tokenizers-0.19.1transformers-4.43.4huggingface-hub-0.24.6I have upgraded Vicuna -7v-v1.5 to llama 3.1 8B in this github repo -https://github.com/baaivision/EVEThis works with the vicuna-7b-v1.5, but not with llama3.1 8B. It should work as there isn't much change. I earlier got rope error, but solved it by upgrading transformers as guided in this issue -https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/discussions/15Who can help?https://github.com/amyerobertshttps://github.com/muellerzrhttps://github.com/SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI run bash eve7b_prealign.sh 0 localhostThis works with the vicuna-7b-v1.5, but not with llama3.1 8B. It should work as there isn't much change. I earlier got rope error, but solved it by upgrading transformers as guided in this issue -https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/discussions/15Expected behaviorThe model should start training\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_54.txt:\n",
      "Title: The model's address is https://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx。I don't know how to load encode.onnx and decoder.onnx, and successfully translate a sentence into another language. Can you help me write an inference code to achieve the translation effect through the encoder and decoder? thank you\n",
      "URL: https://github.com/huggingface/transformers/issues/33210\n",
      "Body:\n",
      "Feature requesthello，The model's address ishttps://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx。Idon't know how to load encode.onnx and decoder.onnx, and successfully translate a sentence into another language. Can you help me write an inference code to achieve the translation effect through the encoder and decoder? thank youMotivationhello，The model's address ishttps://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx。Idon't know how to load encode.onnx and decoder.onnx, and successfully translate a sentence into another language. Can you help me write an inference code to achieve the translation effect through the encoder and decoder? thank youYour contributionhello，The model's address ishttps://huggingface.co/Xenova/nllb-200-distilled-600M/tree/main/onnx。Idon't know how to load encode.onnx and decoder.onnx, and successfully translate a sentence into another language. Can you help me write an inference code to achieve the translation effect through the encoder and decoder? thank you\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_124.txt:\n",
      "Title: confusing deprecation msg forDynamicCache.seen_tokens- nocache_positionin this class\n",
      "URL: https://github.com/huggingface/transformers/issues/32855\n",
      "Body:\n",
      "The message says\"Theseen_tokensattribute is deprecated and will be removed in v4.41. Use thecache_positionmodel input instead.\"however there is nocache_positionin this class (it is present in StaticCache). Perhaps there may be a clearer message.code ref:https://github.com/huggingface/transformers/blame/7c1149120829eb58e42a1ba4dbd2a9fa98864707/src/transformers/cache_utils.py#L91tagging@gante\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_130.txt:\n",
      "Title: Mamba slow implementation datatype mismatch\n",
      "URL: https://github.com/huggingface/transformers/issues/32690\n",
      "Body:\n",
      "System Infotransformers==4.44.0Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSetup a PyTorch Lightning training with float16 precision.SetupMambaConfigwithuse_mambapy=FalseTrainMambaForCausalLMmodelExpected behaviorThe expected behavior is training proceeds with no issues. However, this is the error I got:Code: cache_utils.py, MambaCache class, update_conv_state methodconv_state[:, :, cache_position] = new_conv_state.to(conv_state.device)\n",
      "RuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and Half for the source.I was able to fix the issue by changingconv_state[:, :, cache_position] = new_conv_state.to(conv_state.device)toconv_state[:, :, cache_position] = new_conv_state.to(device=conv_state.device, dtype=conv_state.dtype)I will open a PR with the fix.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_859.txt:\n",
      "Title: Add TF implementation of LongT5 model\n",
      "URL: https://github.com/huggingface/transformers/issues/18063\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_695.txt:\n",
      "Title: Support for the custom kernel for Mask2Former and OneFormer\n",
      "URL: https://github.com/huggingface/transformers/issues/25904\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_865.txt:\n",
      "Title: _batch_encode_plus() got an unexpected keyword argument 'is_pretokenized' using BertTokenizerFast\n",
      "URL: https://github.com/huggingface/transformers/issues/17488\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_871.txt:\n",
      "Title: add FAN model (vision)\n",
      "URL: https://github.com/huggingface/transformers/issues/17234\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_194.txt:\n",
      "Title: Llava-Next with vision_feature_select_strategy == \"full\" returns error image size mismatch\n",
      "URL: https://github.com/huggingface/transformers/issues/32395\n",
      "Body:\n",
      "System Infotransformers==4.42.3Who can help?@zucchini-nlpInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWorking example: usingdefaultvision_feature_select_strategyfromtransformers.models.llava_nextimportLlavaNextForConditionalGeneration,LlavaNextProcessorfromPILimportImageimportrequestsimporttorchmodel=LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\").to(dtype=torch.bfloat16,device=torch.device(\"cuda\"))processor=LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\")url=\"http://images.cocodataset.org/val2017/000000039769.jpg\"image_cats=Image.open(requests.get(url,stream=True).raw)prompt=\"USER: <image>\\nWhat is shown in this image? ASSISTANT:\"input=processor(prompt,image_cats,padding=True,return_tensors=\"pt\").to(\"cuda\")output=model.generate(**input,max_length=128)processor.decode(output[0],skip_special_tokens=True)'USER: \\nWhat is shown in this image? ASSISTANT: The image shows two cats lying on a pink surface, which appears to be a couch or a bed. The cats are resting with their eyes closed, suggesting they are either sleeping or very relaxed. There are also two remote controls placed near the cats, which might indicate that the cats are in a living room or a space where people watch television. The cats have different patterns on their fur, with one being a tabby and the other a calico.'Buggy example withfullvision_feature_select_strategyfromtransformers.models.llava_nextimportLlavaNextForConditionalGeneration,LlavaNextProcessorfromPILimportImageimportrequestsimporttorchmodel=LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\",vision_feature_select_strategy=\"full\").to(dtype=torch.bfloat16,device=torch.device(\"cuda\"))processor=LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\")url=\"http://images.cocodataset.org/val2017/000000039769.jpg\"image_cats=Image.open(requests.get(url,stream=True).raw)prompt=\"USER: <image>\\nWhat is shown in this image? ASSISTANT:\"input=processor(prompt,image_cats,padding=True,return_tensors=\"pt\").to(\"cuda\")output=model.generate(**input,max_length=128)processor.decode(output[0],skip_special_tokens=True)File /opt/conda/lib/python3.10/site-packages/transformers/models/llava_next/modeling_llava_next.py:662, in LlavaNextForConditionalGeneration.pack_image_features(self, image_features, image_sizes, image_newline)\n",
      "height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n",
      "if height * width != base_image_feature.shape[0]:\n",
      "    raise ValueError(\"The number of patches is not consistent with the image size.\")\n",
      "\n",
      "ValueError: The number of patches is not consistent with the image size.Side note: there is another bug. If you passvision_feature_select_strategyto.generate()function, it is not passed toLlavaNextForConditionalGeneration.forward()and it usesdefaultstrategy.model=LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\").to(dtype=torch.bfloat16,>device=torch.device(\"cuda\"))input=processor(prompt,image_cats,padding=True,return_tensors=\"pt\").to(\"cuda\")output=model.generate(**input,vision_feature_select_strategy=\"full\",max_length=128)The example above \"seems\" to usefullstrategy, but it is actually usingdefaultstrategy and returns no error, which is not intended.Back to the main bug example, the reason of such error is due to the following lines:transformers/src/transformers/models/llava_next/modeling_llava_next.pyLines 789 to 792\n",
      "      inc1aa0edifvision_feature_select_strategy==\"default\":selected_image_feature=selected_image_feature[:,1:]elifvision_feature_select_strategy==\"full\":selected_image_feature=selected_image_featureHere,selected_image_featureshape before applying the select strategy is[5, 577, 1024]. If we usedefaultselect strategy one feature is removed from the second dimension, resulting in[5, 576, 1024]feature dimension, which can pass the condition inself.pack_image_features():transformers/src/transformers/models/llava_next/modeling_llava_next.pyLines 662 to 663\n",
      "      inc1aa0edifheight*width!=base_image_feature.shape[0]:raiseValueError(\"The number of patches is not consistent with the image size.\")withheight=width=24thusheight * weight = 576.However, iffullselect strategy is used we have the feature with unchanged dimension[5, 577, 1024]which raisesValueErrorinself.pack_image_features()asheight * width == 576 != 577.Expected behaviorRuns without an error, potentially having a different output from that withdefaultstrategy.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_180.txt:\n",
      "Title: FutureWarning:torch._dynamo.external_utils.is_compilingis deprecated. Usetorch.compiler.is_compilinginstead.\n",
      "URL: https://github.com/huggingface/transformers/issues/32437\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.3PyTorch version (GPU?): 2.5.0a0+git522fa03 (True)pytorch/pytorch@522fa03Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductiontransformers code:https://github.com/huggingface/transformers/blob/13dc6b0853c3cb54e79b18105c0528bc9e84881c/src/transformers/modeling_attn_mask_utils.py#L445C43-L445C69failed test error message:/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:1107:inforwardextended_attention_mask=_prepare_4d_attention_mask_for_sdpa(/usr/local/lib/python3.10/dist-packages/transformers/modeling_attn_mask_utils.py:445:in_prepare_4d_attention_mask_for_sdpaor(hasattr(torch,\"_dynamo\")andtorch._dynamo.is_compiling())________________________________________args=(),kwargs={}\n",
      "\n",
      "    @functools.wraps(arg)defwrapper(*args,**kwargs):>warnings.warn(msg,category=category,stacklevel=stacklevel+1)EFutureWarning:`torch._dynamo.external_utils.is_compiling`isdeprecated.Use`torch.compiler.is_compiling`instead.pytorch code:https://github.com/pytorch/pytorch/blob/1add8c5f1c4909ca2a57d110807684aaa230b7b2/torch/_dynamo/external_utils.py#L18-L22Expected behaviorno future warning on deprecated API usages\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_816.txt:\n",
      "Title: [i18n-<languageCode>] Translating docs to <languageName>\n",
      "URL: https://github.com/huggingface/transformers/issues/20574\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_631.txt:\n",
      "Title: cannot recall the model after registering it using AutoModelForCausalLM.register\n",
      "URL: https://github.com/huggingface/transformers/issues/27427\n",
      "Body:\n",
      "Model descriptionI have registered my custom model using AutoModelForCausalLM.register(CustomAIConfig, CustomAI) but I am unable to call the model . It shows ImportError: cannot import name 'CustomAI' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/init.py) . Can you help me in this?Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationNo response\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_143.txt:\n",
      "Title: Docker container with development environment for Transformers library\n",
      "URL: https://github.com/huggingface/transformers/issues/32628\n",
      "Body:\n",
      "Feature requestA docker container that one can use with or without GPU that has everything to start developing, if it doesn't exist and a reference to it in theCONTRIBUTING.MDfile.MotivationReduce the barrier to start developing in any system with docker.Your contributionI could build it. Would be something in the lines of:FROM python:3.10\n",
      "\n",
      "# Install system dependencies\n",
      "RUN apt-get update && \\\n",
      "    apt-get install -y git && \\\n",
      "    pip install --upgrade pip --no-cache-dir && \\\n",
      "    apt-get clean && \\\n",
      "    rm -rf /var/lib/apt/lists/*\n",
      "\n",
      "# Set a temp directory to install environment\n",
      "WORKDIR /usr/src/temp\n",
      "\n",
      "# Clone the repository\n",
      "RUN git clone https://github.com/huggingface/transformers.git\n",
      "\n",
      "# Set the working directory to the cloned repository\n",
      "WORKDIR /usr/src/temp/transformers\n",
      "\n",
      "# Install development dependencies \n",
      "RUN pip install -e .[dev]\n",
      "\n",
      "# Go tot he working directory for development\n",
      "WORKDIR /usr/src/appplus a docker compose like:services:\n",
      "  app:\n",
      "    build: .\n",
      "    volumes:\n",
      "      - ~:/usr/src/app\n",
      "    ports:\n",
      "      - \"8000:8000\"\n",
      "    entrypoint: [\"tail\", \"-f\", \"/dev/null\"]\n",
      "    environment:\n",
      "      - RUN_SLOW=true\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_396.txt:\n",
      "Title: Integrate IndicTrans2 models and tokenizer into HF Transformers\n",
      "URL: https://github.com/huggingface/transformers/issues/30818\n",
      "Body:\n",
      "Model descriptionIndicTrans2is a multilingual transformer model developed by AI4Bharat, and is available in 3 flavors:indic-en,en-indicandindic-indic. Each flavor has 2 versions, a large 1B model, and a distilled 200M model. The architecture is a standard transformer, very similar to NLLB and M2M models. However, the major difference is the vocabularies of the encoder and decoder and not shared, as they require different languages.Unlike, NLLB and M2M models, IndicTrans2 required specific preprocessing for the inputs. Hence acustom processor class has been developed, and is required for training/inference. More examples can be found in the aforementioned repository.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationAuthors:@AI4Bharat@jaygala24@PranjalChitale@oneraghavan@VarunGumma@sumanthd17@prajdabre@anoopkunchukuttanOfficial GitHub Repository:AI4Bharat/IndicTrans2The HF compatible models and tokenizer are available here as of now:indictrans2-en-indic-1Bindictrans2-en-indic-dist-200Mindictrans2-indic-en-dist-200Mindictrans2-indic-en-1Bindictrans2-indic-indic-1Bindictrans2-indic-indic-dist-320M\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_33.txt:\n",
      "Title: Custom pipeline in remote repo cannot load custom model from remote repo.\n",
      "URL: https://github.com/huggingface/transformers/issues/33272\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Linux-6.8.0-41-generic-x86_64-with-glibc2.39Python version: 3.11.9Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: 0.34.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0+cu124 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA GeForce GTX 1080Who can help?@NarsilI define a custom pipeline ljw20180420/inDelphi_pipeline in the huggingface repository. It use a custom model from ljw20180420/SX_spcas9_inDelphi. I load the pipeline byfromtransformersimportpipelinepipe=pipeline(model=\"ljw20180420/inDelphi_pipeline\",trust_remote_code=True)It reports the following error:Traceback(mostrecentcalllast):File\"/home/ljw/new_fold/old_desktop/CRISPR_AI/inDelphi/test.py\",line3,in<module>pipe=pipeline(model=\"ljw20180420/inDelphi_pipeline\",trust_remote_code=True)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^File\"/home/ljw/new_fold/old_desktop/CRISPR_AI/.conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py\",line833,inpipelinetargeted_task,task_options=clean_custom_task(custom_tasks[task])^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^File\"/home/ljw/new_fold/old_desktop/CRISPR_AI/.conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py\",line544,inclean_custom_tasktask_info[\"pt\"]=tuple(getattr(transformers,c)forcinpt_class_names)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^File\"/home/ljw/new_fold/old_desktop/CRISPR_AI/.conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py\",line544,in<genexpr>task_info[\"pt\"]=tuple(getattr(transformers,c)forcinpt_class_names)^^^^^^^^^^^^^^^^^^^^^^^^File\"/home/ljw/new_fold/old_desktop/CRISPR_AI/.conda/lib/python3.11/site-packages/transformers/utils/import_utils.py\",line1596,in__getattr__raiseAttributeError(f\"module{self.__name__}has no attribute{name}\")AttributeError:moduletransformershasnoattributeinDelphiModelIt seems that pipeline() cannot find my custom model in transformers library. Is it possible to load custom model by custom pipeline?InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfromtransformersimportpipelinepipe=pipeline(model=\"ljw20180420/inDelphi_pipeline\",trust_remote_code=True)Expected behaviorLoad custom model \"ljw20180420/SX_spcas9_inDelphi\" successfully.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_27.txt:\n",
      "Title: \"Qwen2-VL FP16 inference results in errors or gibberish output.\"\n",
      "URL: https://github.com/huggingface/transformers/issues/33294\n",
      "Body:\n",
      "System Infobase this pull request :#33211python: Python 3.10.12infer code:from PIL import Image\n",
      "import requests\n",
      "import torch\n",
      "from torchvision import io\n",
      "from typing import Dict\n",
      "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
      "from torch.cuda.amp import autocast\n",
      "\n",
      "\n",
      "model_dir = \"/workspace/qwenvl-dev/Qwen2-VL-2B-Instruct\"\n",
      "# Load the model in half-precision on the available device(s)\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
      "    model_dir,\n",
      "    torch_dtype=torch.float16,  # Explicitly set to float16 for half-precision\n",
      ")\n",
      "model.to(device)\n",
      "\n",
      "processor = AutoProcessor.from_pretrained(model_dir)\n",
      "\n",
      "# Image\n",
      "image_path = \"/workspace/qwenvl-dev/demo.jpeg\"\n",
      "image = Image.open(image_path)\n",
      "\n",
      "conversation = [\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": [\n",
      "            {\n",
      "                \"type\": \"image\",\n",
      "            },\n",
      "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
      "        ],\n",
      "    }\n",
      "]\n",
      "\n",
      "\n",
      "# Preprocess the inputs\n",
      "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
      "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
      "\n",
      "inputs = processor(\n",
      "    text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
      ")\n",
      "inputs = inputs.to(\"cuda\")\n",
      "\n",
      "output_ids = model.generate(\n",
      "    **inputs,\n",
      "    max_new_tokens=128,\n",
      ")\n",
      "\n",
      "\n",
      "generated_ids = [\n",
      "    output_ids[len(input_ids) :]\n",
      "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
      "]\n",
      "output_text = processor.batch_decode(\n",
      "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
      ")\n",
      "print(output_text)the error informationTraceback (most recent call last):\n",
      "  File \"/workspace/qwenvl-dev/test_infer.py\", line 61, in <module>\n",
      "    output_ids = model.generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2015, in generate\n",
      "    result = self._sample(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2998, in _sample\n",
      "    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
      "RuntimeError: probability tensor contains either `inf`, `nan` or element < 0another infer to set do_sample falseoutput_ids = model.generate(\n",
      "    **inputs,\n",
      "    max_new_tokens=128,\n",
      "    do_sample=False,\n",
      "    top_k=None,\n",
      "    top_p=None,\n",
      "    temperature=None,\n",
      ")infer res['!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!']the float32 res is['The image depicts a serene beach scene with a woman and her dog. The woman is sitting on the sand, wearing a plaid shirt and black pants, and appears to be smiling. She is giving a high-five to the dog, which is sitting on the sand next to her. The dog has a harness around its neck and is looking up at the woman with its front paws raised in a gesture of affection or playfulness. The background shows the ocean with gentle waves, and the sky is clear with a soft glow from the setting or rising sun, casting a warm light over the entire scene. The overall atmosphere is peaceful and joyful']Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom PIL import Image\n",
      "import requests\n",
      "import torch\n",
      "from torchvision import io\n",
      "from typing import Dict\n",
      "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
      "from torch.cuda.amp import autocast\n",
      "\n",
      "\n",
      "model_dir = \"/workspace/qwenvl-dev/Qwen2-VL-2B-Instruct\"\n",
      "# Load the model in half-precision on the available device(s)\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
      "    model_dir,\n",
      "    torch_dtype=torch.float16,  # Explicitly set to float16 for half-precision\n",
      ")\n",
      "model.to(device)\n",
      "\n",
      "processor = AutoProcessor.from_pretrained(model_dir)\n",
      "\n",
      "# Image\n",
      "image_path = \"/workspace/qwenvl-dev/demo.jpeg\"\n",
      "image = Image.open(image_path)\n",
      "\n",
      "conversation = [\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": [\n",
      "            {\n",
      "                \"type\": \"image\",\n",
      "            },\n",
      "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
      "        ],\n",
      "    }\n",
      "]\n",
      "\n",
      "\n",
      "# Preprocess the inputs\n",
      "text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
      "# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n",
      "\n",
      "inputs = processor(\n",
      "    text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n",
      ")\n",
      "inputs = inputs.to(\"cuda\")\n",
      "\n",
      "output_ids = model.generate(\n",
      "    **inputs,\n",
      "    max_new_tokens=128,\n",
      ")\n",
      "\n",
      "\n",
      "generated_ids = [\n",
      "    output_ids[len(input_ids) :]\n",
      "    for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n",
      "]\n",
      "output_text = processor.batch_decode(\n",
      "    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
      ")\n",
      "print(output_text)Expected behaviorI hope this error gets fixed and merged in.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_355.txt:\n",
      "Title: Galore finetuning #stopped\n",
      "URL: https://github.com/huggingface/transformers/issues/31313\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-6.2.10-kd-cluster-x86_64-with-glibc2.31Python version: 3.11.4Huggingface_hub version: 0.23.3Safetensors version: 0.4.3Accelerate version: 0.30.1Accelerate config:    not foundPyTorch version (GPU?): 2.3.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionUsing the code provided by the galore, which I modified:# Configuration parameters\n",
      "model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
      "max_length = 128\n",
      "doc_stride = 128\n",
      "pad_to_max_length = True\n",
      "per_device_train_batch_size = 1\n",
      "per_device_eval_batch_size = 1\n",
      "learning_rate = 0.0002\n",
      "weight_decay = 0.0\n",
      "num_train_epochs = 1\n",
      "gradient_accumulation_steps = 1\n",
      "output_dir = \"/home/IAIS/jdatta/teacher_model\"\n",
      "seed = 42\n",
      "\n",
      "# Load the datasets\n",
      "squad = datasets.load_dataset(\"rajpurkar/squad_v2\")\n",
      "dataset = squad['train'].train_test_split(test_size=0.2)\n",
      "train_dataset = dataset['train']\n",
      "eval_dataset = dataset['test']\n",
      "\n",
      "train_dataset = train_dataset.select(range(1000))\n",
      "eval_dataset = eval_dataset.select(range(500))\n",
      "\n",
      "training_args = TrainingArguments(\n",
      "    output_dir=output_dir,\n",
      "    evaluation_strategy=\"steps\",\n",
      "    warmup_ratio=0.05,\n",
      "    overwrite_output_dir=True,\n",
      "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
      "    per_device_train_batch_size=per_device_train_batch_size,\n",
      "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
      "    num_train_epochs=num_train_epochs,\n",
      "    fp16=True,\n",
      "    eval_steps=10,\n",
      "    save_strategy='steps',\n",
      "    save_steps=10,\n",
      "    save_total_limit=1,\n",
      "    dataloader_num_workers=2,\n",
      "    load_best_model_at_end=True,\n",
      "    report_to=\"none\",\n",
      "    prediction_loss_only=True,\n",
      "    gradient_checkpointing=True,\n",
      "    optim_args=\"rank=64, update_proj_gap=100, scale=0.10\",\n",
      "    optim=\"galore_adafactor\",\n",
      "    optim_target_modules=[\"c_attn\", \"c_proj\", \"q_proj\", \"k_proj\", \"v_proj\", \"down_proj\", \"up_proj\"],\n",
      "    learning_rate=learning_rate,\n",
      "    weight_decay=weight_decay,\n",
      ")\n",
      "\n",
      "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
      "\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=train_dataset,\n",
      "    eval_dataset=eval_dataset,\n",
      "    data_collator=data_collator,\n",
      ")\n",
      "trainer.train()The traning is not starting.It is showing the following comments for 2 hours:/home/IAIS/jdatta/miniconda3/envs/myenv/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning:evaluation_strategyis deprecated and will be removed in version 4.46 of 🤗 Transformers. Useeval_strategyinsteadwarnings.warn(Activated GaLoRE fine-tuning, depending on your model size and hardware, the training might take a while before starting. Please be patient !huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...To disable this warning, you can either:- Avoid usingtokenizersbefore the fork if possible- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...To disable this warning, you can either:- Avoid usingtokenizersbefore the fork if possible- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)Expected behaviorAny parameters to tune?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_427.txt:\n",
      "Title: Weights of LlamaForQuestionAnswering were not initialized from the model checkpoint\n",
      "URL: https://github.com/huggingface/transformers/issues/30381\n",
      "Body:\n",
      "System Infotransformers == 4.39.0When runningmodel = AutoModelForQuestionAnswering.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token=access_token)I get the following error:Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-2-7b-hf and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.28.input_layernorm.weight', 'layers.28.mlp.down_proj.weight', 'layers.28.mlp.gate_proj.weight', 'layers.28.mlp.up_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.28.self_attn.k_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.28.self_attn.q_proj.weight', 'layers.28.self_attn.v_proj.weight', 'layers.29.input_layernorm.weight', 'layers.29.mlp.down_proj.weight', 'layers.29.mlp.gate_proj.weight', 'layers.29.mlp.up_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.29.self_attn.k_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.29.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.30.input_layernorm.weight', 'layers.30.mlp.down_proj.weight', 'layers.30.mlp.gate_proj.weight', 'layers.30.mlp.up_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.30.self_attn.k_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.31.input_layernorm.weight', 'layers.31.mlp.down_proj.weight', 'layers.31.mlp.gate_proj.weight', 'layers.31.mlp.up_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.31.self_attn.k_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.It appears that most of the weights are not initializedWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoModelForQuestionAnsweringmodel_path = \"meta-llama/Llama-2-7b-hf\"access_token = access_tokenmodel = AutoModelForQuestionAnswering.from_pretrained(model_path), token = access_token)Expected behaviorloaded model with pre-trained weights initialized\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_341.txt:\n",
      "Title: GroundingDino - Loss calculation exceptions\n",
      "URL: https://github.com/huggingface/transformers/issues/31434\n",
      "Body:\n",
      "System Infotransformers==4.40.2Python 3.10.14Ubuntu WSL  under Windows 10Who can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI've been trying to fine tune GroundingDino with transformers' GroundingDinoForObjectDetection. To ease things I've been using batch_size = 1.(I haven't tried with any other batch sizes)When running the model, I got this exception:Exception has occurred: RuntimeError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)\n",
      "split_with_sizes expects split_sizes to sum exactly to 2700 (input tensor's size at dimension -1), but got split_sizes=[3]\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/torch/_tensor.py\", line 921, in split\n",
      "    return torch._VF.split_with_sizes(self, split_size, dim)\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py\", line 2723, in forward\n",
      "    indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py\", line 2866, in forward\n",
      "    indices = self.matcher(outputs_without_aux, targets)\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py\", line 3091, in forward\n",
      "    loss_dict = criterion(outputs_loss, labels)\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nitay/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/folder/main.py\", line 84, in train\n",
      "    outputs = model(input_ids=input_ids, pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
      "  File \"/mnt/folder/main.py\", line 98, in <module>\n",
      "    train()\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main (Current frame)\n",
      "    return _run_code(code, main_globals, None,\n",
      "RuntimeError: split_with_sizes expects split_sizes to sum exactly to 2700 (input tensor's size at dimension -1), but got split_sizes=[3](There were indeed 3 bounding boxes in the label data)Expected behaviorLoss should be calculated with no errors\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_590.txt:\n",
      "Title: ffmpeg_microphone does not use current input device on Mac/Darwin\n",
      "URL: https://github.com/huggingface/transformers/issues/28154\n",
      "Body:\n",
      "While going through the HF tutorials for STThere, I found some unexpected behaviour with the ffmpeg_microphone_live function on my Mac. I also just found someone that might be having the same issueherebut it's an issue related to sound in Colab env so I'm creating this separately.The input device index used is always 0, but that might not match the current system input device. Using the current system input device would be the expected behaviour (also according to the other platforms' code that all specifydefaultfor input device). E.g. I was working with my laptop closed (just connected to the monitor) and wanted to capture sound with my headphones but couldn't.The solution seems to be fairly simple. Based on theffmpeg devices documentationthe valuedefaultis also supported for audio in avfoundation, and it will match the current system input device.I've changed this manually in audio_utils.py ffmpeg_microphone(...) and it seems to work as expected.elif system == \"Darwin\":\n",
      "        format_ = \"avfoundation\"\n",
      "        input_ = \":default\"Here's thelinkto the same line in the HF repo.I can make a PR for it if you want. This could also go with adding a param for the device index to those microphone functions similar to how other audio libraries do for easier customisation, which just falls back to use thedefaultinput device.Additional Infotransformers-cli envoutputtransformersversion: 4.35.2Platform: macOS-14.2-arm64-arm-64bitPython version: 3.10.13other info not relevant for this issueCode to reproduce is the snippet in the voice-assistant tutorial. In case the 0th device is not the one you want to listen with, the code will just fail since it won't capture any audio.import sys\n",
      "\n",
      "def transcribe(chunk_length_s=5.0, stream_chunk_s=1.0):\n",
      "    sampling_rate = transcriber.feature_extractor.sampling_rate\n",
      "\n",
      "    mic = ffmpeg_microphone_live(\n",
      "        sampling_rate=sampling_rate,\n",
      "        chunk_length_s=chunk_length_s,\n",
      "        stream_chunk_s=stream_chunk_s,\n",
      "    )\n",
      "\n",
      "    print(\"Start speaking...\")\n",
      "    for item in transcriber(mic, generate_kwargs={\"max_new_tokens\": 128}):\n",
      "        sys.stdout.write(\"\\033[K\")\n",
      "        print(item[\"text\"], end=\"\\r\")\n",
      "        if not item[\"partial\"][0]:\n",
      "            break\n",
      "\n",
      "    return item[\"text\"]According toffmpeg devices documentationyou can print out your system input devices usingffmpeg -f avfoundation -list_devices true -i \"\"For me this gives:[...]\n",
      "[AVFoundation indev @ 0x7fcc33004d00] AVFoundation video devices:\n",
      "[AVFoundation indev @ 0x7fcc33004d00] [0] FaceTime HD Camera\n",
      "[AVFoundation indev @ 0x7fcc33004d00] [1] Rui Silvestre’s iPhone Camera\n",
      "[AVFoundation indev @ 0x7fcc33004d00] [2] Capture screen 0\n",
      "[AVFoundation indev @ 0x7fcc33004d00] AVFoundation audio devices:\n",
      "[AVFoundation indev @ 0x7fcc33004d00] [0] MacBook Pro Microphone\n",
      "[AVFoundation indev @ 0x7fcc33004d00] [1] Rui Silvestre’s iPhone Microphone\n",
      "[AVFoundation indev @ 0x7fcc33004d00] [2] AirPods Pro\n",
      "[AVFoundation indev @ 0x7fcc33004d00] [3] Microsoft Teams AudioThe audio device at index 0 is my MacBook mic but I currently have my AirPods on and would want to use that as my input device. I've also noticed the indexes change fairly frequently depending on which devices are nearby.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_547.txt:\n",
      "Title: Adding Gradient Checkpointing and Flash Attention 2 implementation to VisionTextDualEncoderModel\n",
      "URL: https://github.com/huggingface/transformers/issues/28813\n",
      "Body:\n",
      "VisionTextDualEncoderModel allows training any image and text encoders with a contrastive loss. Would be convenient to add gradient checkpointing as well as flash attention 2 to optimize training.Thank you\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_553.txt:\n",
      "Title: Any plans to support KV Cache offloading to CPU (and NVMe)?\n",
      "URL: https://github.com/huggingface/transformers/issues/28738\n",
      "Body:\n",
      "Feature requestSimilar to how model parameter and optimizer offload is supported using thedeepspeed library, are there plans for natively supporting KV cache offloading as well?MotivationApart from helping accommodate larger batch sizes on a single GPU, this can also significantly improve overall throughput, specially when batch sizes grow very large (resulting in a linear increase in KV cache size).Your contributionI see there already exists an implementation of this:https://github.com/tjruwase/transformers/tree/kvcache-offload-cpu, so maybe this is simply about incorporating those changes in the main repo?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_235.txt:\n",
      "Title: Load Phi 3 small on Nvidia Tesla V100 - Flash Attention\n",
      "URL: https://github.com/huggingface/transformers/issues/32201\n",
      "Body:\n",
      "System InfoHi,I would like to inquire about the possibility of uploading and fine tuning a Phi 3 8k small. When I load the model, I get an error about missing Flash attention. If I want to install the given package, I get this error :RuntimeError: FlashAttention is only supported on CUDA 11.6 and above.  Note: make sure nvcc has a supported version by running nvcc -V.\n",
      "\n",
      "\n",
      "      torch.__version__  = 2.3.1+cu121But I have the required version of pytorch and CUDA (torch 2.3.1 and cuda 12.1)Is it because I am using a Tesla V100 graphics card? Is there any way to load the model also with this graphics card?I found this in the documentation for the Phi 3 mini on Huggingface:If you want to run the model on:NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation=\"eager\"Does this also apply to the Phi3 Small 8k?? Beacause when I try to load it, the error occursmodel = AutoModelForSequenceClassification.from_pretrained(\"path\", num_labels=num_labels ,attn_implementation=\"eager\" )\n",
      "\n",
      "AssertionError: Flash Attention is not available, but is needed for dense attentionOr should I try the ONNX version or it is just for inference?Thank you.Who can help?@arthu@BenjaminBossanInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionExpected behavior\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_209.txt:\n",
      "Title: DeepSpeed ZeRO stage3+Qwen2/Qwen2-57B-A14B-Instruct: RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)\n",
      "URL: https://github.com/huggingface/transformers/issues/32312\n",
      "Body:\n",
      "System Infotransformers==4.40.0trl>=0.8.6deepspeed==0.9.3gpu: A100-SXM4-80GB*32Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductiondz_zero3.json{\"train_micro_batch_size_per_gpu\":\"auto\",\"bf16\": {\"enabled\":\"auto\"},\"fp16\": {\"enabled\":\"auto\",\"loss_scale\":0,\"initial_scale_power\":16,\"loss_scale_window\":1000,\"hysteresis\":2,\"min_loss_scale\":1},\"zero_optimization\": {\"stage\":3,\"overlap_comm\":true,\"contiguous_gradients\":true,\"sub_group_size\":1e9,\"reduce_bucket_size\":\"auto\",\"stage3_prefetch_bucket_size\":\"auto\",\"stage3_param_persistence_threshold\":\"auto\",\"stage3_max_live_parameters\":1e9,\"stage3_max_reuse_distance\":1e9,\"stage3_gather_16bit_weights_on_model_save\":true}\n",
      "}train.pydeftrain(args):model_checkpoint=args.model_checkpointprint(f\"model_checkpoint:{model_checkpoint}@@@###\")Targs=TrainingArguments(report_to=\"tensorboard\",output_dir=args.output_dir,evaluation_strategy=args.eval_save_strategy,save_strategy=args.eval_save_strategy,save_steps=args.save_steps,eval_steps=args.eval_steps,learning_rate=args.learning_rate,per_device_train_batch_size=args.batch_size,per_device_eval_batch_size=args.batch_size,num_train_epochs=args.num_train_epochs,weight_decay=args.weight_decay,load_best_model_at_end=False,overwrite_output_dir=True,deepspeed=args.deepspeed,bf16=args.bf16,\n",
      "    )ds=load_dataset(\"json\",data_files={\"train\":args.train_dataset_path,\"test\":args.test_dataset_path})tokenizer=AutoTokenizer.from_pretrained(model_checkpoint,use_fast=True)ds=change_data(ds,args.data_type)defmodel_init():returnAutoModelForSequenceClassification.from_pretrained(model_checkpoint,return_dict=True)defpreprocess_function(examples):returntokenizer(examples[\"text\"],truncation=True,max_length=4096)encoded_ds=ds.map(preprocess_function,batched=True,num_proc=100)encoded_ds=encoded_ds.rename_column('label','labels')encoded_ds.set_format(type='torch',columns=['text','input_ids','attention_mask','labels'])print(f\"First train sample device:{next(iter(encoded_ds['train']))['input_ids'].device}\")print(f\"First test sample device:{next(iter(encoded_ds['test']))['input_ids'].device}\")# encoded_ds.set_format(\"torch\", device=\"cuda\")num_labels={0:108,1:48,2:27,3:12}current_time=datetime.now().strftime(\"%m%d\")args.output_dir=f\"{args.output_dir}/{current_time}_{args.sub_name}\"ifnotargs.use_param_search:model=AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=num_labels[args.data_type])defcompute_metrics(p):metric=load_metric(\"accuracy\")preds=np.argmax(p.predictions,axis=1)returnmetric.compute(predictions=preds,references=p.label_ids)ifargs.use_param_search:trainer=Trainer(args=Targs,train_dataset=encoded_ds[\"train\"],eval_dataset=encoded_ds[\"test\"],tokenizer=tokenizer,model_init=model_init,compute_metrics=compute_metrics,\n",
      "        )best_run=trainer.hyperparameter_search(n_trials=4,direction=\"maximize\")forn,vinbest_run.hyperparameters.items():setattr(trainer.args,n,v)trainer.train()else:trainer=Trainer(model=model,args=Targs,train_dataset=encoded_ds[\"train\"],eval_dataset=encoded_ds[\"test\"],tokenizer=tokenizer,compute_metrics=compute_metrics,\n",
      "        )trainer.train()error messageTraceback (most recent call last):\n",
      "File \"/checkpoint/binary/train_package/./rm_cls.py\", line 202, in <module>\n",
      "train(script_args)\n",
      "File \"/checkpoint/binary/train_package/./rm_cls.py\", line 159, in train\n",
      "trainer.train()\n",
      "File \"/root/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1859, in train\n",
      "return inner_training_loop(\n",
      "File \"/root/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2203, in _inner_training_loop\n",
      "tr_loss_step = self.training_step(model, inputs)\n",
      "File \"/root/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 3138, in training_step\n",
      "loss = self.compute_loss(model, inputs)\n",
      "File \"/root/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 3161, in compute_loss\n",
      "outputs = model(**inputs)\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "return self._call_impl(*args, **kwargs)\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "return forward_call(*args, **kwargs)\n",
      "File \"/root/.local/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "ret_val = func(*args, **kwargs)\n",
      "File \"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1736, in forward\n",
      "loss = self.module(*inputs, **kwargs)\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "return self._call_impl(*args, **kwargs)\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n",
      "result = forward_call(*args, **kwargs)\n",
      "File \"/root/.local/lib/python3.10/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py\", line 1528, in forward\n",
      "transformer_outputs = self.model(\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "return self._call_impl(*args, **kwargs)\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n",
      "result = forward_call(*args, **kwargs)\n",
      "File \"/root/.local/lib/python3.10/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py\", line 1219, in forward\n",
      "layer_outputs = decoder_layer(\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "return self._call_impl(*args, **kwargs)\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n",
      "result = forward_call(*args, **kwargs)\n",
      "File \"/root/.local/lib/python3.10/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py\", line 915, in forward\n",
      "hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "return self._call_impl(*args, **kwargs)\n",
      "File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n",
      "result = forward_call(*args, **kwargs)\n",
      "File \"/root/.local/lib/python3.10/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py\", line 753, in forward\n",
      "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "File \"/root/.local/lib/python3.10/site-packages/transformers/models/qwen2_moe/modeling_qwen2_moe.py\", line 240, in apply_rotary_pos_emb\n",
      "cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
      "RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)bugI found that there is a device mismatch in the apply_rotary_pos_emb function in modeling_qwen2_moe.py, where both cos and sin are on the CPU, but q, k, and position_ids are on CUDA.defapply_rotary_pos_emb(q,k,cos,sin,position_ids,unsqueeze_dim=1):cos=cos[position_ids].unsqueeze(unsqueeze_dim)sin=sin[position_ids].unsqueeze(unsqueeze_dim)q_embed=(q*cos)+(rotate_half(q)*sin)k_embed=(k*cos)+(rotate_half(k)*sin)returnq_embed,k_embedIt's quite strange. I'm not sure what the issue is, but when I made the following temporary modifications, it was able to run normally.：if position_ids.device != cos.device:\n",
      "        +cos = cos.to(position_ids.device)\n",
      "        -# print(f\"Moved cos to device: {cos.device}\")\n",
      "        +sin = sin.to(position_ids.device)\n",
      "        -# print(f\"Moved sin to device: {sin.device}\")Expected behaviorHow can I resolve this device inconsistency issue? Is it due to my configuration?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_786.txt:\n",
      "Title: Add Restormer\n",
      "URL: https://github.com/huggingface/transformers/issues/22372\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_792.txt:\n",
      "Title: Add InternImage\n",
      "URL: https://github.com/huggingface/transformers/issues/22240\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_745.txt:\n",
      "Title: generation error when same token in \"forced_eos_token_id\" and \"supress_token\" parameter.\n",
      "URL: https://github.com/huggingface/transformers/issues/24099\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_779.txt:\n",
      "Title: Add SwiftFormer\n",
      "URL: https://github.com/huggingface/transformers/issues/22685\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_744.txt:\n",
      "Title: Add SPTSv2\n",
      "URL: https://github.com/huggingface/transformers/issues/24235\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_977.txt:\n",
      "Title: [RFC] introduceconfig.trained_precision\n",
      "URL: https://github.com/huggingface/transformers/issues/11209\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_793.txt:\n",
      "Title: export clip to text encoder and image encoder two onnxs\n",
      "URL: https://github.com/huggingface/transformers/issues/22221\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_787.txt:\n",
      "Title: Multi-node training with Deepspeed hangs whenfull_determinism = True\n",
      "URL: https://github.com/huggingface/transformers/issues/22363\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_208.txt:\n",
      "Title: index is out of bounds for dimension with size 0 (after  4.41.2)\n",
      "URL: https://github.com/huggingface/transformers/issues/32321\n",
      "Body:\n",
      "System Infosee this issue for more details:kijai/ComfyUI-moondream#19Getting this error post this version which is preventing usage of the latest version. I'll try digging for a fix for this but otherwise if seems like an easy pick might be worth fixing.Who can help?@amyeroberts@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionkijai/ComfyUI-moondream#19Expected behaviorkijai/ComfyUI-moondream#19\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_552.txt:\n",
      "Title: DETR: IndexError: Caught IndexError in replica 0 on device 0. IndexError: index 8 is out of bounds for dimension 0 with size 8\n",
      "URL: https://github.com/huggingface/transformers/issues/28740\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.1Platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.37Python version: 3.10.12Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: 0.26.1Accelerate config: \tnot foundPyTorch version (GPU?): 2.1.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: YesUsing distributed or parallel set-up in script?: Not explicitly, but Trainer is picking up 2 GPUsWho can help?@amyerobertsHi. I'm getting the error in the title trying to reproducethis example. The error is real. I don't know what caused it, but I've narrowed the cause to DETR receivingBatchSize x NumGPUsnumber of targets, but expecting onlyBatchSizewhich causes the overflow. If I limit the amount of GPUs to 1 (viaCUDA_VISIBLE_DEVICES=0, for example), it runs ok.Here's the stack trace:Traceback (most recent call last):\n",
      "  File \"/home/mgruner/cellphones-in-the-wild/./train.py\", line 116, in <module>\n",
      "    trainer.train()\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/trainer.py\", line 1869, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/trainer.py\", line 2768, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/trainer.py\", line 2791, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 185, in forward\n",
      "    outputs = self.parallel_apply(replicas, inputs, module_kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 200, in parallel_apply\n",
      "    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 110, in parallel_apply\n",
      "    output.reraise()\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/_utils.py\", line 694, in reraise\n",
      "    raise exception\n",
      "IndexError: Caught IndexError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py\", line 1603, in forward\n",
      "    loss_dict = criterion(outputs_loss, labels)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py\", line 2202, in forward\n",
      "    indices = self.matcher(outputs_without_aux, targets)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py\", line 2330, in forward\n",
      "    indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n",
      "  File \"/opt/pyenv/versions/cellphones-in-the-wild/lib/python3.10/site-packages/transformers/models/detr/modeling_detr.py\", line 2330, in <listcomp>\n",
      "    indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n",
      "IndexError: index 8 is out of bounds for dimension 0 with size 8InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionFollow this tutorial:https://huggingface.co/docs/transformers/tasks/object_detectionExpected behaviorI expect the model to train.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_234.txt:\n",
      "Title: Cannot build documentation on Mac OS\n",
      "URL: https://github.com/huggingface/transformers/issues/32203\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0.dev0Platform: macOS-14.3-arm64-arm-64bitPython version: 3.8.19Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.32.1Accelerate config:    not foundPyTorch version (GPU?): 2.1.2 (False)Tensorflow version (GPU?): 2.13.0 (False)Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)Jax version: 0.4.13JaxLib version: 0.4.13Using distributed or parallel set-up in script?: noWho can help?@stevhliu- N.B. fix found and PR to be made on doc-builder. Raising issue here to document incase anyone else runs into it in the meanwhile.InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun steps to build documentation as described inhttps://github.com/huggingface/transformers/tree/main/docson Mac OS.Initial build docs for transformers docs/source/en/ /var/folders/g7/h9hst8551g74rd1jsf7txvj40000gn/T/tmpqf9yjhon/transformers/main/en\n",
      "Building the MDX files:  49%|███████████████████████████████████████████████████████████████████▌                                                                       | 209/430 [00:10<00:14, 15.64it/s]/Users/jon/repos/github.com/huggingface/transformers/src/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Building the MDX files:  58%|████████████████████████████████████████████████████████████████████████████████▏                                                          | 248/430 [00:13<00:09, 18.72it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jon/.pyenv/versions/3.8.19/envs/transformers/lib/python3.8/site-packages/doc_builder/build_doc.py\", line 197, in build_mdx_files\n",
      "    content, new_anchors, source_files, errors = resolve_autodoc(\n",
      "  File \"/Users/jon/.pyenv/versions/3.8.19/envs/transformers/lib/python3.8/site-packages/doc_builder/build_doc.py\", line 123, in resolve_autodoc\n",
      "    doc = autodoc(\n",
      "  File \"/Users/jon/.pyenv/versions/3.8.19/envs/transformers/lib/python3.8/site-packages/doc_builder/autodoc.py\", line 490, in autodoc\n",
      "    methods = find_documented_methods(obj)\n",
      "  File \"/Users/jon/.pyenv/versions/3.8.19/envs/transformers/lib/python3.8/site-packages/doc_builder/autodoc.py\", line 431, in find_documented_methods\n",
      "    superclasses = clas.mro()[1:]\n",
      "  File \"/Users/jon/repos/github.com/huggingface/transformers/src/transformers/utils/import_utils.py\", line 1526, in __getattribute__\n",
      "    requires_backends(cls, cls._backends)\n",
      "  File \"/Users/jon/repos/github.com/huggingface/transformers/src/transformers/utils/import_utils.py\", line 1514, in requires_backends\n",
      "    raise ImportError(\"\".join(failed))\n",
      "ImportError:\n",
      "TFBertTokenizer requires the tensorflow_text library but it was not found in your environment. You can install it with pip as\n",
      "explained here: https://www.tensorflow.org/text/guide/tf_text_intro.\n",
      "Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jon/.pyenv/versions/transformers/bin/doc-builder\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/jon/.pyenv/versions/3.8.19/envs/transformers/lib/python3.8/site-packages/doc_builder/commands/doc_builder_cli.py\", line 47, in main\n",
      "    args.func(args)\n",
      "  File \"/Users/jon/.pyenv/versions/3.8.19/envs/transformers/lib/python3.8/site-packages/doc_builder/commands/preview.py\", line 175, in preview_command\n",
      "    source_files_mapping = build_doc(\n",
      "  File \"/Users/jon/.pyenv/versions/3.8.19/envs/transformers/lib/python3.8/site-packages/doc_builder/build_doc.py\", line 367, in build_doc\n",
      "    anchors_mapping, source_files_mapping = build_mdx_files(\n",
      "  File \"/Users/jon/.pyenv/versions/3.8.19/envs/transformers/lib/python3.8/site-packages/doc_builder/build_doc.py\", line 230, in build_mdx_files\n",
      "    raise type(e)(f\"There was an error when converting {file} to the MDX format.\\n\" + e.args[0]) from e\n",
      "ImportError: There was an error when converting docs/source/en/model_doc/bert.md to the MDX format.\n",
      "\n",
      "TFBertTokenizer requires the tensorflow_text library but it was not found in your environment. You can install it with pip as\n",
      "explained here: https://www.tensorflow.org/text/guide/tf_text_intro.\n",
      "Please note that you may need to restart your runtime after installation.tensorflow_textis unavailable on Mac OSX.The error is the result ofmro()being called bydoc-builder's autodoc on the dummyTFBertTokenizerfromtransformers.utils.dummy_tensorflow_text_objects. This call results in__getattribute__being called onDummyObjectfromtransformers.utils.import_utils, which callsrequires_backendsthat throws theImportError.Expected behaviorEither:Docs can be built, without auto generated documentation for the platform specific dependencies.Building documentation is not supported on macOS and is documented inhttps://github.com/huggingface/transformers/blob/main/docs/README.md1 is probably preferable. Documentation of platform specific dependencies will still be built by CI on Github actions.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_585.txt:\n",
      "Title: causal_mask in GPT2Attention should not be broadcastable across the seq_len\n",
      "URL: https://github.com/huggingface/transformers/issues/28215\n",
      "Body:\n",
      "System InfoPython             : 3.8.2torch                : 2.2.0.dev20231207+cu121transformers    : 4.31.0torchvision       : 0.17.0.dev20231207+cu121cuda version    : 12.1Intransformers.models.gpt2.modeling_gpt2.GPT2Attentionhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L202thecausal_maskmust have the same shape for the last 2 dims, otherwise if themax_position_embeddings=1while the sequence length is longer than 1, the resulted attention weights leads to attending the future tokens. See the steps to reproduce the behavior for details. Normally, one wouldn't setmax_position_embeddings=1, but nevertheless the broadcasting should not happen.Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionCode to reproduce the issue:import torch\n",
      "import transformers\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
      "\n",
      "bsz, seq_len, hid = 2, 3, 4\n",
      "fig, axes = plt.subplots(ncols=3, figsize=(9,2))\n",
      "for n_positions, ax in zip([1, 2, seq_len], axes):\n",
      "    attn = transformers.models.gpt2.modeling_gpt2.GPT2Attention(transformers.GPT2Config(n_embd=hid, \n",
      "                                                                                        n_layer=1,\n",
      "                                                                                        n_head=1,\n",
      "                                                                                        n_positions=n_positions))\n",
      "    ax.axis(False)\n",
      "    ax.set_title('attn_weights, n_positions=%d' % n_positions, fontsize=9)\n",
      "    attn_input = torch.randn(bsz, seq_len, hid)\n",
      "    try:\n",
      "        attn_output, _, attn_weights = attn(attn_input, output_attentions=True)\n",
      "    except Exception as e:\n",
      "        print('n_positions=%d' % n_positions, attn_input.shape, 'attn_output', 'ERROR:', e)\n",
      "        continue\n",
      "    print('n_positions=%d' % n_positions, 'attn_input', attn_input.shape, 'attn_output', attn_output.shape, 'attn_weights', attn_weights.shape)\n",
      "    \n",
      "    divider = make_axes_locatable(ax)\n",
      "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
      "    im = ax.imshow(attn_weights[0, 0].data.cpu().numpy())\n",
      "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
      "plt.show()Output:n_positions=1 attn_input torch.Size([2, 3, 4]) attn_output torch.Size([2, 3, 4]) attn_weights torch.Size([2, 1, 3, 3])\n",
      "n_positions=2 torch.Size([2, 3, 4]) attn_output ERROR: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 3\n",
      "n_positions=3 attn_input torch.Size([2, 3, 4]) attn_output torch.Size([2, 3, 4]) attn_weights torch.Size([2, 1, 3, 3])Expected behaviorThere should be some error message, for example triggered byassert attn_weights.shape[-2:] == causal_mask.shape[-2:], 'attn_weights and causal_mask must have the same seq length'\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_426.txt:\n",
      "Title: Updatemake-fixupto make sure image processor tests present\n",
      "URL: https://github.com/huggingface/transformers/issues/30384\n",
      "Body:\n",
      "Feature requestUpdatemake-fixupto make sure image processor tests presentSeethis commentMotivationMakemake-fixupmore robustYour contributionI can work on it\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_340.txt:\n",
      "Title: stop_stringsArgument inmodel.generate()Results in Exception if Generation Completes Withoutstop_stringBeing Generated\n",
      "URL: https://github.com/huggingface/transformers/issues/31435\n",
      "Body:\n",
      "System Infotransformers==4.41.2Who can help?@ganteany thoughts here?InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI'm also having issues with the newgenerate()changes when using anystop_stringsargument.Minimal reproducer:Generation with nostop_stringsworks>>> import transformers\n",
      ">>> model = transformers.AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      ">>> tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "\n",
      ">>> output_ids = model.generate(tokenizer=tokenizer, max_new_tokens=4)\n",
      ">>> print(tokenizer.decode(output_ids)[0])\n",
      "<|endoftext|> The U.SGeneration with unseenstop_stringsfails>>> output_ids = model.generate(tokenizer=tokenizer, max_new_tokens=4, stop_strings=\"a\")\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/root/outlines/.myenv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/outlines/.myenv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1661, in generate\n",
      "    prepared_stopping_criteria = self._get_stopping_criteria(\n",
      "  File \"/root/outlines/.myenv/lib/python3.10/site-packages/transformers/generation/utils.py\", line 927, in _get_stopping_criteria\n",
      "    criteria.append(StopStringCriteria(stop_strings=generation_config.stop_strings, tokenizer=tokenizer))\n",
      "  File \"/root/outlines/.myenv/lib/python3.10/site-packages/transformers/generation/stopping_criteria.py\", line 276, in __init__\n",
      "    self.embedding_vec, self.max_valid_positions, self.max_valid_end_lens = self.clean_and_embed_tokens_with_cache(\n",
      "  File \"/root/outlines/.myenv/lib/python3.10/site-packages/transformers/generation/stopping_criteria.py\", line 293, in clean_and_embed_tokens_with_cache\n",
      "    embedding_vec, max_valid_positions, max_valid_end_lens = self._stop_string_create_embedding_vec(\n",
      "  File \"/root/outlines/.myenv/lib/python3.10/site-packages/transformers/generation/stopping_criteria.py\", line 376, in _stop_string_create_embedding_vec\n",
      "    max_valid_positions = max(\n",
      "ValueError: max() arg is an empty sequenceGeneration with seenstop_stringsworks>>> output_ids = model.generate(tokenizer=tokenizer, max_new_tokens=4, stop_strings=\"The\")\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.Desired behavior is that even ifstop_stringsisn't seen by the end of the sequence it generates successfully.It may have been introduced in0d84901Expected behaviormodel.generate(stop_string=...)is successful even ifstop_stringisn't encountered.ValueError: max() arg is an empty sequencedoesn't occur.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_432.txt:\n",
      "Title: If a training job job failed MLFlow will not be reported and MLFlow shows job still running\n",
      "URL: https://github.com/huggingface/transformers/issues/30333\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.0Platform: Linux-6.8.6-arch1-1-x86_64-with-glibc2.39Python version: 3.11.7Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: 0.27.0PyTorch version (GP?): 2.2.2+rocm5.7 (True)Jax version: not installedJaxLib version: not installedUsing GPU in script?: TrueUsing distributed or parallel set-up in script?: <FalseWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSetup MLFlow integration correctly,Run a job.The job failed due to OOM error.Go to MLFlow UI and the job experiment shows status \"Running\"Expected behaviorMLFlow callback should report the job as failure and call end_run() instead of keeping \"Running \"status.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_26.txt:\n",
      "Title: Mask2FormerImageProcessor - fails to process multichannel image\n",
      "URL: https://github.com/huggingface/transformers/issues/33295\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.4Platform: Linux-6.5.0-1027-oem-x86_64-with-glibc2.35Python version: 3.11.9Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.0 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noUsing GPU in script?: yesGPU type: NVIDIA RTX A4500 Laptop GPUWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionColabExpected behaviorThe image processing should be handled correctly.It looks like theget_max_height_widthis using a default parameter rather then a valueinput_data_formatset in the class constructor.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_32.txt:\n",
      "Title: TypeError: MistralForSequenceClassification.forward() got an unexpected keyword argument 'token_type_ids'\n",
      "URL: https://github.com/huggingface/transformers/issues/33280\n",
      "Body:\n",
      "System InfoI got this error when I tried to use sentiment classification pipeline with  \"nvidia/Mistral-NeMo-Minitron-8B-Base\". It works fine with llama 3.1.TypeError: MistralForSequenceClassification.forward() got an unexpected keyword argument 'token_type_ids'transformers              4.43.4See code below:Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionimport torch \n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline \n",
      "\n",
      "torch.random.manual_seed(0) \n",
      "\n",
      "model_id = \"nvidia/Mistral-NeMo-Minitron-8B-Base\" # not working\n",
      "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # working\n",
      "model = AutoModelForSequenceClassification.from_pretrained( \n",
      "    model_id,  \n",
      "    device_map=\"auto\",  \n",
      "    torch_dtype=torch.bfloat16 \n",
      ") \n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
      "\n",
      "\n",
      "\n",
      "pipe = pipeline( \n",
      "    \"sentiment-analysis\", \n",
      "    model=model, \n",
      "    tokenizer=tokenizer, \n",
      ") \n",
      "\n",
      "\n",
      "output = pipe(\"hello how are you today?\")\n",
      "print(output)\n",
      "# TypeError: MistralForSequenceClassification.forward() got an unexpected keyword argument 'token_type_ids'Expected behaviorexpected to be able to run the code\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_397.txt:\n",
      "Title: Cannot restore FSDP checkpoint with LOCAL_STATE_DICT\n",
      "URL: https://github.com/huggingface/transformers/issues/30811\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.1Platform: Linux-5.15.148.2-2.cm2-x86_64-with-glibc2.35Python version: 3.10.2Huggingface_hub version: 0.23.0Safetensors version: 0.4.2Accelerate version: 0.29.1Accelerate config:    not foundPyTorch version (GPU?): 2.1.2.1+gita8e7c98 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: YesUsing distributed or parallel set-up in script?: FSDPWho can help?@pacman100@muellerzrInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI used FSDP with fsdp_state_dict_type = LOCAL_STATE_DICTThe accelerate config is like belowcompute_environment: LOCAL_MACHINE                                                                                                  \n",
      "debug: false                                                                                                                        \n",
      "distributed_type: FSDP                                                                                                              \n",
      "downcast_bf16: 'no'                                                                                                                 \n",
      "fsdp_config:                                                                                                                        \n",
      "  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP                                                                                     \n",
      "  fsdp_backward_prefetch: BACKWARD_PRE                                                                                              \n",
      "  fsdp_cpu_ram_efficient_loading: true                                                                                              \n",
      "  fsdp_forward_prefetch: false                                                                                                      \n",
      "  fsdp_offload_params: false                                                                                                        \n",
      "  fsdp_sharding_strategy: FULL_SHARD                                                                                                \n",
      "  fsdp_state_dict_type: LOCAL_STATE_DICT                                                                                            \n",
      "  fsdp_sync_module_states: true                                                                                                     \n",
      "  fsdp_use_orig_params: true                                                                                                        \n",
      "main_training_function: main                                                                                                        \n",
      "mixed_precision: bf16                                                                                                               \n",
      "rdzv_backend: c10d                                                                                                                  \n",
      "same_network: true                                                                                                                  \n",
      "num_machines: 1                                                                                                                     \n",
      "num_processes: 1                                                                                                                    \n",
      "tpu_env: []                                                                                                                         \n",
      "tpu_use_cluster: false                                                                                                              \n",
      "tpu_use_sudo: false                                                                                                                 \n",
      "use_cpu: falseThe checkpoint structure is like below./trainer_state.json\n",
      "./rng_state_1.pth\n",
      "./pytorch_model_fsdp_rank1.bin\n",
      "./pytorch_model_fsdp_rank0.bin\n",
      "./pytorch_model_fsdp_rank4.bin\n",
      "./rng_state_5.pth\n",
      "./rng_state_4.pth\n",
      "./rng_state_2.pth\n",
      "./rng_state_3.pth\n",
      "./pytorch_model_fsdp_rank6.bin\n",
      "./rng_state_6.pth\n",
      "./pytorch_model_fsdp_rank2.bin\n",
      "./scheduler.pt\n",
      "./rng_state_7.pth\n",
      "./pytorch_model_fsdp_rank5.bin\n",
      "./optimizer_0\n",
      "./optimizer_0/__7_0.distcp\n",
      "./optimizer_0/__1_0.distcp\n",
      "./optimizer_0/.metadata\n",
      "./optimizer_0/__3_0.distcp\n",
      "./optimizer_0/__0_0.distcp\n",
      "./optimizer_0/__4_0.distcp\n",
      "./optimizer_0/__2_0.distcp\n",
      "./optimizer_0/__6_0.distcp\n",
      "./optimizer_0/__5_0.distcp\n",
      "./pytorch_model_fsdp_rank3.bin\n",
      "./pytorch_model_fsdp_rank7.bin\n",
      "./rng_state_0.pthWhen I try to restore the checkpoint fromtrainer.train(resume_from_checkpoint=\"/home/user/checkpoint-10\")I got errortraining.py 146 <module>     \n",
      "main()                                                                                                                              \n",
      "                                                                                                                                    \n",
      "training.py 125 main                                                                                                                \n",
      "train_results = trainer.train(resume_from_checkpoint=checkpoint)                                                                    \n",
      "                                                                                                                                    \n",
      "sft_trainer.py 360 train                                          \n",
      "output = super().train(*args, **kwargs)                                                                                             \n",
      "                                                                  \n",
      "trainer.py 1859 train                                                                                                               \n",
      "return inner_training_loop(                                                                                                         \n",
      "                                                                  \n",
      "trainer.py 2037 _inner_training_loop                              \n",
      "self._load_from_checkpoint(resume_from_checkpoint, self.model_wrapped)\n",
      "                                                                  \n",
      "trainer.py 2431 _load_from_checkpoint\n",
      "raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\")                                                      \n",
      "                                 \n",
      "ValueError:                                                                                                                         \n",
      "Can't find a valid checkpoint at /home/user/checkpoint-10If I used SHARDED_STATE_DICT, I don't have this error.Expected behaviorExpect the checkpoint can be restored\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1016.txt:\n",
      "Title: CharacterBERT\n",
      "URL: https://github.com/huggingface/transformers/issues/9061\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_618.txt:\n",
      "Title: Allow passing 2D attention mask\n",
      "URL: https://github.com/huggingface/transformers/issues/27640\n",
      "Body:\n",
      "Feature requestAllow passing a 2D attention mask inmodel.forward.MotivationWith this feature, it would be much easier to avoid cross-context contamination during pretraining and supervised finetuning when packing the sequences together for more efficient training.Here is an example usecase discussed in (huggingface/trl#805):Your contributionUpon investigation into the source code, I found the current logic of initializing attention masks is mostly a fixed code snippet encoded in each model:ifgetattr(self.config,\"_flash_attn_2_enabled\",False):# 2d mask is passed through the layersattention_mask=attention_maskif(attention_maskisnotNoneand0inattention_mask)elseNoneelse:# 4d mask is passed through the layersattention_mask=_prepare_4d_causal_attention_mask(attention_mask, (batch_size,seq_length),inputs_embeds,past_key_values_length)To enable this behavior may require hacking into each model. I should be able to handle part of them and submit a draft PR. But before that, I want to know if this feature request is reasonable.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1002.txt:\n",
      "Title: Allowdo_lower_case=Truefor any tokenizer\n",
      "URL: https://github.com/huggingface/transformers/issues/10121\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_142.txt:\n",
      "Title: Removing last element of class_queries_logits is not appropriate when do_reduce_labels is set to false.\n",
      "URL: https://github.com/huggingface/transformers/issues/32630\n",
      "Body:\n",
      "https://github.com/huggingface/transformers/blame/50837f20608e9266e3b9a930550fa6104230ccc7/src/transformers/models/mask2former/image_processing_mask2former.py#L998I see that in the mentioned line we havemasks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]which removes last class probability (may be the null class). It's not always true. For example if we don't set do_reduce_labels to true we need the background class as well. Or suppose we have two classes one for background and the other foreground. In that case when argmax is applied on dimension 1 afterwards the argmax always chooses 0 index because we have 1 element in that dimension.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_630.txt:\n",
      "Title: Add Flash Attention 2.0 for T5 Family\n",
      "URL: https://github.com/huggingface/transformers/issues/27441\n",
      "Body:\n",
      "Encountered the following when trying to incorporate Flash attention into a previously devved byt5-small finetuning script.Code to produce:from transformers import T5ForConditionalGeneration, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
      "\n",
      "model_path = \"google/byt5-small\"\n",
      "model = T5ForConditionalGeneration.from_pretrained(model_path,\n",
      "                                                   use_flash_attention_2=True,\n",
      "                                                   )Error:ValueError: The current architecture does not support Flash Attention 2.0. Please open an issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_156.txt:\n",
      "Title: [BUG] ModelCard do not write true optimizer\n",
      "URL: https://github.com/huggingface/transformers/issues/32560\n",
      "Body:\n",
      "This code seems to be outdated and does not consider theoptimparameter. I might submit a PR to fix it.transformers/src/transformers/modelcard.pyLines 876 to 882\n",
      "      ine4522feiftrainer.args.adafactor:hyperparameters[\"optimizer\"]=\"Adafactor\"else:hyperparameters[\"optimizer\"]=(f\"Adam with betas=({trainer.args.adam_beta1},{trainer.args.adam_beta2}) and\"f\" epsilon={trainer.args.adam_epsilon}\")\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_803.txt:\n",
      "Title: add MaxViT [TF]\n",
      "URL: https://github.com/huggingface/transformers/issues/21515\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_817.txt:\n",
      "Title: add MeMViT model\n",
      "URL: https://github.com/huggingface/transformers/issues/20545\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_195.txt:\n",
      "Title: must be real number, not SymFloat for Qwen/Qwen 7B Model with torch.compile inductor backend.\n",
      "URL: https://github.com/huggingface/transformers/issues/32392\n",
      "Body:\n",
      "System InfoTransformers : 4.38.2Torch : 2.3.0 +cpuPython : 3.8Faced \"must be real number, not SymFloat\" error when running Qwen/Qwen-7B Model with auto tokenizer and auto model class from pretrained with torch.compile API.Let me know if any other experiments need to be run.Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionLoad Qwen/Qwen 7B with auto pretrained API.Run with torch.compile(model)Expected behaviorError : must be real number, not SymFloat\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_183.txt:\n",
      "Title: TFCLIPModel.resize_token_embeddings() not working\n",
      "URL: https://github.com/huggingface/transformers/issues/32430\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.4Platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35Python version: 3.12.0Huggingface_hub version: 0.23.1Safetensors version: 0.4.2Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): not installed (NA)Tensorflow version (GPU?): 2.17.0 (True)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@amyeroberts@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI am attempting to extend the token size and embedding size of TFCLIPModel. I successfully added the new tokens to the tokenizer, but I can't resize the token embedding in the model. I keep getting TFCLIPMainLayer does not have attribute \"get_input_embeddings.I have tried subclassing and writing a method to get the embeddings manually but the parent class does not expose the text_model (or vision_model) attributes under TFCLIPMainLayer. I'm probably not going about this in the right way and was wondering if there is a better way to do this?from transformers import TFCLIPModel\n",
      "# Load the pretrained CLIP model\n",
      "clip_model = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "\n",
      "# Resize the token embeddings to match the new vocabulary size\n",
      "clip_model.resize_token_embeddings(50000)Expected behaviorUpdated input embedding to match tokenizer.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_197.txt:\n",
      "Title: 'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0,\n",
      "URL: https://github.com/huggingface/transformers/issues/32382\n",
      "Body:\n",
      "System Infodatasets==2.20.0torch==1.10.2+cu111torchvision==0.11.3+cu111transformers==4.42.4detectron2==0.6opencv-contrib-python==4.10.0.84seqeval==1.2.2accelerate==0.32.1wandbsentencepieceeasyocrsetuptools==59.5.0python-bidi==0.4.2Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI am transferring the relation extraction on LayoutLMv2 from transformers==4.6 to the latest transformers.I use the original re.py code with the LayoutLMv2 built-in transfoemers.However, the model always doesn't work.The output is as following:{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.33}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.34}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.35}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.36}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.37}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.38}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.39}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.4}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.41}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.43}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.44}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.45}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.46}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.47}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.48}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.49}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.5}{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 5.51}The configuration is:\"--standalone\",\"--nnodes=1\",\"--nproc_per_node=1\",\"--master_port=12098\",\"examples/run_xfun_re_inf.py\",\"--model_name_or_path=/home/mypath/layoutxlm_base\",\"--output_dir=/home/myproj/output\",\"--do_train\",\"--do_eval\",\"--lang=zh\",\"--max_steps=5000\",\"--per_device_train_batch_size=2\",\"--warmup_ratio=0.1\",\"--fp16\",\"--learning_rate=5e-5\",\"--logging_steps=1\",Could someone please help solve this problem? Thank you very much!!Expected behaviorGet the normal result\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_140.txt:\n",
      "Title: Odd output from falcon-mamba-7b on mps device\n",
      "URL: https://github.com/huggingface/transformers/issues/32634\n",
      "Body:\n",
      "System Infotransformersversion: 4.45.0.dev0Platform: macOS-14.6.1-arm64-arm-64bitPython version: 3.12.4Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.4.0 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: NoWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "import torch\n",
      "\n",
      "model_id = \"tiiuae/falcon-mamba-7b\"\n",
      "torch_dtype = torch.bfloat16\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch_dtype).to(\"mps\")\n",
      "print(model)\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "\n",
      "input_sequence = tokenizer.encode(\"the largest crater on the moon is\", return_tensors=\"pt\").to('mps')\n",
      "print(input_sequence)\n",
      "\n",
      "output_sequence = model.generate(input_sequence, max_new_tokens=100, do_sample=True)\n",
      "\n",
      "print(tokenizer.decode(output_sequence[0]))Output:the largest crater on the moon is�haped, and the number “13” is very common in occult and Satanic symbology. It has been proposed that some Illuminati think that 666 years of rule is good enough! All the 6's adds up to a total of 21, which is half of 42 (2 x 42 is 84), which when divided by 3 (Satan's signature) adds up to 28, (7 x 4) andExpected behaviorOutput is better when device = \"cpu\":from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "import torch\n",
      "\n",
      "model_id = \"tiiuae/falcon-mamba-7b\"\n",
      "torch_dtype = torch.bfloat16\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch_dtype)\n",
      "print(model)\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "\n",
      "input_sequence = tokenizer.encode(\n",
      "    \"the largest crater on the moon is\", return_tensors=\"pt\")\n",
      "print(input_sequence)\n",
      "\n",
      "output_sequence = model.generate(input_sequence, max_new_tokens=100, do_sample=True)\n",
      "\n",
      "print(tokenizer.decode(output_sequence[0]))Output:the largest crater on the moon is called Orientale; the largest on Mars is named Hellas, after the mythical Greek home of Zeus on earth).\n",
      "It looks like cratering occurred, as on Earth, from a single “shot,” but on a much grander scale. If Earth were as large as Mars and got struck by the same number of asteroids, the Earth’s surface would look as battered as the moon’s. But Earth’s greater gravity and mass (the Earth is 4.5\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_154.txt:\n",
      "Title: Mamba-2 Exploding Gradients\n",
      "URL: https://github.com/huggingface/transformers/issues/32570\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: Linux-5.15.134+release+2.10.0r8-amd64-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.1.0a0+29c30b1 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: distributed (using accelerate launch command)Using GPU in script?: yesGPU type: NVIDIA A100-SXM4-80GB  (2 of them)Who can help?@ArthurZucker@muellerz@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionLoad any training/ eval dataset you want, tokenize, split into 2k partsUse the model config from mamba-2 configuration with those settings (the rest is default):config = config_class(\n",
      "        vocab_size=32000,\n",
      "        hidden_size=1024,\n",
      "        num_hidden_layers=12,\n",
      "        head_dim=128,\n",
      "        expand=2,\n",
      "        num_heads=16,\n",
      "        n_groups=8,\n",
      "        state_size=128,\n",
      "        use_cache=False,\n",
      "        is_training=True,\n",
      " \tresidual_in_fp32=True,\n",
      " \tnorm_before_gate=False,\n",
      " \trms_norm=True, # it can be False, it makes the issue appear a bit later but it still does appear\n",
      "    )Set training arguments and trainer settings like that:training_args = TrainingArguments(\n",
      "    per_device_train_batch_size=32,\n",
      "    per_device_eval_batch_size=32,\n",
      "    gradient_checkpointing=False,\n",
      "    gradient_accumulation_steps=16, # set it so the total batch size is 1M tokens\n",
      "    load_best_model_at_end=False,\n",
      "    num_train_epochs=1,\n",
      "    eval_strategy=\"steps\",\n",
      "    learning_rate=4e-4,\n",
      "    fp16=not torch.cuda.is_bf16_supported(),\n",
      "    bf16=torch.cuda.is_bf16_supported(),\n",
      "    bf16_full_eval=torch.cuda.is_bf16_supported(),\n",
      "    fp16_full_eval=not torch.cuda.is_bf16_supported(),\n",
      "    logging_steps=10,\n",
      "    adam_beta1=0.9,\n",
      "    adam_beta2=0.95,\n",
      "    adam_epsilon=1e-7,\n",
      "    optim=\"adamw_torch\", \n",
      "    save_total_limit=4000,\n",
      "    eval_steps=40,\n",
      "    save_steps=500,\n",
      "    save_strategy=\"steps\",\n",
      "    weight_decay=0.1,\n",
      "    max_grad_norm=1.0,\n",
      "    seed=seed,\n",
      "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
      "    warmup_ratio=0.01,\n",
      "    lr_scheduler_kwargs={\"min_lr_rate\": 0.1},\n",
      "    push_to_hub=True,\n",
      "    hub_private_repo=True,\n",
      "    output_dir=f\"mamba2training\")\n",
      "\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=dataset,\n",
      "    eval_dataset=eval_dataset,\n",
      "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
      ")Train using \"accelerate launch --mixed_precision bf16 train.py\" (if you don't use it, it will crash with \"Nonetype object is not a mapping\" error, it's related to mamba2 and triton not supporting multi threading I believe)After around 100M tokens of training (100 steps), grad_norm will reach billions and eventually infinity (while loss starts rising)Expected behaviorI believe that the script shouldn't require using accelerate launch and also considering the model is pretty small, there shouldn't be any problem with exploding gradients, the higher lr the earlier the problem appears, but from my observation it appears even when using lr as low as just 7e-5\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_168.txt:\n",
      "Title: Fix get_usable_length for StaticCache\n",
      "URL: https://github.com/huggingface/transformers/issues/32503\n",
      "Body:\n",
      "System Infotransformers: 4.45.0.dev0torch: 2.5.0.dev20240716+cpuWho can help?@ArthurZucker@gante@zucchini-nlpInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionIt's a bug only affect the new workflow\"Export to ExecuTorch\"that we're trying to enable. The methodget_usable_lengthshould be override forStaticCachewhere recompile, resizing or evicting the existing cache entry won't be applicable. This is because unlike eager ortorch.compiled model, exported model will be running in a non-python env where recompiling from the eager source isn't available.Expected behaviorThegenerationlength using the exported artifact should not exceed the maximal cache length because the model and the size of the its cache are exported statically. Whenget_usable_lengthreturns 0, it should terminate the generation.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_381.txt:\n",
      "Title: RuntimeError: unable to open file when calling from_pretrained on multiple processes after upgrading hugginface_hub to 0.23.1\n",
      "URL: https://github.com/huggingface/transformers/issues/31019\n",
      "Body:\n",
      "System Info- `transformers` version: 4.40.0\n",
      "- Platform: Linux-6.5.0-26-generic-x86_64-with-glibc2.35\n",
      "- Python version: 3.10.13\n",
      "- Huggingface_hub version: 0.23.1\n",
      "- Safetensors version: 0.4.3\n",
      "- Accelerate version: not installed\n",
      "- Accelerate config: not found\n",
      "- PyTorch version (GPU?): 2.2.0 (True)\n",
      "- Tensorflow version (GPU?): not installed (NA)\n",
      "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
      "- Jax version: not installed\n",
      "- JaxLib version: not installed\n",
      "- Using GPU in script?: Yes, 8xH100\n",
      "- Using distributed or parallel set-up in script?: Yes, PyTorch DDPWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductiondefget_vit(self):fromtransformersimportDinov2Model,Dinov2ConfigreturnDinov2Model.from_pretrained(\"facebook/dinov2-large\")Runget_vit()with torchrun with multiple nodes and multiple GPUs per node, the code would fail after downloading, saying/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.../train.py\", line 143, in main\n",
      "    model = Wrapper().cuda(local_rank)\n",
      "  File \"/root/.../train.py\", line 134, in __init__\n",
      "    self.module = DVIM()\n",
      "  File \"/root/.../model/net.py\", line 235, in __init__\n",
      "    dino = self.get_vit()\n",
      "  File \"/root/.../model/net.py\", line 311, in get_vit\n",
      "    return Dinov2Model.from_pretrained(\"facebook/dinov2-large\")\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3456, in from_pretrained\n",
      "    with safe_open(resolved_archive_file, framework=\"pt\") as f:\n",
      "RuntimeError: unable to open file </root/.cache/huggingface/hub/models--facebook--dinov2-large/snapshots/47b73eefe95e8d44ec3623f8890bd894b6ea2d6c/model.safetensors> in read-only mode: No such file or directory (2)The same code was working with the following environment:- `transformers` version: 4.40.0\n",
      "- Platform: Linux-6.5.0-26-generic-x86_64-with-glibc2.35\n",
      "- Python version: 3.10.13\n",
      "- Huggingface_hub version: 0.22.2\n",
      "- Safetensors version: 0.4.3\n",
      "- Accelerate version: not installed\n",
      "- Accelerate config: not found\n",
      "- PyTorch version (GPU?): 2.2.0 (True)\n",
      "- Tensorflow version (GPU?): not installed (NA)\n",
      "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
      "- Jax version: not installed\n",
      "- JaxLib version: not installed\n",
      "- Using GPU in script?: Yes\n",
      "- Using distributed or parallel set-up in script?: Yes, PyTorch DDPExpected behaviorThe code should work.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_18.txt:\n",
      "Title: Can't run docker because of flash attention 2?\n",
      "URL: https://github.com/huggingface/transformers/issues/33335\n",
      "Body:\n",
      "System InfoI am trying to run this gpt4o app and when trying to run docker; I get the same response every time. I have installed everything that I could and even specifically installed 4.40 to be sure that it would work properly; since it says that it does not support 4.41 or higher... I have included the system info details and the terminal output below. Any assistance or ideas on this would be extremely helpful. This is the repo:FardinHash/GPT-4otransformers-cli envtransformersversion: 4.40.0Platform: Linux-5.4.0-190-generic-x86_64-with-glibc2.29Python version: 3.8.10Huggingface_hub version: 0.24.6Safetensors version: 0.4.5Accelerate version: 0.34.2Accelerate config:    not foundPyTorch version (GPU?): 2.4.1+cu121 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:root@WorkshopTest:/var/sftp/uploads/panelTest/fffffff# docker compose up[+] Running 1/0✔ Container fffffff-gpt4o-1  Created                                                                                                                                                                    0.0sAttaching to gpt4o-1gpt4o-1  | Requirement already satisfied: flash-attn in /usr/local/lib/python3.12/site-packages (2.6.3)gpt4o-1  | Requirement already satisfied: torch in /usr/local/lib/python3.12/site-packages (from flash-attn) (2.4.1)gpt4o-1  | Requirement already satisfied: einops in /usr/local/lib/python3.12/site-packages (from flash-attn) (0.8.0)gpt4o-1  | Requirement already satisfied: filelock in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (3.15.4)gpt4o-1  | Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (4.12.2)gpt4o-1  | Requirement already satisfied: sympy in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (1.13.2)gpt4o-1  | Requirement already satisfied: networkx in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (3.3)gpt4o-1  | Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (3.1.4)gpt4o-1  | Requirement already satisfied: fsspec in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (2024.6.1)gpt4o-1  | Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (74.1.2)gpt4o-1  | Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)gpt4o-1  | Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)gpt4o-1  | Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)gpt4o-1  | Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (9.1.0.70)gpt4o-1  | Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.3.1)gpt4o-1  | Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (11.0.2.54)gpt4o-1  | Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (10.3.2.106)gpt4o-1  | Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (11.4.5.107)gpt4o-1  | Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.0.106)gpt4o-1  | Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (2.20.5)gpt4o-1  | Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)gpt4o-1  | Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/site-packages (from torch->flash-attn) (3.0.0)gpt4o-1  | Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.6.68)gpt4o-1  | Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/site-packages (from jinja2->torch->flash-attn) (2.1.5)gpt4o-1  | Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/site-packages (from sympy->torch->flash-attn) (1.3.0)gpt4o-1  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead:https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.gpt4o-1  | Traceback (most recent call last):gpt4o-1  |   File \"/usr/src/app/app.py\", line 4, ingpt4o-1  |     from bot import chatbot, model_inference, BOT_AVATAR, EXAMPLES, model_selector, decoding_strategy, temperature, max_new_tokens, repetition_penalty, top_pgpt4o-1  |   File \"/usr/src/app/bot.py\", line 31, ingpt4o-1  |     \"idefics2-8b-chatty\": Idefics2ForConditionalGeneration.from_pretrained(gpt4o-1  |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^gpt4o-1  |   File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3826, in from_pretrainedgpt4o-1  |     config = cls._autoset_attn_implementation(gpt4o-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^gpt4o-1  |   File \"/usr/local/lib/python3.12/site-packages/transformers/models/idefics2/modeling_idefics2.py\", line 1129, in _autoset_attn_implementationgpt4o-1  |     config = super()._autoset_attn_implementation(gpt4o-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^gpt4o-1  |   File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 1556, in _autoset_attn_implementationgpt4o-1  |     cls._check_and_enable_flash_attn_2(gpt4o-1  |   File \"/usr/local/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 1667, in _check_and_enable_flash_attn_2gpt4o-1  |     raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")gpt4o-1  | ImportError: FlashAttention2 has been toggled on, but it cannot be used due to the following error: Flash Attention 2 is not available. Please refer to the documentation ofhttps://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2to install Flash Attention 2.gpt4o-1 exited with code 1Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionUsing the library GPT-4o from FardinHash here on GitHub, just running the 'docker compose up' command results in this output each time...Expected behaviorGet the error output claiming that Flash Attention 2 must be installed; even though it already is.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_395.txt:\n",
      "Title: torchrun breaks with load_model_at_end and with metric_for_best_model=eval_f1 on question_answering example\n",
      "URL: https://github.com/huggingface/transformers/issues/30819\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.0.dev0Platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.31Python version: 3.10.14Huggingface_hub version: 0.23.0Safetensors version: 0.4.3Accelerate version: 0.29.3Accelerate config:    not foundPyTorch version (GPU?): 2.1.0 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: tried using ddp, but the setting is single system, multi-gpuWho can help?@muellerzr@pacman100@ArthurZucker@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI clone the main branch of transformers andpip install -e .in the clonedtransformersfolder.I then runtorchrun --nproc_per_node 2 run_qa.py   --model_name_or_path google-bert/bert- base-uncased   --dataset_name squad   --do_train   --do_eval   --per_device_train_batch_size 12   --learning_rate 3e-5   --num_train_epochs 2   --max_seq_length 384   --doc_stride 128   --output_dir /tmp/debug_squad --max_steps 20 --eval_steps 2 --save_steps 2 --save_total_limit 2 --load_best_model_at_end True --metric_for_best_model eval_f1 --max_eval_samples 20 --eval_strategy steps --save_strategy steps 2>&1 | tee scratch.logThe code errors out with KeyError: eval_f1.I believe this happens because the compute_metrics function computes the eval_f1 metric on one process but the trainer_save_checkpoint()method checks for the metric on all processes and therefore, some other process beats process 0 so it doesn't find the key leading to this error. (transformers/src/transformers/trainer.pyLine 2820\n",
      "      in1360801ifmetricsisnotNoneandself.args.metric_for_best_modelisnotNone:)Expected behaviorIdeally, the code should seamlessly run using torchrun.  There should be no key error. The trainer should be able to handle single process eval_f1 along with multi-process metric computation done in other workloads such as summarization.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_24.txt:\n",
      "Title: can't resume lora training due to wandb logging num params\n",
      "URL: https://github.com/huggingface/transformers/issues/33320\n",
      "Body:\n",
      "System InfoHi,I have some trained checkpoints that i'd like to resume fromall of them are lora checkpointsbut when resuming i get the following error in trainertrainer.train(resume_from_checkpoint=script_args.resume_from_checkpoint)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1938, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
      "    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 460, in on_train_begin\n",
      "    return self.call_event(\"on_train_begin\", args, state, control)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 507, in call_event\n",
      "    result = getattr(callback, event)(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py\", line 900, in on_train_begin\n",
      "    self.setup(args, state, model, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/integrations/integration_utils.py\", line 853, in setup\n",
      "    self._wandb.config[\"model/num_parameters\"] = model.num_parameters()\n",
      "  File \"/root/.local/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 149, in __setitem__\n",
      "    key, val = self._sanitize(key, val)\n",
      "  File \"/root/.local/lib/python3.10/site-packages/wandb/sdk/wandb_config.py\", line 258, in _sanitize\n",
      "    raise config_util.ConfigError(\n",
      "wandb.sdk.lib.config_util.ConfigError: Attempted to change value of key \"model/num_parameters\" from 0 to 266240I assume that the fact that this is a lora training is relevant because the error describes a change in number or params (which shouldn't be logged as 0 from the first place)and even though in lineintegration_util.py#L838the wandb config dict was set with allow_val_change=Truei still get the above error. in line/integration_utils.py#L853any idea on how to solve this?ThanksWho can help?@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionstep 1. train a small model with dpo lorastep 2. try to resume withtrainer.train(resume_from_checkpoint=True)while settingos.environ[\"WANDB_RESUME\"] = \"allow\"\n",
      "os.environ[\"WANDB_RUN_ID\"] = script_args.run_id # same run_id as previous run\n",
      "DPOConfig(\n",
      "    output_dir=script_args.output_dir # same previous run out dir\n",
      "    run_name=script_args.run_name # same run_name as previous run \n",
      "..\n",
      ")\n",
      "\n",
      "...\n",
      "\n",
      "trainer.train(resume_from_checkpoint=True)Expected behaviorbeing able to resume previous training\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_30.txt:\n",
      "Title: 'DepthEstimationPipeline' object has no attribute 'image_size' when num_workers > 0\n",
      "URL: https://github.com/huggingface/transformers/issues/33288\n",
      "Body:\n",
      "System InfoOS: Linux (Ubuntu 22)transformers: 4.44.2torch: 2.3.1Who can help?@NarsilInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSee code snippet below to reproduce:fromtransformersimportpipelineimporttorchpipe=pipeline(model=\"LiheYoung/depth-anything-base-hf\",device=0,num_workers=1)# To reproduce, the input needs to be an array of images, and it can just be 1 image in the array.# A single image not placed in an array will not reproduce the issue.pipe([\"<YOUR IMAGE>\"])This results in:File \"[...]/site-packages/transformers/pipelines/depth_estimation.py\", line 109, in postprocess\n",
      "    predicted_depth.unsqueeze(1), size=self.image_size[::-1], mode=\"bicubic\", align_corners=False\n",
      "AttributeError: 'DepthEstimationPipeline' object has no attribute 'image_size'Upon investigation, you will see thatself.image_sizeis set inpreprocess()which is executed by one of the workers while (I believe)postprocess()is executed on the main process. Because these 2 processes do not share the same class instance of DepthEstimationPipeline,self.image_sizeends up never being defined in the main processes's address space. This makes the DepthEstimationPipeline only work whenpreprocess()andpostprocess()are executed on the same process when num_workers=0.A quick fix would be for DepthEstimationPipeline to accept animage_sizeargument which can be passed topreprocess_params --> preprocess(). Then the user can create a pipeline by doingpipe=pipeline(model=\"LiheYoung/depth-anything-base-hf\",device=0,num_workers=1,image_size=(256,256))for example. Of course, it's not as clean since it doesn't dynamically infer the image sizes from the user's input, but it should help. Any other ideas?Expected behaviorForDepthEstimationPipeline.__call__()to execute successfully with multiprocessing.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_342.txt:\n",
      "Title: Minimum required accelerate library is not compatible\n",
      "URL: https://github.com/huggingface/transformers/issues/31433\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-6.6.13-gentoo-x86_64-AMD_Ryzen_7_PRO_6850U_with_Radeon_Graphics-with-glibc2.39Python version: 3.12.3Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.21.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu121 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?: NAWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionpip install accelerate==0.21.0pip install transformerspip install setuptoolspython[+]>>> from transformers import Trainer\n",
      "[+]>>> Trainer()\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/home/dbn/src/rockfish/tmp/venv/lib/python3.12/site-packages/transformers/trainer.py\", line 402, in __init__\n",
      "    self.create_accelerator_and_postprocess()\n",
      "  File \"/home/dbn/src/rockfish/tmp/venv/lib/python3.12/site-packages/transformers/trainer.py\", line 4535, in create_accelerator_and_postprocess\n",
      "    self.accelerator = Accelerator(**args)\n",
      "                       ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Accelerator.__init__() got an unexpected keyword argument 'use_seedable_sampler'Expected behaviorThere should be no error unexpected keyword.The requirement in setup.py for accelerate is set to >=0.21.0, but it seems that it needs to be at least 0.26.0.This issue is mentioned in#29216#28111\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_424.txt:\n",
      "Title: Removempsworkaround forisin()\n",
      "URL: https://github.com/huggingface/transformers/issues/30430\n",
      "Body:\n",
      "Feature requestRemovempsworkaround forisin()Motivation#30376introduced a workaround forisin()onmpsdevices, because PyTorch does not support that op yet:pytorch/pytorch#77764 (comment).Going forward, it'd be desirable to use the much more readableisin()version. This issue is meant to track PyTorch support ofisin()onmpsso we can remove the workaround and simplify the code.Your contributionI can submit a PR when the op is eventually supported.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_430.txt:\n",
      "Title: Schedule Free Optimizers (pytorch) and Sophia optimizer\n",
      "URL: https://github.com/huggingface/transformers/issues/30359\n",
      "Body:\n",
      "Feature requestSeehttps://github.com/facebookresearch/schedule_freeandhttps://github.com/Liuhong99/Sophia-- These optimizers have very different properties and are often useful over the existing choices.MotivationSchedule free optimizer (especially schedule-free SGD) offers increased convergence over the existing implementation with no additional memory requirements. There's some variability on the end gradients but this is an equitable engineering tradeoff in many cases. In the same vein, sophia has been shown to converge twice as fast as adam on some language modeling tasks.Your contributionThese would add in additional dependencies if included straight on, I'm not sure what the typical approach for this is. I'd be willing to implement both features but I'm unsure how the addition of dependencies should go.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_356.txt:\n",
      "Title: merge_and_unloadfor a quantized model ruins its quality\n",
      "URL: https://github.com/huggingface/transformers/issues/31293\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-5.15.0-1044-nvidia-x86_64-with-glibc2.35Python version: 3.10.0Huggingface_hub version: 0.23.0Safetensors version: 0.4.2Accelerate version: 0.30.1Accelerate config:    - compute_environment: LOCAL_MACHINE- distributed_type: NO- mixed_precision: bf16- use_cpu: False- debug: False- num_processes: 1- machine_rank: 0- num_machines: 1- gpu_ids: 0- rdzv_backend: static- same_network: True- main_training_function: main- enable_cpu_affinity: False- downcast_bf16: no- tpu_use_cluster: False- tpu_use_sudo: False- tpu_env: []PyTorch version (GPU?): 2.3.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: YesUsing distributed or parallel set-up in script?: Notrl==0.9.3Who can help?@ArthurZucker,@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionHi,I found really strange behaviour when calling.merge_and_unload()method. More precisely, this is a must-have phase if you want to further use the model with other frameworks (e.g. withvllmfor inference), but it dramatically impairs the model performance.I tested this in 6 settings on a grammar checking task with Phi-3 model:QLoRA +bf16=Truein training arguments: model quality is severely damaged (0.12 loss for a PeftModel vs 0.56 loss for a merged model)QLoRA +fp16=Truein training arguments: model quality is severely damaged (0.12 loss for a PeftModel vs 0.56 loss for a merged model)QLoRA without specifyingfp16 / bf16(correct me if I'm wrong, I believe such setting preserves the usage of torch.float32): model quality is severely damaged (0.12 loss for a PeftModel vs 0.56 loss for a merged model)LoRA +bf16=Truein training arguments: model quality is slightly damaged (0.125 loss for a PeftModel vs 0.135 loss for a merged model)LoRA +fp16=Truein training arguments: model quality is NOT damaged (0.11791 loss for a PeftModel vs 0.11796 loss for a merged model, which is due to dtype change)LoRA without specifyingfp16 / bf16: model quality is NOT damaged (0.11793883 vs 0.11793886).These observations are robust across different tasks, models, and even architectures (e.g. in the example I'm using a CasualLM, yet for sequence classification models these observations hold).I believe there may be a bug forbf16=Trueparameter in training arguments. Still, for QLoRA performance decrease occurs for other dtypes as well.For convenience, I attach the .ipynb notebooks for all the 6 settings (Github won't let me upload .ipynb, so please download these .txt files and change their extension to .ipynb). I usedtrlhere to make it easier to follow the code - I observe absolutely the same behaviour when using atransformersimplementation (with TrainingArguments, Trainer, etc.). Below I attach the code for the first setting (I'd call it the most \"erroneous\" one) with QLoRA +bf16=True:importnumpyasnpfromdatasetsimportload_dataset,DatasetDictfrompeftimportLoraConfigimporttorchfromtransformersimport(AutoModelForCausalLM,AutoTokenizer,set_seed,BitsAndBytesConfig,EarlyStoppingCallback)fromtrlimportSFTConfig,SFTTrainer,DataCollatorForCompletionOnlyLM### Model & tokenizer loading partbnb_config=BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_use_double_quant=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_compute_dtype=torch.bfloat16,\n",
      ")model_name='microsoft/Phi-3-mini-4k-instruct'model=AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config,cache_dir='../al-nlg/cache',attn_implementation='eager',trust_remote_code=True)tokenizer=AutoTokenizer.from_pretrained(model_name,model_max_length=128)tokenizer.pad_token=tokenizer.eos_tokenpeft_config=LoraConfig(r=128,lora_alpha=128,target_modules=['o_proj','qkv_proj','gate_up_proj','down_proj','lm_head'],lora_dropout=0.1,bias=\"none\",task_type=\"CAUSAL_LM\",\n",
      ")### Data loading partdata=load_dataset('juancavallotti/multilingual-gec')data=DatasetDict({'train':data['train'].select(range(1000)),'eval':data['train'].select(range(1000,2000))\n",
      "})data=data.map(lambdax: {'messages': [\n",
      "            {'role':'user','content':x['modified']},\n",
      "            {'role':'assistant','content':x['sentence']},\n",
      "        ]\n",
      "        \n",
      "    },batched=False,remove_columns=data['train'].column_names)### Trainer setting partset_seed(42)train_args=SFTConfig(output_dir='tmp',num_train_epochs=1,per_device_train_batch_size=8,per_device_eval_batch_size=8,learning_rate=3e-5,bf16=True,bf16_full_eval=False,evaluation_strategy=\"epoch\",report_to='none',gradient_checkpointing=True,gradient_checkpointing_kwargs={\"use_reentrant\":False},\n",
      ")collator=DataCollatorForCompletionOnlyLM(instruction_template='<|user|>',response_template='<|assistant|>',tokenizer=tokenizer,mlm=False)trainer=SFTTrainer(model,args=train_args,train_dataset=data['train'],eval_dataset=data['eval'],data_collator=collator,peft_config=peft_config)trainer.train()### Evaluation of a PeftModel after training (should coincide with the score we got during `trainer.train`)trainer.evaluate()['eval_loss']>>>0.1241287887096405### Merge and unload, and re-evaluate for a model after mergemerged_model=trainer.model.merge_and_unload()trainer.evaluate()['eval_loss']>>>0.5563217997550964qlora_fp16.txtqlora_float32.txtqlora_bf16.txtlora_fp16.txtlora_float32.txtlora_bf16.txtExpected behaviorI can expect a minor drop in performance but definitely not to have the loss increased 4x times. I bet there are bugs:In quantization /merge_and_unloadimplementationbf16=Trueproduces errors since even without quantization, it increases the model's loss (which does not happen if disabling this option).Kindly tell me if I can help here further.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_418.txt:\n",
      "Title: Saved weights differ from the original model\n",
      "URL: https://github.com/huggingface/transformers/issues/30543\n",
      "Body:\n",
      "System Infotransformers              4.40.1peft                      0.10.0Who can help?@sanchit-gandhi@Rocketknight1@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI have fine-tuned a GPT2 Model using SFTTrainer. I merge base model and trained adapters with the code below. I also extended the vocabulary.peft_model_path = \"checkpoint\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(peft_model_path, device_map=\"auto\")\n",
      "base_model_path = \"openai-community/gpt2\"\n",
      "base_model = GPT2LMHeadModel.from_pretrained(base_model_path, device_map=\"auto\")\n",
      "\n",
      "base_model.resize_token_embeddings(len(tokenizer))\n",
      "    \n",
      "model = PeftModel.from_pretrained(base_model, peft_model_path, device_map=\"auto\")\n",
      "merged_model = model.merge_and_unload()I test this model and the results are okay. Then I want to save this merged_model using the code below.tokenizer.save_pretrained(save_path)\n",
      "merged_model.save_pretrained(save_path)Lastly, I open the saved model with the code below.tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
      "model = AutoModelForCausalLM.from_pretrained(save_path)The model that I load from the save_path does not work well. It repeats the same token or gives random tokens from the base vocabulary.Model before SaveGPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(66156, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=66156, bias=False)\n",
      ")Loaded Model:GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(66156, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=66156, bias=False)\n",
      ")Now let's look at the weights.Model Before SaveOrderedDict([('transformer.wte.weight',\n",
      "              tensor([[-0.2500,  0.2324,  0.0162,  ...,  0.0013, -0.4432,  0.2431],\n",
      "                      [ 0.3332, -0.1894, -0.2949,  ...,  0.2883, -0.0411,  0.3148],\n",
      "                      [-0.2403, -0.1975,  0.4091,  ..., -0.3482,  0.5244,  0.1759],\n",
      "                      ...,\n",
      "                      [ 0.3374, -0.1371, -0.2627,  ..., -0.6586, -0.5067, -0.0226],\n",
      "                      [-0.1031,  0.1453, -0.9022,  ..., -0.3682,  0.4504,  0.3242],\n",
      "                      [-0.5442, -0.6574, -0.0881,  ..., -0.2370, -0.3048,  0.7317]],\n",
      "                     device='cuda:0')),\n",
      "             ('transformer.wpe.weight',\n",
      "              tensor([[-1.2274e-01, -1.5239e-01,  1.9312e-01,  ..., -2.8421e-03,\n",
      "                       -5.4589e-02,  3.1110e-02],\n",
      "                      [-1.1096e-02, -1.4371e-01, -5.5172e-02,  ...,  1.8058e-01,\n",
      "                        4.9604e-02,  4.3034e-02],\n",
      "                      [ 6.0074e-02, -2.2665e-01,  2.2515e-01,  ..., -1.2376e-02,\n",
      "                        1.3002e-01, -1.2887e-02],\n",
      "                      ...,\n",
      "                      [ 3.5125e-01, -1.1077e+00,  1.3558e-01,  ..., -2.0376e-01,\n",
      "                       -3.5680e-01,  3.1987e-01],\n",
      "                      [ 4.0268e-01, -4.6439e-01, -8.0140e-04,  ...,  1.4744e-01,\n",
      "                        1.2033e-01, -8.1738e-02],\n",
      "                      [ 2.6610e-04,  3.0272e-03, -1.7086e-03,  ..., -4.6506e-03,\n",
      "                       -2.3541e-03, -5.7855e-03]], device='cuda:0')),\n",
      "             ('transformer.h.0.ln_1.weight',\n",
      "              tensor([0.2232, 0.1820, 0.1534, 0.1917, 0.2036, 0.1948, 0.1467, 0.1865, 0.2143,\n",
      "                      0.1956, 0.2118, 0.2153, 0.1882, 0.2074, 0.1871, 0.2040, 0.2044, 0.1900,\n",
      "                      0.1952, 0.0475, 0.1909, 0.2115, 0.1971, 0.2202, 0.1998, 0.2108, 0.2303,\n",
      "                      ...\n",
      "                      0.1662, 0.1982, 0.1582, 0.1935, 0.2182, 0.2067, 0.1855, 0.1778, 0.1900,\n",
      "                      0.2124, 0.1215, 0.2092, 0.1929, 0.2434, 0.1936, 0.1948, 0.0622, 0.1852,\n",
      "                      0.1868, 0.2035, 0.2310, 0.1794, 0.1655, 0.1756, 0.2074, 0.2194, 0.2152,\n",
      "                      0.0502, 0.2294, 0.1950, 0.2149, 0.2024, 0.1727, 0.0657, 0.1919, 0.1847,\n",
      "                      0.1900, 0.1825, 0.1898], device='cuda:1')), ...]Loaded ModelOrderedDict([('transformer.wte.weight',\n",
      "              tensor([[-0.0952, -0.0785,  0.0155,  ..., -0.1458, -0.0334,  0.0052],\n",
      "                      [ 0.0234, -0.0811,  0.0049,  ...,  0.1172, -0.0847,  0.0343],\n",
      "                      [-0.1060,  0.0711,  0.1621,  ..., -0.0243, -0.1103, -0.0732],\n",
      "                      ...,\n",
      "                      [-0.0358,  0.0035,  0.0351,  ..., -0.0633, -0.0200, -0.0084],\n",
      "                      [-0.1555,  0.0488,  0.0125,  ..., -0.0582,  0.0440, -0.1661],\n",
      "                      [-0.0095,  0.1273, -0.0158,  ...,  0.0115, -0.1641, -0.0303]],\n",
      "                     device='cuda:0')),\n",
      "             ('transformer.wpe.weight',\n",
      "              tensor([[-1.2274e-01, -1.5239e-01,  1.9312e-01,  ..., -2.8421e-03,\n",
      "                       -5.4589e-02,  3.1110e-02],\n",
      "                      [-1.1096e-02, -1.4371e-01, -5.5172e-02,  ...,  1.8058e-01,\n",
      "                        4.9604e-02,  4.3034e-02],\n",
      "                      [ 6.0074e-02, -2.2665e-01,  2.2515e-01,  ..., -1.2376e-02,\n",
      "                        1.3002e-01, -1.2887e-02],\n",
      "                      ...,\n",
      "                      [ 3.5125e-01, -1.1077e+00,  1.3558e-01,  ..., -2.0376e-01,\n",
      "                       -3.5680e-01,  3.1987e-01],\n",
      "                      [ 4.0268e-01, -4.6439e-01, -8.0140e-04,  ...,  1.4744e-01,\n",
      "                        1.2033e-01, -8.1738e-02],\n",
      "                      [ 2.6610e-04,  3.0272e-03, -1.7086e-03,  ..., -4.6506e-03,\n",
      "                       -2.3541e-03, -5.7855e-03]], device='cuda:0')),\n",
      "             ('transformer.h.0.ln_1.weight',\n",
      "              tensor([0.2232, 0.1820, 0.1534, 0.1917, 0.2036, 0.1948, 0.1467, 0.1865, 0.2143,\n",
      "                      0.1956, 0.2118, 0.2153, 0.1882, 0.2074, 0.1871, 0.2040, 0.2044, 0.1900,\n",
      "                      0.1952, 0.0475, 0.1909, 0.2115, 0.1971, 0.2202, 0.1998, 0.2108, 0.2303,\n",
      "                      ...\n",
      "                      0.1662, 0.1982, 0.1582, 0.1935, 0.2182, 0.2067, 0.1855, 0.1778, 0.1900,\n",
      "                      0.2124, 0.1215, 0.2092, 0.1929, 0.2434, 0.1936, 0.1948, 0.0622, 0.1852,\n",
      "                      0.1868, 0.2035, 0.2310, 0.1794, 0.1655, 0.1756, 0.2074, 0.2194, 0.2152,\n",
      "                      0.0502, 0.2294, 0.1950, 0.2149, 0.2024, 0.1727, 0.0657, 0.1919, 0.1847,\n",
      "                      0.1900, 0.1825, 0.1898], device='cuda:0')), ...]I only call two functions save_pretrained then load_pretrained why are the weights different? I tried to change weights afterloading the model, it started to work fine again. Then, I tried to save that model, then the same problem, saved model is different than loaded model.Expected behaviorThe model weights are supposed to be the same.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_587.txt:\n",
      "Title: Call.destroy()onDeepSpeedEnginesomewhere post training\n",
      "URL: https://github.com/huggingface/transformers/issues/28178\n",
      "Body:\n",
      "System Infotransformers==4.36.2accelerate==0.25.0deepspeed==0.12.5Who can help?I was using deepspeed stage 2 with Trainer and accelerate and at the end of training when the Trainer has been garbage collected, I noticed my GPU VRAM was not clearing even after aggressively callinggc.collect()andtorch.cuda.empty_cache()I spent some time debugging and narrowed it down to deepspeed optimizer not removing hooks on pytorch tensors.I have submitted a PR on Deepspeed:microsoft/DeepSpeed#4858But to invoke this logicengine.destroy()must be called in some place post-trainingFor now, I am manually calling it outside the trainer post-training and can confirm it works, would be nice if Trainer can take care of it or there is some note in the docs.@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionTrain any model with Zero 2 + gradient accumulation, delete and let the trainer garbage collect, model parameters would still linger around in the GPU memoryExpected behaviorGPU memory should be reclaimable post training\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_236.txt:\n",
      "Title: Supportfrom_pretrainedofFlaxPretrainedModelfrom sharded.safetensorsweights\n",
      "URL: https://github.com/huggingface/transformers/issues/32200\n",
      "Body:\n",
      "Feature requestCurrentlyFlaxPretrainedModelonly supports loading pretrained models from sharded PyTorch weights or single-file.safetensors. It's worth adding support for loading sharded.safetensors.MotivationRecent open-source language models trained with PyTorch are likely only to release sharded.safetensorsweights. The lack of support for loading from the dominating paradigm makes it troublesome to use these models in Jax.Your contributionI'm relatively new to the implementation of the saving & loading mechanism oftransformers, but I'd love to try to work with this feature if the core team doesn't have enough bandwidth.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_550.txt:\n",
      "Title: Allow setting different decoder_start_token_ids for each item in a batch in the generate function.\n",
      "URL: https://github.com/huggingface/transformers/issues/28763\n",
      "Body:\n",
      "Feature request@ganteThegeneratefunction has adecoder_start_token_idargument that allows the specification of the decoder start token when generating from an encoder-decoder model (e.g. mT5). Currently,decoder_start_token_idmust be an integer, which means that the same start token is used for all elements in the batch. I request that you allow the specification of different start tokens for each element of the batch. For this purpose,decoder_start_token_idmust be a tensor with shape(batch_size,).MotivationSome multilingual encoder-decoder models use thedecoder_start_token_idto indicate the target language. Thus, this change would allow generation into multiple target languages in parallel, as illustrated in the code below.Your contributionimport re\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      "\n",
      "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
      "\n",
      "article_text = \"\"\"Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people's scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We're expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.\"\"\"\n",
      "\n",
      "model_name = \"csebuetnlp/mT5_m2m_crossSum_enhanced\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
      "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
      "\n",
      "get_lang_id = lambda lang: tokenizer._convert_token_to_id(\n",
      "    model.config.task_specific_params[\"langid_map\"][lang][1]\n",
      ")\n",
      "\n",
      "target_langs = [\"portuguese\", \"spanish\"]\n",
      "\n",
      "input_ids = tokenizer(\n",
      "    [WHITESPACE_HANDLER(article_text)],\n",
      "    return_tensors=\"pt\",\n",
      "    padding=\"max_length\",\n",
      "    truncation=True,\n",
      "    max_length=512\n",
      ")[\"input_ids\"]\n",
      "input_ids = input_ids.expand(len(target_langs), -1)   # shape (num_target_languages, num_input_tokens)\n",
      "\n",
      "decoder_start_token_id = torch.tensor(\n",
      "    [get_lang_id(t) for t in target_langs],\n",
      "    dtype=input_ids.dtype,\n",
      "    device=input_ids.device\n",
      ")  # shape (num_target_languages,)\n",
      "\n",
      "output_ids = model.generate(\n",
      "    input_ids=input_ids,\n",
      "    decoder_start_token_id=decoder_start_token_id,\n",
      "    max_length=84,\n",
      "    no_repeat_ngram_size=2,\n",
      "    num_beams=4,\n",
      ")\n",
      "\n",
      "summaries = tokenizer.batch_decode(\n",
      "    output_ids,\n",
      "    skip_special_tokens=True,\n",
      "    clean_up_tokenization_spaces=False\n",
      ")\n",
      "\n",
      "print(summaries)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_544.txt:\n",
      "Title: Detr models crashes when changing the num_queries parameter in the config\n",
      "URL: https://github.com/huggingface/transformers/issues/28865\n",
      "Body:\n",
      "System Infotransformersversion: 4.36.2Platform: Linux-5.15.133+-x86_64-with-glibc2.35Python version: 3.10.10Huggingface_hub version: 0.20.2Safetensors version: 0.4.1Accelerate version: 0.26.1Accelerate config:    not foundPyTorch version (GPU?): 2.1.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: Yes, Tesla T4Using distributed or parallel set-up in script?: NoWho can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionLoad the model with a customnum_querieshyperparameter.id2label = {0: 'Test'}\n",
      "label2id = {'Test': 0}\n",
      "model_name = \"facebook/detr-resnet-50\"\n",
      "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
      "detr = DetrForObjectDetection.from_pretrained(\n",
      "    model_name,\n",
      "    id2label=id2label,\n",
      "    label2id=label2id,\n",
      "    ignore_mismatched_sizes=True,\n",
      "    num_queries=5\n",
      ")Train (or just run the forward pass with an input containinglabels)I got the following error╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n",
      "│ in <module>:1                                                                                    │\n",
      "│                                                                                                  │\n",
      "│ ❱ 1 trainer.train()                                                                              │\n",
      "│   2                                                                                              │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/trainer.py:1537 in    │\n",
      "│ train                                                                                            │\n",
      "│                                                                                                  │\n",
      "│   1534 │   │   │   finally:                                                                      │\n",
      "│   1535 │   │   │   │   hf_hub_utils.enable_progress_bars()                                       │\n",
      "│   1536 │   │   else:                                                                             │\n",
      "│ ❱ 1537 │   │   │   return inner_training_loop(                                                   │\n",
      "│   1538 │   │   │   │   args=args,                                                                │\n",
      "│   1539 │   │   │   │   resume_from_checkpoint=resume_from_checkpoint,                            │\n",
      "│   1540 │   │   │   │   trial=trial,                                                              │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/trainer.py:1854 in    │\n",
      "│ _inner_training_loop                                                                             │\n",
      "│                                                                                                  │\n",
      "│   1851 │   │   │   │   │   self.control = self.callback_handler.on_step_begin(args, self.state,  │\n",
      "│   1852 │   │   │   │                                                                             │\n",
      "│   1853 │   │   │   │   with self.accelerator.accumulate(model):                                  │\n",
      "│ ❱ 1854 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)                      │\n",
      "│   1855 │   │   │   │                                                                             │\n",
      "│   1856 │   │   │   │   if (                                                                      │\n",
      "│   1857 │   │   │   │   │   args.logging_nan_inf_filter                                           │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/trainer.py:2735 in    │\n",
      "│ training_step                                                                                    │\n",
      "│                                                                                                  │\n",
      "│   2732 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device)                    │\n",
      "│   2733 │   │                                                                                     │\n",
      "│   2734 │   │   with self.compute_loss_context_manager():                                         │\n",
      "│ ❱ 2735 │   │   │   loss = self.compute_loss(model, inputs)                                       │\n",
      "│   2736 │   │                                                                                     │\n",
      "│   2737 │   │   if self.args.n_gpu > 1:                                                           │\n",
      "│   2738 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu parallel training        │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/trainer.py:2758 in    │\n",
      "│ compute_loss                                                                                     │\n",
      "│                                                                                                  │\n",
      "│   2755 │   │   │   labels = inputs.pop(\"labels\")                                                 │\n",
      "│   2756 │   │   else:                                                                             │\n",
      "│   2757 │   │   │   labels = None                                                                 │\n",
      "│ ❱ 2758 │   │   outputs = model(**inputs)                                                         │\n",
      "│   2759 │   │   # Save past state if it exists                                                    │\n",
      "│   2760 │   │   # TODO: this needs to be fixed and made cleaner later.                            │\n",
      "│   2761 │   │   if self.args.past_index >= 0:                                                     │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518    │\n",
      "│ in _wrapped_call_impl                                                                            │\n",
      "│                                                                                                  │\n",
      "│   1515 │   │   if self._compiled_call_impl is not None:                                          │\n",
      "│   1516 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]        │\n",
      "│   1517 │   │   else:                                                                             │\n",
      "│ ❱ 1518 │   │   │   return self._call_impl(*args, **kwargs)                                       │\n",
      "│   1519 │                                                                                         │\n",
      "│   1520 │   def _call_impl(self, *args, **kwargs):                                                │\n",
      "│   1521 │   │   forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.fo  │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527    │\n",
      "│ in _call_impl                                                                                    │\n",
      "│                                                                                                  │\n",
      "│   1524 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │\n",
      "│   1525 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │\n",
      "│   1526 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │\n",
      "│ ❱ 1527 │   │   │   return forward_call(*args, **kwargs)                                          │\n",
      "│   1528 │   │                                                                                     │\n",
      "│   1529 │   │   try:                                                                              │\n",
      "│   1530 │   │   │   result = None                                                                 │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/models/detr/modeling  │\n",
      "│ _detr.py:1603 in forward                                                                         │\n",
      "│                                                                                                  │\n",
      "│   1600 │   │   │   │   auxiliary_outputs = self._set_aux_loss(outputs_class, outputs_coord)      │\n",
      "│   1601 │   │   │   │   outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs                     │\n",
      "│   1602 │   │   │                                                                                 │\n",
      "│ ❱ 1603 │   │   │   loss_dict = criterion(outputs_loss, labels)                                   │\n",
      "│   1604 │   │   │   # Fourth: compute total loss, as a weighted sum of the various losses         │\n",
      "│   1605 │   │   │   weight_dict = {\"loss_ce\": 1, \"loss_bbox\": self.config.bbox_loss_coefficient}  │\n",
      "│   1606 │   │   │   weight_dict[\"loss_giou\"] = self.config.giou_loss_coefficient                  │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518    │\n",
      "│ in _wrapped_call_impl                                                                            │\n",
      "│                                                                                                  │\n",
      "│   1515 │   │   if self._compiled_call_impl is not None:                                          │\n",
      "│   1516 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]        │\n",
      "│   1517 │   │   else:                                                                             │\n",
      "│ ❱ 1518 │   │   │   return self._call_impl(*args, **kwargs)                                       │\n",
      "│   1519 │                                                                                         │\n",
      "│   1520 │   def _call_impl(self, *args, **kwargs):                                                │\n",
      "│   1521 │   │   forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.fo  │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527    │\n",
      "│ in _call_impl                                                                                    │\n",
      "│                                                                                                  │\n",
      "│   1524 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │\n",
      "│   1525 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │\n",
      "│   1526 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │\n",
      "│ ❱ 1527 │   │   │   return forward_call(*args, **kwargs)                                          │\n",
      "│   1528 │   │                                                                                     │\n",
      "│   1529 │   │   try:                                                                              │\n",
      "│   1530 │   │   │   result = None                                                                 │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/models/detr/modeling  │\n",
      "│ _detr.py:2202 in forward                                                                         │\n",
      "│                                                                                                  │\n",
      "│   2199 │   │   outputs_without_aux = {k: v for k, v in outputs.items() if k != \"auxiliary_outpu  │\n",
      "│   2200 │   │                                                                                     │\n",
      "│   2201 │   │   # Retrieve the matching between the outputs of the last layer and the targets     │\n",
      "│ ❱ 2202 │   │   indices = self.matcher(outputs_without_aux, targets)                              │\n",
      "│   2203 │   │                                                                                     │\n",
      "│   2204 │   │   # Compute the average number of target boxes across all nodes, for normalization  │\n",
      "│   2205 │   │   num_boxes = sum(len(t[\"class_labels\"]) for t in targets)                          │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518    │\n",
      "│ in _wrapped_call_impl                                                                            │\n",
      "│                                                                                                  │\n",
      "│   1515 │   │   if self._compiled_call_impl is not None:                                          │\n",
      "│   1516 │   │   │   return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]        │\n",
      "│   1517 │   │   else:                                                                             │\n",
      "│ ❱ 1518 │   │   │   return self._call_impl(*args, **kwargs)                                       │\n",
      "│   1519 │                                                                                         │\n",
      "│   1520 │   def _call_impl(self, *args, **kwargs):                                                │\n",
      "│   1521 │   │   forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.fo  │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527    │\n",
      "│ in _call_impl                                                                                    │\n",
      "│                                                                                                  │\n",
      "│   1524 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │\n",
      "│   1525 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │\n",
      "│   1526 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │\n",
      "│ ❱ 1527 │   │   │   return forward_call(*args, **kwargs)                                          │\n",
      "│   1528 │   │                                                                                     │\n",
      "│   1529 │   │   try:                                                                              │\n",
      "│   1530 │   │   │   result = None                                                                 │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115 in  │\n",
      "│ decorate_context                                                                                 │\n",
      "│                                                                                                  │\n",
      "│   112 │   @functools.wraps(func)                                                                 │\n",
      "│   113 │   def decorate_context(*args, **kwargs):                                                 │\n",
      "│   114 │   │   with ctx_factory():                                                                │\n",
      "│ ❱ 115 │   │   │   return func(*args, **kwargs)                                                   │\n",
      "│   116 │                                                                                          │\n",
      "│   117 │   return decorate_context                                                                │\n",
      "│   118                                                                                            │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/models/detr/modeling  │\n",
      "│ _detr.py:2323 in forward                                                                         │\n",
      "│                                                                                                  │\n",
      "│   2320 │   │   bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)                               │\n",
      "│   2321 │   │                                                                                     │\n",
      "│   2322 │   │   # Compute the giou cost between boxes                                             │\n",
      "│ ❱ 2323 │   │   giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_c  │\n",
      "│   2324 │   │                                                                                     │\n",
      "│   2325 │   │   # Final cost matrix                                                               │\n",
      "│   2326 │   │   cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.g  │\n",
      "│                                                                                                  │\n",
      "│ /home/jovyan/obj-detection/.venv/lib/python3.10/site-packages/transformers/models/detr/modeling  │\n",
      "│ _detr.py:2388 in generalized_box_iou                                                             │\n",
      "│                                                                                                  │\n",
      "│   2385 │   # degenerate boxes gives inf / nan results                                            │\n",
      "│   2386 │   # so do an early check                                                                │\n",
      "│   2387 │   if not (boxes1[:, 2:] >= boxes1[:, :2]).all():                                        │\n",
      "│ ❱ 2388 │   │   raise ValueError(f\"boxes1 must be in [x0, y0, x1, y1] (corner) format, but got {  │\n",
      "│   2389 │   if not (boxes2[:, 2:] >= boxes2[:, :2]).all():                                        │\n",
      "│   2390 │   │   raise ValueError(f\"boxes2 must be in [x0, y0, x1, y1] (corner) format, but got {  │\n",
      "│   2391 │   iou, union = box_iou(boxes1, boxes2)                                                  │\n",
      "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "ValueError: boxes1 must be in [x0, y0, x1, y1] (corner) format, but got tensor([[nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan]], device='cuda:0')The same code works fine without changing the defaultnum_queries.Expected behaviorI would expect the model to run as normal.I am fine tuning the model in a custom dataset which should not have more than a couple of objects per image, and expected the number of queries to have no impact other than limiting the maximum number of objects found.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_222.txt:\n",
      "Title: Export to ExecuTorch\n",
      "URL: https://github.com/huggingface/transformers/issues/32253\n",
      "Body:\n",
      "Feature requestUnlock a new workflow for on-device use-cases viatorch.exportandExecuTorch.So ideally I'd like to get the following workflow working:Load a model with StaticCache:model = AutoModelForCausalLM.from_pretrained(\n",
      "    hf_model_repo,\n",
      "    config=config,\n",
      "    attn_implementation=\"sdpa\",\n",
      "    cache_config={\n",
      "        \"use_cache\": True, \n",
      "        \"cache_implementation\": \"static\", \n",
      "        \"max_cache_length\": 128,\n",
      "    },  # Mandatory field to set ONLY for \"Export to ExecuTorch\" workflow, optional in other use-cases\n",
      ")Then we canexport the model with StaticCache w/o passing the cache-related args to the forward().# This is the `forward()` signature for PreTrainedModel\n",
      "# \n",
      "# def forward(\n",
      "#     input_ids: torch.LongTensor = None,\n",
      "#     attention_mask: Optional[torch.Tensor] = None,\n",
      "#     position_ids: Optional[torch.LongTensor] = None,\n",
      "#     past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n",
      "#     inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "#     labels: Optional[torch.LongTensor] = None,\n",
      "#     use_cache: Optional[bool] = None,\n",
      "#     output_attentions: Optional[bool] = None,\n",
      "#     output_hidden_states: Optional[bool] = None,\n",
      "#     return_dict: Optional[bool] = None,\n",
      "#     cache_position: Optional[torch.LongTensor] = None,\n",
      "# ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
      "\n",
      "# Will NOT require passing `attention_mask`, `past_key_values`, `use_cache`, etc. optional cache config \n",
      "# fields to `forward()` to get can exported model with `StaticCache` enabled.\n",
      "\n",
      "exported = torch.export(\n",
      "    model, \n",
      "    args=(model_inputs,), \n",
      "    kwargs={\"position_ids\": <val>, \"inputs_embeds\": <val>, \"cache_position\": <val>}or further lower to ExecuTorch with predefined recipes like:executorch_m = export_to_executorch(\n",
      "    model, \n",
      "    export_args={\"input_ids\": <val>, \"position_ids\": <val>, \"inputs_embeds\": <val>, \"cache_position\": <val>},\n",
      "    recipes=\"xnnpack_q8\",  # 8bit quantized and delegate to XNNPACK\n",
      ")\n",
      "\n",
      "# The exported model is statically self-contained, i.e. whether it's using cache, type of cache, max length of \n",
      "# the cache and sequence, etc. There is no need to specify those configs at generation time. They will be \n",
      "# ignored and warned if specified.ExecuTorch supported delegates toXNNPACK backend,Apple Core MLandMPS,Qualcomm QNN,ARM Ethos-U,Vulkan GPUand more. We can create recipes for supported backends, so that users can use it directly for their targeted use-cases.Use the exported/lowered artifact for inference:# The exported artifact only contains the `Transformer` that predict a single token, the autoregressive logics \n",
      "# will be in the `generate`:\n",
      "\n",
      "generate(model=executorch_m, prompt=\"Hello world\")  # Will generate up to the maximal sequence length/cache lengthThe example workflow above shows direct integration betweentorch.export+ExecuTorchand HFtransformersmodels. Eventually this workflow could be accessible viaoptimum exporters-et.Issues TrackerStaticCacheMakeStaticCachecompatible withtorch.export: PRMake static cache compatible with torch.export#32168Make Cache statically configurable at model construction time#32500Fix get_usable_length for StaticCache#32503Support dynamic length slicing inStaticCache: PR[WIP] Dynamic length in static cache#30862Implementgenerate(inference) for torch exported text-generation models#32504ModelsLlama is ExecuTorch compatible#32505CLIP is ExecuTorch compatible#32506Bert is ExecuTorch compatible#32507Bart/Wav2Vec2 is ExecuTorch compatible#32508TrOCR is ExecuTorch compatible#32509E2E workflow viaoptimumExport-to-ExecuTorch viaoptimumintegration#32511MotivationLet me explain the motivation in a bigger picture.The Ultimate GoalThe goal is to enable a new workflow \"Export toExecuTorch\" from edge use-cases, just like onnx, tflite, torchscript, etc. viaOptimum.Why is the option to statically configure cache needed?torch.export()doesn't support passing the StaticCache instance as a param toforward().The dynamo tracing will fail with some error like this:E    torch._dynamo.exc.UserError: It looks like one of the inputs with type `<class 'transformers.cache_utils.StaticCache'>` is not supported or pytree-flattenable.\n",
      "E    Exported graphs inputs can only contain the following supported types: [<class 'torch.Tensor'>, <class 'torch.SymInt'>, <class 'torch.SymFloat'>, <class 'torch.SymBool'>, <class 'torch.ScriptObject'>, <class 'NoneType'>, <class 'complex'>, <class 'torch.dtype'>, <class 'str'>, <class 'bool'>, <class 'ellipsis'>, <class 'int'>, <class 'torch.layout'>, <class 'code'>, <class 'torch.memory_format'>, <class 'bytes'>, <class 'float'>, <class 'torch.device'>].Not onlytorch.export(), ExecuTorch also requires the model to be self-contained statically so that the Runtime can just load the serialized binary (.pte) and run as-is, which means, the spec of the StaticCache must be part of the serialized binary.For example, in ExecuTorch we have a c++ runtime for LLMs that could load the exported transform model for inference. To utilize that runtime, theforward()must comply with the same signature, which looks like:def forward(\n",
      "    token: torch.Tensor,\n",
      "    input_pos: Optional[torch.Tensor],\n",
      ") -> torch.Tensor:As shown in the prototype PR#31706and in PR#32168, we can make it compatible with theforward()of Hugging Face transformersPreTrainedModelby just statically instantiatingStaticCachein the adapterforward()method. However, it's not scalable to add such an adapterforward()for all models that want to participate to \"Export to ExecuTorch\" workflow.How to generate in a more scalable way?If we could make the Cache statically configurable at model construction time, e.g. viaAutoConfig, there is no need to pass some optional args to theforward()of Hugging Face transformersPreTrainedModel, e.g.attention_mask,past_key_values,use_cache, neither for export nor forgenerate/inference using the exported artifact.Is it compatible with existing config, e.g. generation config?Yes, it's unlocking a new option mainly for export use-case and shouldn't have conflict with non-export use-cases where the cache can still be passed through thegeneration_config.Your contributionCo-design the \"Export to ExecuTorch\" workflow.Co-design thegeneratefor exported model and the integration inOptimumHere is how ExecuTorch implements thegenerate()for llama2/3 ineager pythonandc++.cc:@amyeroberts@gante@ArthurZucker@michaelbenayoun\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_949.txt:\n",
      "Title: Request: New LM Adapted checkpoints for T5\n",
      "URL: https://github.com/huggingface/transformers/issues/12384\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_746.txt:\n",
      "Title: Add keypoint-detection task\n",
      "URL: https://github.com/huggingface/transformers/issues/24044\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_753.txt:\n",
      "Title: Adding GPTNeoX (Tensorflow version)\n",
      "URL: https://github.com/huggingface/transformers/issues/23814\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_960.txt:\n",
      "Title: How to train the new wav2vec unsupervised model using hugging face ?\n",
      "URL: https://github.com/huggingface/transformers/issues/12144\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_974.txt:\n",
      "Title: Megatron fused CUDA kernels to improve Hugging Face model classes' scalability\n",
      "URL: https://github.com/huggingface/transformers/issues/11368\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_790.txt:\n",
      "Title: Native support of ChatGLM-6b\n",
      "URL: https://github.com/huggingface/transformers/issues/22290\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_579.txt:\n",
      "Title: DPT normalization causes contouring when there are significant disparities in depth values between adjacent areas\n",
      "URL: https://github.com/huggingface/transformers/issues/28292\n",
      "Body:\n",
      "System InfoPython 3.10.12transformers-4.36.2Who can help?@stevhliu@NielsRoggeInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import DPTImageProcessor, DPTForDepthEstimation\n",
      "import torch\n",
      "import numpy as np\n",
      "from PIL import Image\n",
      "import requests\n",
      "\n",
      "url = \"https://images.unsplash.com/photo-1605146768851-eda79da39897?q=80&w=2970&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
      "image = Image.open(requests.get(url, stream=True).raw)\n",
      "\n",
      "processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
      "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
      "\n",
      "# prepare image for the model\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    predicted_depth = outputs.predicted_depth\n",
      "\n",
      "# interpolate to original size\n",
      "prediction = torch.nn.functional.interpolate(\n",
      "    predicted_depth.unsqueeze(1),\n",
      "    size=image.size[::-1],\n",
      "    mode=\"bicubic\",\n",
      "    align_corners=False,\n",
      ")\n",
      "\n",
      "# visualize the prediction\n",
      "output = prediction.squeeze().cpu().numpy()\n",
      "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
      "depth = Image.fromarray(formatted)\n",
      "display(depth)Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.bias']You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.Expected behaviorAnecdotally, the local scaling methodology used by get_depth_map athttps://huggingface.co/diffusers/controlnet-depth-sdxl-1.0seems to work better for models that perform better at identifying close-range depth. The global scaling methodology seems to work better for models that perform better at identifying far-range depth. I combined them below:def get_depth_map(image, feature_extractor, depth_estimator, scale_local):\n",
      "    inputs = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n",
      "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
      "        depth_map = depth_estimator(inputs).predicted_depth\n",
      "\n",
      "    depth_map = torch.nn.functional.interpolate(\n",
      "        depth_map.unsqueeze(1),\n",
      "        size=image.size[::-1],\n",
      "        mode=\"bicubic\",\n",
      "        align_corners=False,\n",
      "    )\n",
      "\n",
      "    if scale_local:\n",
      "        depth_min = torch.amin(depth_map, dim=[1, 2, 3], keepdim=True)\n",
      "        depth_max = torch.amax(depth_map, dim=[1, 2, 3], keepdim=True)\n",
      "        depth_map = (depth_map - depth_min) / (depth_max - depth_min)\n",
      "        image = torch.cat([depth_map] * 3, dim=1)\n",
      "    \n",
      "        image = image.permute(0, 2, 3, 1).cpu().numpy()[0]\n",
      "        image = Image.fromarray((image * 255.0).clip(0, 255).astype(np.uint8))\n",
      "        return image\n",
      "    \n",
      "    output = depth_map.squeeze().cpu().numpy()\n",
      "    formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
      "    return Image.fromarray(formatted)\n",
      "\n",
      "\n",
      "depth_estimator_hybrid = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\").to(\"cuda\")\n",
      "depth_estimator_dinov2_nyu = DPTForDepthEstimation.from_pretrained(\"facebook/dpt-dinov2-giant-nyu\").to(\"cuda\")\n",
      "    \n",
      "image_processor_hybrid = AutoImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
      "image_processor_dinov2_nyu = AutoImageProcessor.from_pretrained(\"facebook/dpt-dinov2-giant-nyu\")\n",
      "\n",
      "# Close range depth\n",
      "bad_close_result = get_depth_map(image, image_processor_hybrid, depth_estimator_hybrid, False)\n",
      "good_close_result = get_depth_map(image, image_processor_hybrid, depth_estimator_hybrid, True)\n",
      "\n",
      "# Far range depth\n",
      "downscaled_image = image.resize((1024, 1024))  # This image is too big for my GPU to processdpt-dinov2-giant-nyu so I downscaled it\n",
      "good_far_result = get_depth_map(downscaled_image, image_processor_dinov2_nyu, depth_estimator_dinov2_nyu, False)\n",
      "bad_far_result = get_depth_map(downscaled_image, image_processor_dinov2_nyu, depth_estimator_dinov2_nyu, True)display(bad_close_result)display(good_close_result)display(good_far_result)display(bad_far_result)Sufficiently blurring the image prior to detecting depth also gets rid of this, ie:blurred_image = image.filter(ImageFilter.GaussianBlur(radius=5))\n",
      "display(get_depth_map(blurred_image, image_processor_hybrid, depth_estimator_hybrid, False))\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_237.txt:\n",
      "Title: Model loading is uneven on GPUs with AutomodelforCasualLM\n",
      "URL: https://github.com/huggingface/transformers/issues/32199\n",
      "Body:\n",
      "System Infopython 3.10.10torch 2.3.1transformers             4.43.2optimum                  1.17.1auto_gptq                0.7.1bitsandbytes             0.43.2accelerate               0.33.0Llama3.1 8B Instruct gets loaded like this. So I cant even go more than 1 batch size while finetuningWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionimportsys,gc,torch,random,osimportnumpyasnpimportpandasaspdimporttimefromdatasetsimportload_dataset,Dataset,DatasetDictfromtransformersimportAutoTokenizer,AutoModelForCausalLM,GPTQConfigfrompeftimportprepare_model_for_kbit_training,LoraConfig,get_peft_modelfromtransformersimportTrainer,TrainingArguments,DataCollatorForLanguageModeling,BitsAndBytesConfigfromtrlimportSFTTrainerimportwandbfromtransformersimportAutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfigwandb.init(mode='disabled')CONTEXT_LENGTH=8192output_dir=\"outputs_mi\"model_id=\"./llama_models/Meta-Llama-3.1-8B-Instruct-gptq-4bit/\"tokenizer=AutoTokenizer.from_pretrained(model_id,max_seq_length=CONTEXT_LENGTH)tokenizer.add_eos_token=Truetokenizer.padding_side='right'tokenizer.pad_token=tokenizer.eos_tokenmodel=AutoModelForCausalLM.from_pretrained(model_id,device_map='auto')model.config.use_cache=False# silence the warnings. Please re-enable for inference!model.gradient_checkpointing_enable()model=prepare_model_for_kbit_training(model)config=LoraConfig(r=64,lora_alpha=64,target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],lora_dropout=0,bias=\"none\",task_type=\"CAUSAL_LM\",\n",
      ")model=get_peft_model(model,config)print(model.print_trainable_parameters())data_files={\"train\":\"full_label_train_data.csv\",\"test\":\"full_label_test_data.csv\"}dataset=load_dataset(\"csv\",data_files=data_files)print(dataset)training_arguments=TrainingArguments(output_dir=output_dir,num_train_epochs=100,overwrite_output_dir=True,per_device_train_batch_size=1,per_device_eval_batch_size=1,gradient_accumulation_steps=4,optim=\"paged_adamw_8bit\",save_strategy='epoch',# save_steps = 500,warmup_ratio=0.2,logging_steps=2,learning_rate=4e-4,# gradient_checkpointing=True,# gradient_checkpointing_kwargs={\"use_reentrant\": True},weight_decay=0.001,fp16=False,bf16=True,max_steps=-1,max_grad_norm=0.3,group_by_length=True,lr_scheduler_type=\"linear\",use_cpu=False,report_to=\"tensorboard\",eval_strategy=\"epoch\")Expected behaviorI would like the model to be loaded evenly so that I can finetune with a larger batch size\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_551.txt:\n",
      "Title: Adding CrossMAE\n",
      "URL: https://github.com/huggingface/transformers/issues/28753\n",
      "Body:\n",
      "Model descriptionHey,the recently releasedCrossMAEseems like it would be a nice addition to transformers.Basically the model improves MAE by using Cross-Attention instead of Self-Attention on the tokens and thereby decreasing the needed FLOPS quite significantly. At the same time it seems to be able to keep the performance of MAE or even improve it a bit.Maybe there are already plans of integrating it@NielsRogge?Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationProject Page:https://crossmae.github.io/GitHub Repo:https://github.com/TonyLianLong/CrossMAEPaper:https://arxiv.org/pdf/2401.14391.pdf\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_586.txt:\n",
      "Title: Verify interpolation of image processors\n",
      "URL: https://github.com/huggingface/transformers/issues/28180\n",
      "Body:\n",
      "Feature requestAs pointed out in#27742, some image processors might need a correction on the default interpolation method being used (resampling in Pillow). We could check this on a per-model basis.MotivationInterpolation methods have a slight (often minimal) impact on performance. However it would be great to verify this on a per-model basis.e.g.ViT's image processor defaults to BILINEAR but should use BICUBIC as seenhere. We can update the default values of the image processors, but can't update the configs on the hub as this would break people's fine-tuned models.Your contributionI could work on this, but this seems like a good first issue for first contributors.To be checked (by comparing against original implementation):beitbitclipconvnextconvnextv2cvtdata2vec-visiondeitdinatdinov2efficientformerefficientnetfocalnetimagegptlevitmobilenet_v1mobilenet_v2mobilevitmobilevitv2natperceiverpoolformerpvtregnetresnetsegformersiglipswiftformerswinswinv2vanvitvit_hybridvit_msn\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_592.txt:\n",
      "Title: [Trainer.train] learning rate logging inconsistency: learning rate for the future step is logged\n",
      "URL: https://github.com/huggingface/transformers/issues/28124\n",
      "Body:\n",
      "System InfoNAWho can help?@muellerzrand@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThisline of code steps forward the LR scheduler, before_maybe_log_save_evaluateis called. This means the learning rate logged represents the learning in the upcoming iteration.For most of the use cases, the differences between them is small. However, in certain cases, this caused confusion.Expected behaviorThe learning rate for the current iteration is logged.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_431.txt:\n",
      "Title: [Finetuning OneFormer] Seems not to use multiple GPUs, with both DataParallel and Accelerate\n",
      "URL: https://github.com/huggingface/transformers/issues/30340\n",
      "Body:\n",
      "System Infoaccelerate                0.29.3\n",
      "torch                     1.11.0+cu113\n",
      "transformers              4.39.3Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI did followthis tutorialand was able to finetune OneFormer. However, when I try to finetune the model on multi GPUs, it did not work.I did two approaches:1. Using DataParallelimport torch.nn as nn\n",
      "# some code the same as your tutorial\n",
      "processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx\n",
      "\n",
      "train_dataset = CustomDataset(processor)\n",
      "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=16)\n",
      "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
      "\n",
      "model = nn.DataParallel(model)\n",
      "device = 'cuda'\n",
      "model.to(device)\n",
      "model.train()\n",
      "\n",
      "for epoch in range(20):  # loop over the dataset multiple times\n",
      "    for batch in train_dataloader:\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "        batch = {k:v.to(device) for k,v in batch.items()}\n",
      "\n",
      "        # forward pass\n",
      "        outputs = model(**batch)\n",
      "\n",
      "        # backward pass + optimize\n",
      "        loss = outputs.loss\n",
      "        print(\"Loss:\", loss.item())\n",
      "        loss.backward()\n",
      "        optimizer.step()This code running normally but just only GPU:0 was utilized, the other GPUs do not seems to work.Here is the result from nvidia-smi while it's running:+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.239.06   Driver Version: 470.239.06   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 55%   58C    P2   196W / 356W |  20651MiB / 24268MiB |     71%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:3C:00.0 Off |                  N/A |\n",
      "| 59%   57C    P2   121W / 356W |      8MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 53%   54C    P2   120W / 356W |      8MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "| 53%   47C    P2   118W / 356W |      8MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 60%   58C    P2   137W / 356W |      8MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:D9:00.0 Off |                  N/A |\n",
      "| 60%   58C    P2   111W / 356W |      8MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2170      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A   2809467      C   python                          20643MiB |\n",
      "|    1   N/A  N/A      2170      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A      2170      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    3   N/A  N/A      2170      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    4   N/A  N/A      2170      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    5   N/A  N/A      2170      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+2. Using AccelerateFollowingthis tutorial, I modified the code as following:processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx\n",
      "\n",
      "train_dataset = CustomDataset(processor)\n",
      "# val_dataset = CustomDataset(processor)\n",
      "\n",
      "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=16)\n",
      "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
      "\n",
      "\n",
      "accelerator = Accelerator()\n",
      "model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n",
      "\n",
      "\n",
      "model.train()\n",
      "\n",
      "for epoch in range(20):  # loop over the dataset multiple times\n",
      "    for batch in train_dataloader:\n",
      "\n",
      "        # zero the parameter gradients\n",
      "        optimizer.zero_grad()\n",
      "        # batch = {k:v.to(device) for k,v in batch.items()}\n",
      "\n",
      "        # forward pass\n",
      "        outputs = model(**batch)\n",
      "\n",
      "        # backward pass + optimize\n",
      "        loss = outputs.loss\n",
      "        print(\"Loss:\", loss.item())\n",
      "        accelerator.backward(loss)\n",
      "        optimizer.step()This code was running normally, except only GPU:0 works.I'm quite sure that I'm missing something here. Can you please point me to the right direction? Thank you so much!Expected behaviorUsing nvidia-smi, I should see multiple GPUs being utilized.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_343.txt:\n",
      "Title: Dropout sync across GPUs causes major performance drops\n",
      "URL: https://github.com/huggingface/transformers/issues/31412\n",
      "Body:\n",
      "System InfoGPT2torch==2.3.1DDPusing transformers Trainer 4.41.2Who can help?@muellerzr@SunMarc(Trainer code)@ArthurZuckerand@younesbelkada(text models)InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWhen you set the random seed to a constant X the dropout masks will be identical across all devices being trained on with DDPset_seed(40)data=torch.ones_like(torch.randn(1,10))dropout=torch.nn.Dropout(p=0.5)for_inrange(2):print({'device':os.environ.get('RANK'),'dropout mask':dropout(data)})On 1 GPU I get{'device':'0','dropout mask':tensor([[0.,0.,2.,2.,2.,0.,2.,0.,2.,0.]])}\n",
      "{'device':'0','dropout mask':tensor([[2.,0.,2.,2.,0.,2.,0.,2.,0.,0.]])}On 4 GPU I get the same dropout masked being applied{'device':'2','dropout mask':tensor([[0.,0.,2.,2.,2.,0.,2.,0.,2.,0.]])}\n",
      "{'device':'2','dropout mask':tensor([[2.,0.,2.,2.,0.,2.,0.,2.,0.,0.]])}\n",
      "\n",
      "{'device':'1','dropout mask':tensor([[0.,0.,2.,2.,2.,0.,2.,0.,2.,0.]])}\n",
      "{'device':'1','dropout mask':tensor([[2.,0.,2.,2.,0.,2.,0.,2.,0.,0.]])}\n",
      "\n",
      "{'device':'3','dropout mask':tensor([[0.,0.,2.,2.,2.,0.,2.,0.,2.,0.]])}\n",
      "{'device':'3','dropout mask':tensor([[2.,0.,2.,2.,0.,2.,0.,2.,0.,0.]])}\n",
      "\n",
      "{'device':'0','dropout mask':tensor([[0.,0.,2.,2.,2.,0.,2.,0.,2.,0.]])}\n",
      "{'device':'0','dropout mask':tensor([[2.,0.,2.,2.,0.,2.,0.,2.,0.,0.]])}When training a model this leads to poor performance. See this gap which scales with the number of GPUs. If I turn dropout off then I see no gap whatsoever.  This is training on 1m samples and can be observed with any seed. You see exploding gradient norms when training too.I tried to set torch.manual_seed(seed + rank) in order to vary the performance. For smaller models this fixes the issue.  But it has a critical flaw which is that it breaks the data ordering and you get duplicate data on each device. Here the data row is a hash of the data being fed inwith 1 GPU the data is unique{'data': [1416901,1311812,1333073,1410935],'losses':tensor([[0.4966],\n",
      "        [0.2563],\n",
      "        [0.2142],\n",
      "        [0.6520]],device='cuda:0',grad_fn=<NegBackward0>),'rank':'0','local_rank':'0'}\n",
      "\n",
      "{'data': [1109164,1386701,1297717,989532],'losses':tensor([[0.0630],\n",
      "        [0.0800],\n",
      "        [0.0653],\n",
      "        [0.7069]],device='cuda:0',grad_fn=<NegBackward0>),'rank':'0','local_rank':'0'}with 4 GPUs you get duplicate data across devices{'data': [1416901],'losses':tensor([[0.8843]],device='cuda:1',grad_fn=<NegBackward0>),'rank':'1','local_rank':'1'}\n",
      "{'data': [1416901],'losses':tensor([[0.5725]],device='cuda:0',grad_fn=<NegBackward0>),'rank':'0','local_rank':'0'}\n",
      "{'data': [1109164],'losses':tensor([[0.0828]],device='cuda:3',grad_fn=<NegBackward0>),'rank':'3','local_rank':'3'}\n",
      "{'data': [1010993],'losses':tensor([[0.2223]],device='cuda:2',grad_fn=<NegBackward0>),'rank':'2','local_rank':'2'}\n",
      "\n",
      "{'data': [1325519],'losses':tensor([[0.7159]],device='cuda:1',grad_fn=<NegBackward0>),'rank':'1','local_rank':'1'}\n",
      "{'data': [1360317],'losses':tensor([[0.4719]],device='cuda:2',grad_fn=<NegBackward0>),'rank':'2','local_rank':'2'}\n",
      "{'data': [1284812],'losses':tensor([[2.2623]],device='cuda:3',grad_fn=<NegBackward0>),'rank':'3','local_rank':'3'}\n",
      "{'data': [1109164],'losses':tensor([[0.0876]],device='cuda:0',grad_fn=<NegBackward0>),'rank':'0','local_rank':'0'}Expected behaviorShould be a way to set the random seed to control dropout without destroying the data ordering when doing DDP.I am happy to submit a MR to fix this issue if given some pointers about how to implement it.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_425.txt:\n",
      "Title: Add HelpingAI-3B-v2.2: Emotionally Intelligent Conversational AI\n",
      "URL: https://github.com/huggingface/transformers/issues/30396\n",
      "Body:\n",
      "Model descriptionIntroductionHelpingAI-3B-v2.2 is a state-of-the-art large language model specializing in emotionally intelligent conversation. With advanced emotional understanding capabilities, it can engage in empathetic dialogue tailored to the user's emotional state and context.Emotional Intelligence CapabilitiesHelpingAI-3B-v2.2 exhibits several key traits that enable emotionally resonant responses:Emotion recognition and validationEmpathetic perspective-takingGenerating emotionally supportive languageContextual emotional attunementUsing appropriate tone, word choice and emotional expressionWhether comforting someone grieving, celebrating positive news, or addressing complex feelings, HelpingAI-3B-v2.2 can adapt its communication style with emotional nuance.Examples of Emotionally Intelligent Responses\"I'm really sorry to hear about your friend's loss. 😔 Losing a parent can be incredibly difficult and traumatic, and it's completely understandable that they are struggling with grief. It's important to show them empathy and support during this tough time. You can start by simply being there for them, listening to them, and offering a shoulder to lean on. 💕 It's also important to acknowledge their feelings and let them know that it's okay to grieve and take as much time as they need to process their emotions. Offering practical help, such as running errands or cooking for them, can also make a big difference. Remember, small gestures of kindness and compassion can go a long way in comforting someone during difficult times.\"\"Hey there! 🌟 It's a beautiful day today, so you should cook something delicious and enjoy the sunny weather! How about trying out a new recipe and let your creativity flow? Whether it's a classic dish or something experimental, the kitchen is the perfect place to let your imagination run wild. Write me a list of ingredients you have on hand, and I'll help you come up with a wonderful culinary creation! 🍲🌞\"The model tailors its language, tone and emotional content to be contextually appropriate, combining emotional intelligence with factual knowledge and practical suggestions.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationhttps://huggingface.co/collections/OEvortex/emotional-intelligence-6625cc3247851d717e9fa382\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_31.txt:\n",
      "Title: transformers 4.44.2 doesn't work with torch.compile and torch.export on T5 generate()\n",
      "URL: https://github.com/huggingface/transformers/issues/33283\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Linux-5.19.0-0_fbk12_hardened_11583_g0bef9520ca2b-x86_64-with-glibc2.34Python version: 3.10.14Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.5.0a0+git33ba952 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA PG509-210Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe following code breaks:importtorchimporttransformersfromtransformersimportGenerationConfigfromtransformersimportAutoConfigdefgenerate_inputs_for_model(model_cls,model):eval_context=torch.randint(0,model.config.vocab_size, (4,2048)).to(\"cuda\")return{\"input_ids\":eval_context}config=AutoConfig.from_pretrained(\"t5-small\")model_cls=getattr(transformers,\"AutoModelForSeq2SeqLM\")model=model_cls.from_config(config).to(\"cuda\")example_inputs=generate_inputs_for_model(model_cls,model)example_inputs=(example_inputs[\"input_ids\"],)generation_config=GenerationConfig(max_new_tokens=256,pad_token_id=0,eos_token_id=None,do_sample=False,num_beams=1,use_cache=True,\n",
      ")classGenerationWrapper(torch.nn.Module):def__init__(self,model,generation_config):super().__init__()self.model=modelself.generation_config=generation_configdefforward(self,inputs):returnself.model.generate(inputs,self.generation_config)model=GenerationWrapper(model,generation_config)# torch.compile repromodel_opt=torch.compile(model)output=model_opt(*example_inputs)# torch.export reprotorch.export.export(model,args=example_inputs,strict=False)With the following error:ValueError: `decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.If I manually adddecoder_start_token_id=0to the GenerationConfig. Then both compile and export work, although very slow.Expected behaviorExpected generate to work like before without manually specifyingdecoder_start_token_idorbos_token_idin theGenerationConfig.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_25.txt:\n",
      "Title: tokenizersave_pretrainedcan not handle non-string value in dtype\n",
      "URL: https://github.com/huggingface/transformers/issues/33304\n",
      "Body:\n",
      "System Infopython3.10transformers             4.36.2torch                    2.1.2torchaudio               2.1.2torchvision              0.16.2Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import T5Tokenizer\n",
      "import torch\n",
      "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-base\", torch_dtype=torch.bfloat16)\n",
      "tokenizer.save_pretrained('./')Expected behaviorTraceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/home/jiawhuang/miniconda3/envs/rlhflow/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2430, in save_pretrained\n",
      "    out_str = json.dumps(tokenizer_config, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\"\n",
      "  File \"/home/jiawhuang/miniconda3/envs/rlhflow/lib/python3.10/json/__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "  File \"/home/jiawhuang/miniconda3/envs/rlhflow/lib/python3.10/json/encoder.py\", line 201, in encode\n",
      "    chunks = list(chunks)\n",
      "  File \"/home/jiawhuang/miniconda3/envs/rlhflow/lib/python3.10/json/encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/home/jiawhuang/miniconda3/envs/rlhflow/lib/python3.10/json/encoder.py\", line 405, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"/home/jiawhuang/miniconda3/envs/rlhflow/lib/python3.10/json/encoder.py\", line 438, in _iterencode\n",
      "    o = _default(o)\n",
      "  File \"/home/jiawhuang/miniconda3/envs/rlhflow/lib/python3.10/json/encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type dtype is not JSON serializableExplanationMy conjecture is that, when I load tokenizer with bfloat16,tokenizer.dtypeis assigned bytorch.bfloat16. When saving the tokenizer, the dtype was not handled.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_394.txt:\n",
      "Title: Resuming from checkpoint runs into OOM\n",
      "URL: https://github.com/huggingface/transformers/issues/30822\n",
      "Body:\n",
      "System InfoUsing GPU in script: A100 80 GB; Driver Version: 550.54.15; CUDA-Version: 12.4Using distributed or parallel setup: NoWho can help?@ArthurZucker@muellerz@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThis is the definition of my custom model:class CustomLongformer(LongformerModel):\n",
      "\n",
      "    def __init__(self, config):\n",
      "        super().__init__(config, add_pooling_layer=False)\n",
      "        self.linear = nn.Linear(\n",
      "            in_features=1024,\n",
      "            out_features=47,\n",
      "        )\n",
      "        self.custom_embeddding = nn.Embedding(\n",
      "            num_embeddings=14,\n",
      "            embedding_dim=1024,\n",
      "        )\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional = None,\n",
      "        attention_mask: Optional = None,\n",
      "        global_attention_mask: Optional = None,  # This attention mask is 1 at time-index 0, and 0 elsewhere (CLS-only global attention)\n",
      "        inputToEmbedding: Optional = None,\n",
      "        labels: Optional = None,\n",
      "    ) -> LongformerSequenceClassifierOutput:\n",
      "        hidden = super().forward(\n",
      "            input_ids=input_ids,\n",
      "            attention_mask=attention_mask,\n",
      "            global_attention_mask=global_attention_mask,\n",
      "        ).last_hidden_state\n",
      "        embeddings = self.custom_embeddding(inputToEmbedding)\n",
      "        hidden = hidden[:, 0, :]  # Select first token of each sequence (CLS Token)\n",
      "        hidden = hidden + embeddings  # LM output + folder embedding\n",
      "        logits = self.linear(hidden)\n",
      "        if labels is not None:\n",
      "            loss = F.cross_entropy(\n",
      "                input=logits,\n",
      "                target=labels,\n",
      "            )\n",
      "            return {\n",
      "                \"loss\": loss,\n",
      "                \"logits\": logits,\n",
      "            }\n",
      "        return {\n",
      "            \"logits\": logits,\n",
      "        }\n",
      "\n",
      "config = LongformerConfig(\n",
      "    vocab_size=tokenizer.vocab_size,\n",
      "    max_position_embeddings=1024+ 1,\n",
      "    num_hidden_layers=24,\n",
      "    num_attention_heads=16,\n",
      "    intermediate_size=4096,\n",
      "    hidden_size=1024,\n",
      "    attention_window=256,\n",
      "    bos_token_id=tokenizer.cls_token_id,\n",
      "    eos_token_id=tokenizer.sep_token_id,\n",
      "    pad_token_id=tokenizer.pad_token_id,\n",
      ")\n",
      "\n",
      "model = CustomLongformer(config)I have a dataset with long texts which are chunked to samples of 1024 tokens (padded to said length if required).These are my training arguments:training_args = TrainingArguments(\n",
      "    output_dir=\"some/path\",\n",
      "    do_train=True,\n",
      "    evaluation_strategy=\"steps\",\n",
      "    per_device_train_batch_size=30,\n",
      "    per_device_eval_batch_size=30,\n",
      "    gradient_accumulation_steps=2,\n",
      "    eval_steps=4_000,\n",
      "    learning_rate=1e-5,\n",
      "    logging_steps=20,\n",
      "    num_train_epochs=3,\n",
      "    lr_scheduler_type=\"cosine\",\n",
      "    load_best_model_at_end=True,\n",
      "    warmup_steps=100,\n",
      "    save_strategy=\"steps\",\n",
      "    save_steps=4_000,\n",
      "    save_total_limit=5,\n",
      "    fp16=True,\n",
      "    dataloader_num_workers=6,\n",
      "    optim=\"adamw_torch\",\n",
      "    report_to=\"tensorboard\",\n",
      " )Now all that's left is a trainer to start/continue training:trainer = Trainer(\n",
      "    model=model,\n",
      "    data_collator=CustomDataCollator(),  # This only creates training batches (tried with some default as well)\n",
      "    args=training_args,\n",
      "    train_dataset=ds[\"training\"],\n",
      "    eval_dataset=ds[\"validation\"],\n",
      ")\n",
      "trainer.train(resume_from_checkpoint=True)Whenever I start training from scratch (not resuming from checkpoint), everything works fine, and I can train for days. But as soon as I want to start from a checkpoint saved during training, I get an OutOfMemory error. The GPU is not occupied by any other task, and I know for sure that there are no leaks from other processes happening. At the same time, the OOM says that it failed allocating 120 MiB GPU Memory, but in fact, more than 7 GiB are still free according to nvidia-smi.Expected behaviorReturning from checkpoint should not run into any OOM problems if the model trained successfully before. The expected behavior can be achieved by settingos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\", but this is a) only a hacky solution and b) results in much longer training times.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_19.txt:\n",
      "Title: DINOV2 generate different outputs for the same inputs\n",
      "URL: https://github.com/huggingface/transformers/issues/33334\n",
      "Body:\n",
      "System Infotransformer: 4.44.2python: 3.11platform: ubuntu 20.04torch: 2.2.2cuda: 12.5Who can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionHere's a minimal example for me to reproduce this along with the image I used.import torch\n",
      "from PIL import Image\n",
      "from transformers import AutoImageProcessor, AutoModel\n",
      "\n",
      "# Initialize the DinoV2Embedding instance\n",
      "model_name = \"facebook/dinov2-large\"\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "preprocess = AutoImageProcessor.from_pretrained(model_name)\n",
      "model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
      "\n",
      "# Path to the image file\n",
      "img_file_path = \"\"  # Replace with your image path\n",
      "\n",
      "# Loop to get embeddings multiple times\n",
      "for i in range(5):\n",
      "    with torch.no_grad():\n",
      "\n",
      "        image = Image.open(img_file_path)\n",
      "        inputs = preprocess(images=image, return_tensors=\"pt\").to(device)\n",
      "        outputs = model(**inputs)\n",
      "        print(f\"Output {i + 1}: {outputs}\")Expected behaviorShould generate consistent output such that the embedding could be used for retrieval.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_380.txt:\n",
      "Title: Using accelerate launch FDSP cause weight saved after 2nd time onwards to be incomplete\n",
      "URL: https://github.com/huggingface/transformers/issues/31034\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.1Platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.23.1Safetensors version: 0.4.2Accelerate version: 0.30.1Accelerate config:    - compute_environment: LOCAL_MACHINE- distributed_type: FSDP- mixed_precision: bf16- use_cpu: False                                                                                                        - debug: False                                                                                                          - num_processes: 5                                                                                                      - machine_rank: 0                                                                                                       - num_machines: 1                                                                                                       - rdzv_backend: static                                                                                                  - same_network: True                                                                                                    - main_training_function: main                                                                                          - enable_cpu_affinity: False                                                                                            - fsdp_config: {'fsdp_auto_wrap_policy': 'SIZE_BASED_WRAP', 'fsdp_backward_prefetch': 'BACKWARD_PRE', 'fsdp_cpu_ram_efficient_loading': True, 'fsdp_forward_prefetch': False, 'fsdp_min_num_params': 100000000, 'fsdp_offload_params': False, 'fsdp_sharding_strategy': 'SHARD_GRAD_OP', 'fsdp_state_dict_type': 'FULL_STATE_DICT', 'fsdp_sync_module_states': True, 'fsdp_use_orig_params': True}                                                                                              - downcast_bf16: no                                                                                                     - tpu_use_cluster: False                                                                                                - tpu_use_sudo: False- tpu_env: []- dynamo_config: {'dynamo_backend': 'INDUCTOR'}PyTorch version (GPU?): 2.3.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: FDSP on 5 GPUs in 1 nodeWho can help?@pacman100@muellerzInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionTrain a model with FDSP as configured in accelerate configureFirsttimesaved weight OK (doesn't matter when the weight is saved, mid-epoch, first step, end of 100 epochs etc. as long as it's first time)2nd time saving onwards the weights are magically ~100MB smaller with all the keys BUT no weight in some of them, and wrong shape in others. Causes error when loading:I have so far tested both SigLIP and OWLv2, both has the same issue. Other models may also. Happens with both safetensor and pytorch.bin. pytorch_model_fdsp.bin is also missing them.I have set state dict to FULL.Expected behaviorNo issue saving\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_155.txt:\n",
      "Title: when i update transformers from 4.38.1 to  version==4.42.0, it happened that Failed to import trl.models.modeling_base because of the following error (look up to see its traceback): '>' not supported between instances of 'NoneType' and 'str'\n",
      "URL: https://github.com/huggingface/transformers/issues/32565\n",
      "Body:\n",
      "System Infowhen i update transformers from 4.38.1 to  version==4.42.0, it happened that Failed to import trl.models.modeling_base because of the following error (look up to see its traceback): '>' not supported between instances of 'NoneType' and 'str'Who can help?@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionwhen i update transformers from 4.38.1 to  version==4.42.0, it happened that Failed to import trl.models.modeling_base because of the following error (look up to see its traceback): '>' not supported between instances of 'NoneType' and 'str'Expected behavior@muellerzr@SunMarc\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_633.txt:\n",
      "Title: dinov2 with REGISTERS\n",
      "URL: https://github.com/huggingface/transformers/issues/27379\n",
      "Body:\n",
      "Model descriptionDear huggingface team,The fair team published an improved version of dinov2VISION TRANSFORMERS NEED REGISTERS. The models and checkpoints are available in thedinov2 website, but not in hugging face.Could you add this new model? I really appreciate your work.Best Wishes,ZongzeOpen source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationdinov2 reg checkpoint\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1029.txt:\n",
      "Title: Add FAVOR+ / Performer attention\n",
      "URL: https://github.com/huggingface/transformers/issues/7675\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_814.txt:\n",
      "Title: Flaky feature extraction tests for Donut\n",
      "URL: https://github.com/huggingface/transformers/issues/20738\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_196.txt:\n",
      "Title: Gemma 2 returns NaN when using default attn (sdpa) with padding\n",
      "URL: https://github.com/huggingface/transformers/issues/32390\n",
      "Body:\n",
      "System InfoPython 3.10Transformers 4.43.3Linux (Colab notebook)Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe default gemma 2 2b attn results in NaN for padding tokens. A simple demo can be seen below (also reproduced in thiscolab notebook):importtorchfromtransformersimportAutoModelForCausalLM,AutoTokenizermodel=AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\",device_map=\"auto\")tokenizer=AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")inputs=tokenizer([\"Hello I am a couch\",\"cats\"],return_tensors=\"pt\",padding=True).to('cuda')withtorch.no_grad():outputs=model(**inputs,output_hidden_states=True)print(outputs.logits)This returns the followingtensor([[[-24.3121,  -8.7513,  -6.9736,  ..., -18.3960, -17.4268, -24.3171],\n",
      "         [-16.8873,  -4.7767,   5.8828,  ...,  -9.4981,  -9.3307, -16.7723],\n",
      "         [-18.3313,   1.3191,  -4.6598,  ...,  -2.4244,   1.6774, -18.2153],\n",
      "         [-18.9110,  -5.8708, -11.7827,  ...,  -5.6606,  -4.2607, -18.8535],\n",
      "         [-20.1359,  -8.4194, -15.1834,  ..., -13.0231, -11.8288, -19.9716],\n",
      "         [-16.8807,   5.8885,   0.1881,  ...,  -3.7045,  -6.0659, -16.8421]],\n",
      "        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]]],\n",
      "       device='cuda:0')This can be fixed by changing theattn_implementationto anything exceptsdpaExpected behaviorUsing padding should not result in NaN for normal inputs to gemma 2 2b\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_828.txt:\n",
      "Title: Add VATT model\n",
      "URL: https://github.com/huggingface/transformers/issues/19865\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_182.txt:\n",
      "Title: WavLM Model is not loaded properly\n",
      "URL: https://github.com/huggingface/transformers/issues/32433\n",
      "Body:\n",
      "System InfoI dont have this commadWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionJust install the package (without specific version) for wavlm model and running this code:from transformers import AutoProcessor, WavLMModel, AutoModelimport torchmodel = AutoModel.from_pretrained(\"microsoft/wavlm-large\")I got the following warning, which should not be shown:Some weights of the model checkpoint at microsoft/wavlm-large were not used when initializing WavLMModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']This IS expected if you are initializing WavLMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).This IS NOT expected if you are initializing WavLMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of WavLMModel were not initialized from the model checkpoint at microsoft/wavlm-large and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']Expected behaviorI should get the loaded model without this warning.I suspect the warning comes from this line:transformers/src/transformers/models/wavlm/modeling_wavlm.pyLine 282\n",
      "      in7e5d46difhasattr(nn.utils.parametrizations,\"weight_norm\"):Since the weights are from old version packages so it's familiar with this parameterizations module\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_186.txt:\n",
      "Title: Recent changes is causing \"found at least two devices\"\n",
      "URL: https://github.com/huggingface/transformers/issues/32420\n",
      "Body:\n",
      "System Infotransformers 4.43.3, python 3.10, linuxWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI have received multiple reports that model loading behaviour recently changed which is causing a device error. This can usually be fixed by specifying thedevice_map, but prior to recent changes (I don't know when this happened), the model was loaded and could inference without any issues on multiple GPUs.RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0Referenced issues:casper-hansen/AutoAWQ#510casper-hansen/AutoAWQ#558casper-hansen/AutoAWQ#571Expected behaviorThe expected behavior is that we do not see these errors with the default settings ofdevice_map=None. I am generally not sure what exactly changed, so it is hard to be more precise\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_145.txt:\n",
      "Title: End-to-end generation compile cannot work !!!\n",
      "URL: https://github.com/huggingface/transformers/issues/32616\n",
      "Body:\n",
      "System Infoubuntu 22.04python 3.10torch 2.3.1transformers 4.44.0Who can help?@gante@fxmartyInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionInstall the torch==2.3.1 && transformers==4.44.0just copy the example code in Release NotesfromtransformersimportAutoModelForCausalLM,AutoTokenizerimporttorchimportcopymodel=AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\",torch_dtype=torch.bfloat16,device_map=\"auto\")tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")# compile generatecompiled_generate=torch.compile(model.generate,fullgraph=True,mode=\"reduce-overhead\")# compiled generate does NOT accept parameterization except a) model inputs b) a generation configgeneration_config=copy.deepcopy(model.generation_config)generation_config.pad_token_id=model.config.eos_token_idmodel_inputs=tokenizer([\"Write a poem about the market crashing in summer\"],return_tensors=\"pt\")model_inputs=model_inputs.to(model.device)output_compiled=compiled_generate(**model_inputs,generation_config=generation_config)print(output_compiled)Run it ! But I got the Error:Traceback (most recent call last):File \"/workspace/LLM/temp.py\", line 19, inoutput_compiled = compiled_generate(**model_inputs, generation_config=generation_config)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 451, in _fnreturn fn(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 921, in catch_errorsreturn callback(frame, cache_entry, hooks, frame_state, skip=1)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 400, in _convert_frame_assertreturn _compile(File \"/usr/lib/python3.10/contextlib.py\", line 79, in innerreturn func(*args, **kwds)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 676, in _compileguarded_code = compile_inner(code, one_graph, hooks, transform)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapperr = func(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 535, in compile_innerout_code = transform_code_object(code, transform)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1036, in transform_code_objecttransformations(instructions, code_options)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 165, in _fnreturn fn(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 500, in transformtracer.run()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2149, in runsuper().run()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in runand self.step()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in stepgetattr(self, inst.opname)(inst)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapperreturn inner_fn(self, inst)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1260, in CALL_FUNCTION_EXself.call_function(fn, argsvars.items, kwargsvars)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_functionself.push(fn.call_function(self, args, kwargs))File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/lazy.py\", line 94, in realize_and_forwardreturn getattr(self.realize(), name)(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 335, in call_functionreturn super().call_function(tx, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_functionreturn super().call_function(tx, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_functionreturn tx.inline_user_function_return(File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_returnreturn InliningInstructionTranslator.inline_call(self, fn, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/dynamo/symbolic_convert.py\", line 2285, in inline_callreturn cls.inline_call(parent, func, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/dynamo/symbolic_convert.py\", line 2399, in inline_calltracer.run()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in runand self.step()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in stepgetattr(self, inst.opname)(inst)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapperreturn inner_fn(self, inst)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1260, in CALL_FUNCTION_EXself.call_function(fn, argsvars.items, kwargsvars)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_functionself.push(fn.call_function(self, args, kwargs))File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_functionreturn super().call_function(tx, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_functionreturn tx.inline_user_function_return(File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_returnreturn InliningInstructionTranslator.inline_call(self, fn, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/dynamo/symbolic_convert.py\", line 2285, in inline_callreturn cls.inline_call(parent, func, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/dynamo/symbolic_convert.py\", line 2399, in inline_calltracer.run()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in runand self.step()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in stepgetattr(self, inst.opname)(inst)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapperreturn inner_fn(self, inst)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1219, in CALL_FUNCTIONself.call_function(fn, args, {})File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_functionself.push(fn.call_function(self, args, kwargs))File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 335, in call_functionreturn super().call_function(tx, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_functionreturn super().call_function(tx, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_functionreturn tx.inline_user_function_return(File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_returnreturn InliningInstructionTranslator.inline_call(self, fn, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/dynamo/symbolic_convert.py\", line 2285, in inline_callreturn cls.inline_call(parent, func, args, kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/dynamo/symbolic_convert.py\", line 2399, in inline_calltracer.run()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in runand self.step()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in stepgetattr(self, inst.opname)(inst)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1311, in LOAD_ATTRresult = BuiltinVariable(getattr).call_function(File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builtin.py\", line 687, in call_functionresult = handler(tx, *args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/builtin.py\", line 1304, in call_getattrreturn obj.var_getattr(tx, name)File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/misc.py\", line 289, in var_getattrfor name in self.inspected.inspect_parameter_names()File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/base.py\", line 341, in inspect_parameter_namesFile \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/exc.py\", line 190, in unimplementedraise Unsupported(msg)torch._dynamo.exc.Unsupported: inspect_parameter_names: FunctoolsPartialVariable()from user code:File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 36, in innerreturn fn(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_contextreturn func(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1689, in generateself._validate_model_kwargs(model_kwargs.copy())File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1207, in _validate_model_kwargsmodel_args |= set(inspect.signature(self.forward).parameters)Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more informationYou can suppress this exception and fall back to eager by setting:import torch._dynamotorch._dynamo.config.suppress_errors = TrueAs the infor suggested, I add these words:importtorch._dynamotorch._dynamo.config.suppress_errors=TrueBut I still got the same Error!Expected behaviorRun successfully\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_623.txt:\n",
      "Title: Getting equivalent results between Transformer's resize and tf.image.resize\n",
      "URL: https://github.com/huggingface/transformers/issues/27601\n",
      "Body:\n",
      "Feature requestFor the SigLIP model (#26522), I'd like to get equivalent results between tf.image.resize and the resize method available in Transformers.Here's what I tried:from PIL import Image\n",
      "import requests\n",
      "\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "\n",
      "def resize(image, size, method=\"bilinear\", antialias=False):\n",
      "    \"\"\"Resizes image to a given size.\"\"\"\n",
      "    # Note: use TF-2 version of tf.image.resize as the version in TF-1 is\n",
      "    # buggy: https://github.com/tensorflow/tensorflow/issues/6720.\n",
      "    # In particular it was not equivariant with rotation and lead to the network\n",
      "    # to learn a shortcut in self-supervised rotation task, if rotation was\n",
      "    # applied after resize.\n",
      "    dtype = image.dtype\n",
      "    tf_dtype = tf.type_spec_from_value(image).dtype\n",
      "    image = tf.image.resize(image, size, method=method, antialias=antialias)\n",
      "    return tf.cast(tf.clip_by_value(image, tf_dtype.min, tf_dtype.max), dtype)\n",
      "\n",
      "# load image\n",
      "url = 'https://cdn.openai.com/multimodal-neurons/assets/apple/apple-ipod.jpg'\n",
      "image = Image.open(requests.get(url, stream=True).raw)\n",
      "\n",
      "# get original pixel values\n",
      "original_pixel_values = resize(np.array(image), size=(224,224))\n",
      "\n",
      "# get our pixel values\n",
      "from transformers.image_transforms import resize\n",
      "\n",
      "pixel_values = resize(np.array(image), size=(224,224), resample=Image.Resampling.BILINEAR)\n",
      "\n",
      "# verify results\n",
      "np.testing.assert_array_equal(original_pixel_values, pixel_values)This currently fails with:AssertionError: \n",
      "Arrays are not equal\n",
      "\n",
      "Mismatched elements: 87370 / 150528 (58%)\n",
      "Max absolute difference: 255\n",
      "Max relative difference: 255.\n",
      " x: array([[[127, 101,  59],\n",
      "        [136, 112,  72],\n",
      "        [129, 109,  72],...\n",
      " y: array([[[131, 105,  63],\n",
      "        [138, 114,  74],\n",
      "        [126, 108,  70],...MotivationWould be great to have equivalent results such that logits match with the original implementation.Your contributionI provide a notebookherefor testing.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_151.txt:\n",
      "Title: Gradient checkpointing warning\n",
      "URL: https://github.com/huggingface/transformers/issues/32576\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.2Platform: Linux-4.18.0-477.27.1.el8_8.x86_64-x86_64-with-glibc2.28Python version: 3.9.4Huggingface_hub version: 0.23.2Safetensors version: 0.4.3Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: Tesla V100-FHHL-16GBWho can help?@ArthurZucker@muellerzr@sunmaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionHi, I need help regarding gradient checkpoining settings for a fine tuning LLM model. I want to use it for less gpu memory usage. The system info lists the system information and library versions.I am doing a text classification task using the AutoModelForSequenceClassification class with the Llama3 8b model. I load the model, then prepare the model forkbit trainig, useLoRAtechnique usingLoraConfigandget_peft_modeland usegradient_checkpointing=Truein Huggingface Trainer.Without gradient_checkpointing=True the training takes9:40 hoursand has about84% accuracy.If I use gradient_checkpointing=True, the training takes about4:47 hoursand has only70% accuracy. If I specify gradient_checkpointing=True in Trainer, I get these warnings:env/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: \n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant \n",
      "parameter should be passed explicitly. In version 2.4 we \n",
      "will raise an exception if use_reentrant is not passed. \n",
      "use_reentrant=False is recommended, but if you need \n",
      "to preserve the current default behavior, you can pass \n",
      "use_reentrant=True. Refer to docs for more details on the\n",
      " differences between the two variants.\n",
      " \n",
      "**warnings.warn(\n",
      "env/lib/python3.9/site-packages/torch/utils/checkpoint.py:91: \n",
      "UserWarning: None of the inputs have requires_grad=True. \n",
      "Gradients will be None\n",
      "  warnings.warn(**Thanks for any helpExpected behavior\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_21.txt:\n",
      "Title: Phi-3 maximum recursion depth exceeded when execute AutoModelForCausalLM.from_pretrained\n",
      "URL: https://github.com/huggingface/transformers/issues/33328\n",
      "Body:\n",
      "System InfoCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.transformersversion: 4.44.2Platform: Windows-10-10.0.22631-SP0Python version: 3.11.3Huggingface_hub version: 0.24.6Safetensors version: 0.4.5Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.1+cpu (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI ran this code several times successfully but I got the error and from that point I keep getting the error, this is the simple code I am runningimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipelinetorch.random.manual_seed(0)model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\",device_map=\"cuda\",torch_dtype=\"auto\",trust_remote_code=True,)Expected behaviorI expected to initialize the tokenizer so I can use it later in the code.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_35.txt:\n",
      "Title: prepare_fa2_from_position_ids error in training with batch_size > 1\n",
      "URL: https://github.com/huggingface/transformers/issues/33268\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): 2.13.1 (True)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: \"ddp\", \"deepspeed_stage_2\"Using GPU in script?: tested on 8xH100 and 1xA100-40GBGPU type: NVIDIA A100-SXM4-40GBWho can help?@RhuiDih@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThePR 31629allowed packing with no cross-contamination and without requiring to deal with attention masks for flash-attention-2.However, prepare_fa2_from_position_ids function produces an error when training with a batch_size greater than 1.Below is an end-to-end example to reproduce the error:import numpy as np\n",
      "import torch\n",
      "import lightning\n",
      "\n",
      "from transformers import MistralForCausalLM, MistralConfig\n",
      "\n",
      "\n",
      "config = MistralConfig(max_position_embeddings = 1024,\n",
      "                       hidden_size = 1024,\n",
      "                       intermediate_size = 3584,\n",
      "                       num_hidden_layers = 8,\n",
      "                       pad_token_id = 0,\n",
      "                       bos_token_id = 2,\n",
      "                       eos_token_id = 3)\n",
      "config._attn_implementation = \"flash_attention_2\"\n",
      "batch_size = 2\n",
      "\n",
      "class CustomDataset(torch.utils.data.Dataset):\n",
      "    def __init__(self, max_num_pack_attempts):\n",
      "        self.dataset = []\n",
      "        for _ in range(100_000):\n",
      "            # Generate samples of different lengths\n",
      "            sample_len = np.random.randint(10, config.max_position_embeddings)\n",
      "            tokens = np.random.randint(0, config.vocab_size, size = sample_len)\n",
      "            self.dataset.append(tokens)\n",
      "        self.max_num_pack_attempts = max_num_pack_attempts\n",
      "\n",
      "\n",
      "    def get_single_sample(self):\n",
      "        idx = np.random.randint(0, len(self.dataset))\n",
      "        tokens = self.dataset[idx]\n",
      "        return tokens.tolist()\n",
      "\n",
      "    def generate_pack(self):\n",
      "        input_ids = []\n",
      "        labels = [0] # placeholder for the model to shift right by 1\n",
      "        position_ids = []\n",
      "        num_failed_attempts = 0\n",
      "        \n",
      "        while (len(input_ids) < config.max_position_embeddings) and (num_failed_attempts < self.max_num_pack_attempts):\n",
      "            sample = self.get_single_sample()\n",
      "        \n",
      "            # If there is empty room\n",
      "            if len(input_ids) + len(sample) + 1 < config.max_position_embeddings:\n",
      "                input_ids += [config.bos_token_id] + sample\n",
      "                labels += sample + [config.eos_token_id]\n",
      "                position_ids += range(len(sample) + 1)\n",
      "            else:\n",
      "                num_failed_attempts += 1\n",
      "        \n",
      "        # Pad\n",
      "        input_ids = input_ids + [config.pad_token_id] * (config.max_position_embeddings - len(input_ids))\n",
      "        position_ids = position_ids + [config.pad_token_id] * (config.max_position_embeddings - len(position_ids))\n",
      "        labels = labels + [-100] * (config.max_position_embeddings - len(labels))\n",
      "    \n",
      "        return {\n",
      "                'input_ids': torch.tensor(input_ids),\n",
      "                'position_ids': torch.tensor(position_ids),\n",
      "                'labels': torch.tensor(labels, dtype=torch.long)\n",
      "               }\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        return self.generate_pack()\n",
      "\n",
      "\n",
      "class CustomModel(lightning.LightningModule):\n",
      "    def __init__(self, config, learning_rate):\n",
      "        super(CustomModel, self).__init__()\n",
      "        self.model = MistralForCausalLM(config = config)\n",
      "        self.learning_rate = learning_rate\n",
      "\n",
      "        num_params = sum(p.numel() for p in self.model.parameters())\n",
      "        print(f'Number of parameters in Mistral: {num_params:,}')\n",
      "\n",
      "    def forward(self, input_ids, position_ids, labels = None):\n",
      "        return self.model(input_ids = input_ids,\n",
      "                          position_ids = position_ids,\n",
      "                          labels = labels,\n",
      "                          use_cache=False)\n",
      "\n",
      "    def training_step(self, batch, batch_idx):\n",
      "        input_ids = batch['input_ids']\n",
      "        position_ids = batch['position_ids']\n",
      "        labels = batch['labels']\n",
      "        \n",
      "        outputs = self(input_ids, position_ids, labels)\n",
      "        loss = outputs.loss\n",
      "        return loss\n",
      "\n",
      "    def configure_optimizers(self):\n",
      "        opt = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, betas = (0.9, 0.95), eps = 1e-8, weight_decay = 0.1)\n",
      "        return {\"optimizer\": opt}\n",
      "\n",
      "data_loader = torch.utils.data.DataLoader(CustomDataset(10), batch_size=batch_size, num_workers=1)\n",
      "model = CustomModel(config, 3e-4)\n",
      "print(model.model.model.layers[0].self_attn) # print the model's self attention layer name to make sure it uses FA2\n",
      "\n",
      "# TRAINER\n",
      "trainer = lightning.Trainer(\n",
      "    max_steps = 2_000,\n",
      "    accelerator=\"gpu\",\n",
      "    precision = \"bf16-mixed\",\n",
      "    limit_train_batches = 1_000\n",
      ")\n",
      "\n",
      "trainer.fit(model, train_dataloaders=data_loader)The error:Epoch 0:   0%|                                                                                                                                             | 0/1000 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/fa2_from_position_ids_test.py\", line 111, in <module>\n",
      "    trainer.fit(model, train_dataloaders=data_loader)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 250, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 190, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 268, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 167, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/core/module.py\", line 1306, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py\", line 153, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py\", line 238, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/amp.py\", line 75, in optimizer_step\n",
      "    return super().optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 122, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 484, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 89, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/optim/adamw.py\", line 204, in step\n",
      "    loss = closure()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/precision.py\", line 108, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 144, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 129, in closure\n",
      "    step_output = self._step_fn()\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 317, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py\", line 319, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py\", line 390, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/home/ubuntu/fa2_from_position_ids_test.py\", line 91, in training_step\n",
      "    outputs = self(input_ids, position_ids, labels)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ubuntu/fa2_from_position_ids_test.py\", line 81, in forward\n",
      "    return self.model(input_ids = input_ids,\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1033, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 808, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 549, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 382, in forward\n",
      "    attn_output = _flash_attention_forward(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py\", line 272, in _flash_attention_forward\n",
      "    query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = prepare_fa2_from_position_ids(\n",
      "  File \"/home/ubuntu/.local/lib/python3.10/site-packages/transformers/modeling_flash_attention_utils.py\", line 166, in prepare_fa2_from_position_ids\n",
      "    key = key.view(-1, key.size(-2), key.size(-1))\n",
      "RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.When batch_size is set to 1, the training takes place without an error.I conducted tests on 8xH100 and 1xA100-40GB, trying different training strategies, e.g., \"ddp\", \"deepspeed_stage_2\" and ended up with the same error.Expected behaviorThe training should be possible without an error for different batch_size values.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_384.txt:\n",
      "Title: Training GPT2 with run_clm.py exceeds the described memory amount .\n",
      "URL: https://github.com/huggingface/transformers/issues/30969\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.0.dev0Platform: Linux-6.5.0-28-generic-x86_64-with-glibc2.17Python version: 3.8.19Huggingface_hub version: 0.22.2Safetensors version: 0.4.2Accelerate version: 0.29.2Accelerate config:    not foundPyTorch version (GPU?): 1.10.0+cu111 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@ArthurZuckerand@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionpython run_clm.py--model_name_or_path openai-community/gpt2--dataset_name wikitext--dataset_config_name wikitext-2-raw-v1--per_device_train_batch_size 8--per_device_eval_batch_size 8--do_train--do_eval--overwrite_output_dir--output_dir /tmp/test-clmExpected behaviorThe example in the script mentions training with a K80 GPU at a batch size of 8, noting that the K80 has 24GB of memory. However, when I use an RTX 3090 with a batch size set to 4, it consumes 20GB of memory without modifying any settings. Why is this the case?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_347.txt:\n",
      "Title: Batch is empty when fine-tuning flan-t5 using LoRA\n",
      "URL: https://github.com/huggingface/transformers/issues/31357\n",
      "Body:\n",
      "System Infotransformersversion: 4.38.2Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.35Python version: 3.11.9Huggingface_hub version: 0.23.2Safetensors version: 0.4.3Accelerate version: 0.30.1Accelerate config:    not foundPyTorch version (GPU?): 2.3.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe issue is reported here:https://discuss.huggingface.co/t/valueerror-the-batch-received-was-empty-your-model-wont-be-able-to-train-on-it-double-check-that-your-training-dataset-contains-keys-expected-by-the-model-args-kwargs-label-ids-label/20200Expected behaviorCorrectly working fine-tuning with LoRA. Anyway, I thing the bug can be solved adding:self._signature_columns+=list(set([\"labels\",\"input_ids\"]))To the function_set_signature_columns_if_needed-here.May not be the best way to go tho.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_353.txt:\n",
      "Title: Support saving models trained with DeepSpeed in Trainer callbacks\n",
      "URL: https://github.com/huggingface/transformers/issues/31338\n",
      "Body:\n",
      "Feature requestTrainer callbackspass a modelto any registered callback, but this model cannot be saved if training with DeepSpeed Stage 3 (needs access toTrainer.acceleratorandTrainer.model_wrapped)MotivationI have a custom callback that logs a model to a tracking server at the end of training, but need access toTrainer.acceleratorandTrainer.model_wrappedin my callback to prevent skipping saving sharded tensors. I believe this may lead to a bug in the officialwandbcallback which uses a\"fake trainer\"(I don't usewandbso can't confirm, but see an error similar to one reported in another repo if I try the \"fake trainer\" approach in my custom callback e.g.,axolotl-ai-cloud/axolotl#1092)Your contributionI can contribute this feature, but wanted to get guidance on the design. Roughlytrainer_callback.CallbackHandlerreceives an additional arg,accelerator, which will be passedTrainer.acceleratoron instantiationtrainer_callback.CallbackHandlerhas an attributemodel_wrappedwhich gets updated withTrainer.model_wrappedbyTrainertrainer_callback.CallbackHandler.call_eventwill pass alongself.acceleratorandself.model_wrappedwhen calling  callbacksThis allows my callback to do something likestate_dict = accelerator.get_state_dict(model_wrapped)and pass that along tomodel.save_pretrainedas to not skip saving sharded tensorsIf there is a better design or if this is better classified as a bug, please let me know\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_435.txt:\n",
      "Title: Support multipletimmas backbones withforward_intermediatesmethod\n",
      "URL: https://github.com/huggingface/transformers/issues/30307\n",
      "Body:\n",
      "Feature requestCurrently, inTimmBackbonerequires that thetimmmodel has areturn_layers, which I'm not sure if any model implements:transformers/src/transformers/models/timm_backbone/modeling_timm_backbone.pyLines 80 to 83\n",
      "      inc15aad0# These are used to control the output of the model when called. If output_hidden_states is True, then# return_layers is modified to include all layers.self._return_layers=self._backbone.return_layersself._all_layers={layer[\"module\"]:str(i)fori,layerinenumerate(self._backbone.feature_info.info)}Lately,timmimplemented  aforward_intermediatesmethod to almost all networks for  feature extraction -issueandpr.MotivationThus, will little changes, we will be able to doimporttransformersmodel=transformers.Mask2FormerModel(config=transformers.Mask2FormerConfig(use_timm_backbone=True,backbone=\"vit_small_patch16_224\",\n",
      "    ),\n",
      ")as currently we get the error:AttributeError:'FeatureGetterNet'object has no attribute'return_layers'Your contributionReview and/or implemented the PR\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_596.txt:\n",
      "Title: Can i convert open-clip trained models (.pt) using code “src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py” ？\n",
      "URL: https://github.com/huggingface/transformers/issues/28072\n",
      "Body:\n",
      "Model descriptionopenclip：https://github.com/mlfoundations/open_clipi use openclip  to train model and get \"epoch_400.pt\".and i want to convert this \"epoch_400.pt\" to hf, so i run:python src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py     --pytorch_dump_folder_path \"./openclip_syf_hf\"     --checkpoint_path \"/openclip_output/2023_12_07-15_24_24-model_ViT-B-32-lr_0.0005-b_256-j_8-p_amp/checkpoints/epoch_400.pt\"     --config_path \"/open_clip-main/src/open_clip/model_configs/ViT-B-32.json\"but get bug:Traceback (most recent call last):File \"/home/anaconda3/envs/transformer/lib/python3.8/site-packages/clip/clip.py\", line 130, in loadmodel = torch.jit.load(opened_file, map_location=device if jit else \"cpu\").eval()File \"/home/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/jit/_serialization.py\", line 164, in loadcpp_module = torch._C.import_ir_module_from_buffer(RuntimeError: PytorchStreamReader failed locating file constants.pkl: file not foundDuring handling of the above exception, another exception occurred:Traceback (most recent call last):File \"src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py\", line 150, inconvert_clip_checkpoint(args.checkpoint_path, args.pytorch_dump_folder_path, args.config_path)File \"/home/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_contextreturn func(*args, **kwargs)File \"src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py\", line 120, in convert_clip_checkpointpt_model, _ = load(checkpoint_path, device=\"cpu\", jit=False)File \"/home/anaconda3/envs/transformer/lib/python3.8/site-packages/clip/clip.py\", line 137, in loadstate_dict = torch.load(opened_file, map_location=\"cpu\")File \"/home/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/serialization.py\", line 795, in loadreturn _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)File \"/home/anaconda3/envs/transformer/lib/python3.8/site-packages/torch/serialization.py\", line 1002, in _legacy_loadmagic_number = pickle_module.load(f, **pickle_load_args)EOFError: Ran out of inputso i am wondering if i can convert open-clip trained models (.pt) using code “src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py” ？Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationNo response\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_569.txt:\n",
      "Title: How to use an efficient encoder as shared EncoderDecoderModel?\n",
      "URL: https://github.com/huggingface/transformers/issues/28388\n",
      "Body:\n",
      "Feature requestEfficient encoder like destilBERT, ALBERT or ELECTRA aren't supported as decoder of the EncoderDecoderModel and so they can't be shared as encoder and decoder.MotivationWarm-starting shared models is a powerful way to build transformer models. Yet the efficient models can't be used.Your contributionWe could implement the support for destilBERT, ALBERT or ELECTRA. They shouldn't be that different from other encoders.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_555.txt:\n",
      "Title: GPT2 cannot be used with device_map='auto'; Report \"found at least two devices\"\n",
      "URL: https://github.com/huggingface/transformers/issues/28672\n",
      "Body:\n",
      "System Infotransformersversion: 4.36.2Platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.35Python version: 3.9.18Huggingface_hub version: 0.20.2Safetensors version: 0.4.1Accelerate version: 0.26.1Accelerate config: \tnot foundPyTorch version (GPU?): 2.1.2+cu118 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionA simple reproducer here:fromtransformersimportGPT2LMHeadModel# create a sample input:batch_ids={'input_ids':torch.tensor([[312,134,56,712,351,89,63,550,971,2]]),'attention_mask':torch.tensor([[1,1,1,1,1,1,1,1,1,1]]),\n",
      "}gpt2_large=GPT2LMHeadModel.from_pretrained('gpt2-large',cache_dir='./cache_dir',device_map='auto')gpt2=GPT2LMHeadModel.from_pretrained('gpt2',cache_dir='./cache_dir',device_map='auto')loss_gpt2_large=gpt2_large(**batch_ids,labels=batch_ids['input_ids']).lossloss_gpt2=gpt2(**batch_ids,labels=batch_ids['input_ids']).lossExpected behaviorIt works well to generateloss_gpt2_large, but it will report error when generatingloss_gpt2:RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)I am not sure why this behaves differently with the same model class. Could you please provide any comments on this? Thanks in advance!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_233.txt:\n",
      "Title: Broken accuracy on LLaMa 3.1 70B -- worse than even 8B\n",
      "URL: https://github.com/huggingface/transformers/issues/32205\n",
      "Body:\n",
      "System Infotransformers==4.43.1As built into vLLM or TGI as well.4H100 for llama 3.1 70B or 1H100 for llama3.1 8BWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionvllm-project/vllm#6760Expected behaviorI'd expect long-context performance to not be worse for 70B llama 3.1 compared to 8B, and for the answers to be \"normal\" and not so brief.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_227.txt:\n",
      "Title: Add New Optimizer\n",
      "URL: https://github.com/huggingface/transformers/issues/32225\n",
      "Body:\n",
      "Feature requestI want to add a new optimizer that I worked on itIs there any doc or anything how to add this new optimizer to all files of transformers library?MotivationThis optimizer would be great to implement on transformers and reduce time to train a new modelYour contributionI can make a clone of this repo and then changes file and submit PR. But I can't find a document how to implement my algorithm to transformers Files because  I found so much classes and there are a lots of file that need changes.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_541.txt:\n",
      "Title: Add MistralForQuestionAnswering\n",
      "URL: https://github.com/huggingface/transformers/issues/28908\n",
      "Body:\n",
      "Feature requestAdd a MistralForQuestionAnswering class to themodeling_mistral.pyso Mistral models have AutoModelForQuestionAnswering support (by also adding Mistral models to the MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES  in themodeling_auto.pyfile.Motivation1 - Evaluation benchmarks likeSquadorFaQUADare commonly used to evaluate language models.2 - Many decoder-only transformers (BLOOM,Falcon,OpenAI GPT-2,GPT Neo,GPT NeoX,GPT-J, etc.) have support for the AutoModelForQuestionAnswering.3 - Creating a fine-tuning/evaluation procedure using things like AutoModelForQuestionAnswering and evaluate.load('squad') is very simple, making these features very helpful and desirable.4 - On the contrary, if one cannot use AutoModelForQuestionAnswering, like in the Llama style models, everything becomes more difficult.Hence, I would like to request the addition of a MistralForQuestionAnswering class to the modeling_mistral.py file. Hence, we could all easily perform experiments with Mistral models and squad-style Q&A benchmarks:Your contributionI have recently added LlamaForQuestionAnswering class in modeling_llama.py file. I can do the same for Mistral.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_964.txt:\n",
      "Title: [testing] making network tests more reliable\n",
      "URL: https://github.com/huggingface/transformers/issues/12061\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_958.txt:\n",
      "Title: KeyError: 'labels' during Distilling Zero Shot Classification\n",
      "URL: https://github.com/huggingface/transformers/issues/12182\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_780.txt:\n",
      "Title: Open AI GPT Model Implementation in Flax\n",
      "URL: https://github.com/huggingface/transformers/issues/22647\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_743.txt:\n",
      "Title: MeZo Forward Pass Implementation\n",
      "URL: https://github.com/huggingface/transformers/issues/24264\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_781.txt:\n",
      "Title: Addoutput_hidden_stateandoutput_scoresto Flax generate\n",
      "URL: https://github.com/huggingface/transformers/issues/22612\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_795.txt:\n",
      "Title: Add BEiTv3\n",
      "URL: https://github.com/huggingface/transformers/issues/22178\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_971.txt:\n",
      "Title: Distil BART for text simplification\n",
      "URL: https://github.com/huggingface/transformers/issues/11594\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_226.txt:\n",
      "Title: SinkCache with Qwen1.5 broken in 4.43.0+\n",
      "URL: https://github.com/huggingface/transformers/issues/32233\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.2Platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35Python version: 3.9.16Huggingface_hub version: 0.24.0Safetensors version: 0.4.2Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.1.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: NoUsing GPU in script?: NoGPU type: NVIDIA RTX A6000Who can help?@zucchini-nlp@ganteInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun the code below on CPU (using GPU hides the actual error behind aRuntimeError: CUDA error: device-side assert triggered).***Works fine in v4.42.4, error appears only in 4.43.0+******Tested Llama-2, Mistral, and Qwen1.5 models. Issue only appears to affect Qwen1.5, but may impact other models that I didn't test.***This appears to be a separate issue from#31381fromtransformersimportAutoModelForCausalLM,AutoTokenizer,SinkCachefromtransformers.trainer_utilsimportset_seed# Load the model and tokenizer# model_name = \"meta-llama/Llama-2-7b-hf\" # <-- this works!# model_name = \"mistralai/Mistral-7B-v0.1\" # <-- this works!model_name=\"Qwen/Qwen1.5-1.8B\"# <-- this doesn't work!tokenizer=AutoTokenizer.from_pretrained(model_name)model=AutoModelForCausalLM.from_pretrained(model_name)# Prepare the inputinput_text=\"The quick brown fox jumps over the lazy\"inputs=tokenizer(input_text,return_tensors=\"pt\")# Generate the outputset_seed(42)sink_cache=SinkCache(window_length=50,num_sink_tokens=8)output=model.generate(**inputs,use_cache=True,past_key_values=sink_cache,max_new_tokens=100,do_sample=True,top_p=0.9,\n",
      ")# Decode the outputoutput_text=tokenizer.decode(output[0])print(output_text)Traceback:Traceback (most recent call last):\n",
      "  File \"/remote/ayuser/user/sinkcache_test.py\", line 19, in <module>\n",
      "    output = model.generate(\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/transformers/generation/utils.py\", line 1989, in generate\n",
      "    result = self._sample(\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/transformers/generation/utils.py\", line 2932, in _sample\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1054, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 856, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 596, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 496, in forward\n",
      "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
      "  File \"/home/user/anaconda3/envs/bark/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py\", line 149, in apply_rotary_pos_emb\n",
      "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
      "IndexError: index 50 is out of bounds for dimension 0 with size 50Expected behaviorThe generation would complete with no error.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_540.txt:\n",
      "Title: Observed_masks not behaving as expected\n",
      "URL: https://github.com/huggingface/transformers/issues/28914\n",
      "Body:\n",
      "System Infocompute_environment: LOCAL_MACHINEdeepspeed_config: {}distributed_type: MULTI_GPUfsdp_config: {}machine_rank: 0main_process_ip: nullmain_process_port: nullmain_training_function: mainmixed_precision: fp16num_machines: 1num_processes: 4use_cpu: falseWho can help?@pacman100@muellerzrInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI am doing TimeSeriesTransformerForPrediction but I am getting the following error when trying to train the model.torch/nn/parallel/distributed.py\", line 1026, in forward\n",
      "    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n",
      "RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \n",
      "making sure all `forward` function outputs participate in calculating loss. \n",
      "If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\n",
      "Parameter indices which did not receive grad for rank 2: 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172\n",
      " In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error\n",
      "    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n",
      "RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by \n",
      "making sure all `forward` function outputs participate in calculating loss. \n",
      "If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).Expected behaviorFor context, this issue is happening for TimeSeriesTransformerForPrediction.From what I can tell, it is only happening when there are 0's in the beginning of the past_values segments. I believe that the error is getting thrown because the past_observed_mask is putting a corresponding 0 for all the 0's before any non-zero value (see pictures below). I would like the algorithm to learn/train on the 0's, since they are indeed 0's and not NaN or missing values (as the 0 in the past_observed_mask description would infer).When I take the advice of the error message and set the find_unused_parameters=True, I get the following error:ValueError: Expected parameter df (Tensor of shape (256, 45)) of distribution Chi2() to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values.Can someone please advice how to fix this issue?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_554.txt:\n",
      "Title: Load an EncoderDecoderModel as AutoModel\n",
      "URL: https://github.com/huggingface/transformers/issues/28721\n",
      "Body:\n",
      "System Infotransformersversion: 4.35.0Platform: Linux-5.15.0-91-lowlatency-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.17.3Safetensors version: 0.4.0Accelerate version: 0.24.1Accelerate config: \tnot foundPyTorch version (GPU?): 2.1.2+cu121 (False)Tensorflow version (GPU?): 2.11.0 (False)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: noUsing distributed or parallel set-up in script?: noWho can help?@ArthurZuckerand@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoTokenizer, AutoModeltokenizer = AutoTokenizer.from_pretrained(\"Bachstelze/instructionRoberta-base\")model = AutoModel.from_pretrained(\"Bachstelze/instructionRoberta-base\", output_attentions=True)Expected behaviorLoad the EncoderDecoderModel as AutoModel. \"BertGenerationConfig\" is supported, though this seems outdated.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_232.txt:\n",
      "Title: Does GroundingDINO support batched inference?\n",
      "URL: https://github.com/huggingface/transformers/issues/32206\n",
      "Body:\n",
      "It seems like grounding-dino states in the documentation that it can take a batch of images, but when I try to do so, I get an error, as specified here -https://discuss.huggingface.co/t/how-to-perform-batch-inference-on-groundingdino-model/90940.Is it supposed to work?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_568.txt:\n",
      "Title: Add FlashAttention-2 support for Mask2Former model\n",
      "URL: https://github.com/huggingface/transformers/issues/28409\n",
      "Body:\n",
      "Feature requestIs it possible to add FlashAttention-2 support to the Mask2Former model?MotivationSince it is already availble for ViT, it would be great to have it on Mask2Former too.Maybe the additional input masks to the decoder layer represent a major challenge?Your contributionI could help by testing the implementations.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_352.txt:\n",
      "Title: Implementation Issue of Phi3SuScaledRotaryEmbedding\n",
      "URL: https://github.com/huggingface/transformers/issues/31339\n",
      "Body:\n",
      "Feature requestImproving the Implementation of Phi3SuScaledRotaryEmbedding to Reduce Unnecessary ComputationI'm not entirely sure if there is a deeper meaning to the implementation here. It seems that the inv_freq could be completely initialized in the__init__method aslong_inv_freqandshort_inv_freq.The only parameter used here is the part that gets the device.transformers/src/transformers/models/phi3/modeling_phi3.pyLines 131 to 137\n",
      "      in25245ec@torch.no_grad()defforward(self,x,position_ids,seq_len=None):seq_len=torch.max(position_ids)+1ifseq_len>self.original_max_position_embeddings:ext_factors=torch.tensor(self.long_factor,dtype=torch.float32,device=x.device)else:ext_factors=torch.tensor(self.short_factor,dtype=torch.float32,device=x.device)MotivationFixing this code can eliminate a redundant computation. Moreover, self.inv_freq used here doesn't seem appropriate to be a attribute.I tested the modified version locally. Although I couldn't test with long contexts due to my device's performance, the new implementation doesn't seem to show any issues with short sentences.Your contributionI can fix it if this isn't intentional.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_420.txt:\n",
      "Title: Support align_corners=True in image_transforms module\n",
      "URL: https://github.com/huggingface/transformers/issues/30525\n",
      "Body:\n",
      "Feature requestFor a new model I'm working on#30136I'd need to resize images in the image processor usingalign_corners=True, as the original code usestorch.nn.functional(..., align_corners=True)for resizing images during pre-processing.MotivationWould be great to have this option available so that we can remove the torch dependency from the image processorYour contributionNot sure I can look into this, but@molbapshowed interest in looking into this\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_346.txt:\n",
      "Title: Problems when using SinkCache for model.generate()\n",
      "URL: https://github.com/huggingface/transformers/issues/31381\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-5.4.0-173-generic-x86_64-with-glibc2.31Python version: 3.10.0Huggingface_hub version: 0.23.3Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@zucchini-nlp@gante@tomaarseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI am trying to use sinkcache for multi-turn dialog where the cache should contains the previous turn's dialog. Here is my code:import torch\n",
      "\n",
      "from transformers import SinkCache\n",
      "\n",
      "from transformers import AutoConfig, LlamaForCausalLM, AutoTokenizer\n",
      "\n",
      "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
      "model = LlamaForCausalLM.from_pretrained(\n",
      "      model_id, low_cpu_mem_usage=True, device_map='auto',\n",
      "      torch_dtype=torch.bfloat16,cache_dir=\"cache\")\n",
      "model = model.eval()\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id,cache_dir=\"cache\")\n",
      "tokenizer.pad_token = tokenizer.eos_token\n",
      "tokenizer.padding_side = 'left'\n",
      "\n",
      "device = model.device\n",
      "\n",
      "cache = SinkCache(window_length=1024, num_sink_tokens=4)\n",
      "prefix_list=[\"hello,my name is yy\",\"what is my name?\"]\n",
      "for prefix in prefix_list:\n",
      "    inputs = tokenizer(prefix, return_tensors='pt').to(device)\n",
      "\n",
      "    \n",
      "    gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=64,\n",
      "                            use_cache=True,\n",
      "                            past_key_values=cache,\n",
      "                            pad_token_id=tokenizer.pad_token_id)\n",
      "    \n",
      "    decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n",
      "\n",
      "    print(decoded)I am not sure if  I was doing it right. But I got the following error:Traceback (most recent call last):\n",
      "  File \"/data/yaoy/long_context/repeat_sirllm/main.py\", line 25, in <module>\n",
      "    gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=64,\n",
      "  File \"/data/yaoy/anaconda/envs/long/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/data/yaoy/anaconda/envs/long/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1758, in generate\n",
      "    result = self._sample(\n",
      "  File \"/data/yaoy/anaconda/envs/long/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2390, in _sample\n",
      "    model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n",
      "  File \"/data/yaoy/anaconda/envs/long/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1326, in _get_initial_cache_position\n",
      "    model_kwargs[\"cache_position\"] = torch.arange(past_length, cur_len, device=input_ids.device)\n",
      "RuntimeError: upper bound and larger bound inconsistent with step signExpected behaviorCache should include information from previous conversations.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_408.txt:\n",
      "Title: Add TableTransformerImageProcessor\n",
      "URL: https://github.com/huggingface/transformers/issues/30718\n",
      "Body:\n",
      "Feature requestTheTable Transformeris a model with basically the same architecture asDETR.Now, when people do this:from transformers import AutoImageProcessor\n",
      "\n",
      "processor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-detection\")\n",
      "print(type(processor))this will printDetrImageProcessor.However, Table Transformer has some specific image processing settings which aren't exactly the same as in DETR:from torchvision import transforms\n",
      "\n",
      "class MaxResize(object):\n",
      "    def __init__(self, max_size=800):\n",
      "        self.max_size = max_size\n",
      "\n",
      "    def __call__(self, image):\n",
      "        width, height = image.size\n",
      "        current_max_size = max(width, height)\n",
      "        scale = self.max_size / current_max_size\n",
      "        resized_image = image.resize((int(round(scale*width)), int(round(scale*height))))\n",
      "        \n",
      "        return resized_image\n",
      "\n",
      "# this is required for the table detection models\n",
      "detection_transform = transforms.Compose([\n",
      "    MaxResize(800),\n",
      "    transforms.ToTensor(),\n",
      "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
      "])\n",
      "\n",
      "# this is required for the table structure recognition models\n",
      "structure_transform = transforms.Compose([\n",
      "    MaxResize(1000),\n",
      "    transforms.ToTensor(),\n",
      "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
      "])Hence we could create a separateTableTransformerImageProcessorwhich replicates this.MotivationWould be great to 100% replicate original preprocessing settingsYour contributionI could work on this but would be great if someone else can take this up\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_391.txt:\n",
      "Title: Kosmos-2.5 implementation in transformers\n",
      "URL: https://github.com/huggingface/transformers/issues/30877\n",
      "Body:\n",
      "Model descriptionHello everyone,The Kosmos-2.5 is a multimodal literate model that can be used for tasks such as OCR and text-rich image comprehension. It includes a ViT encoder, a Resampler, and a shared decoder module. To the best of my knowledge, the architecture of this model is similar to Kosmos-2 but has some differences. Due to these differences, using this model in Transformers requires a standalone implementation.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationPaper:https://arxiv.org/pdf/2309.11419Code:https://github.com/microsoft/unilm/tree/master/kosmos-2.5Authors:@Dod-o@wolfshow\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_34.txt:\n",
      "Title: Static KV cache status: How to use it? Does it work for all models?\n",
      "URL: https://github.com/huggingface/transformers/issues/33270\n",
      "Body:\n",
      "I see that there are many PRs aboutStaticCache, but I couldn't find a clear documentation on how to use it.What I wantTo not have Transformers allocate memory dynamically for the KV cache when usingmodel.generate(), as that leads to increased memory usage (due to garbage collection not happening fast/often enough) and worse performance.To use that by default always, for every model, for every supported quantization backend (AutoAWQ, AutoGPTQ, AQLM, bitsandbytes, etc).Who can help?Maybe@gante\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_636.txt:\n",
      "Title: Add ReCag model\n",
      "URL: https://github.com/huggingface/transformers/issues/27308\n",
      "Body:\n",
      "Model descriptionWant to add a new model ReCag in Transformers.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationNo response\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_144.txt:\n",
      "Title: Static Cache is broken with multi-gpu inference\n",
      "URL: https://github.com/huggingface/transformers/issues/32624\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: Linux-6.5.0-15-generic-x86_64-with-glibc2.35Python version: 3.10.14Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.5.0.dev20240812+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA A100 80GB PCIeWho can help?@ArthurZucker@ganteInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionCurrently setting the cache type to staticmodel.generation_config.cache_implementation =\"static\"or usingStaticCachebreaks with multi-gpu. It throws the following error, probably because the cache is not placed on the right device on some layers:File/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:169,inadd_hook_to_module.<locals>.new_forward(module,*args,**kwargs)167output=module._old_forward(*args,**kwargs)168else:-->169output=module._old_forward(*args,**kwargs)170returnmodule._hf_hook.post_forward(module,output)File/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:640,inLlamaSdpaAttention.forward(self,hidden_states,attention_mask,position_ids,past_key_value,output_attentions,use_cache,cache_position,position_embeddings,**kwargs)637ifpast_key_valueisnotNone:638# sin and cos are specific to RoPE models; cache_position needed for the static cache639cache_kwargs={\"sin\":sin,\"cos\":cos,\"cache_position\":cache_position}-->640key_states,value_states=past_key_value.update(key_states,value_states,self.layer_idx,cache_kwargs)642key_states=repeat_kv(key_states,self.num_key_value_groups)643value_states=repeat_kv(value_states,self.num_key_value_groups)File/opt/conda/lib/python3.10/site-packages/transformers/cache_utils.py:1083,inStaticCache.update(self,key_states,value_states,layer_idx,cache_kwargs)1080try:1081# If using several devices (e.g.: multiple GPUs), we need to ensure everything is on the same one1082cache_position.to(device=k_out.device)->1083k_out.index_copy_(2,cache_position,key_states)1084v_out.index_copy_(2,cache_position,value_states)1085exceptNotImplementedError:1086# The operator 'aten::index_copy.out' is not currently implemented for the MPS device.RuntimeError:Expectedalltensorstobeonthesamedevice,butfoundatleasttwodevices,cuda:0andcuda:1! (whencheckingargumentforargumentindexinmethodwrapper_CUDA_index_copy_)I can reproduce this with Llama3 70B on 2xA100. The dynamic cache version is working fine and the static cache on a single GPU with a smaller model (same model but quantized) works fine.Expected behaviorStatic cache generation should work with multi-gpu runtime.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_622.txt:\n",
      "Title: Extend Chat Template Tokenization for Training/Finetuning\n",
      "URL: https://github.com/huggingface/transformers/issues/27609\n",
      "Body:\n",
      "Feature requestExtendtokenizer.apply_chat_templatewith functionality for training/finetuning, returningattention_masksand (optional)labels(for ignoring \"System\" and \"User\" messages during loss computation).I think this requires the following steps:Adding support for taking in a batch of conversations (e.g.,List[Conversation := List[Dict[str, str]])Invoking the nativetokenizer.__call__()after applying the template to each example (passing through padding, truncation, any other parameters).Important: Adding an optional output forlabels-- a \"masked\" version of the returnedinput_idswith tokens corresponding to the System/User roles set to be ignored for loss computation (e.g., set toIGNORE_INDEX = -100).MotivationThe newtokenizer.apply_chat_templatefeature is great, and resolves a lot of ambiguity when it comes to formatting inputs for chat-based LLMs.However, right now it's geared for inference-time usage, only taking a single \"conversation\" and outputting theinput_ids(tokens) after applying the chat template.When finetuning models on chat-based data, it would be really nice to unify theapply_chat_templateAPI with thetokenizer.__call__()API, returningattention_masksand (optionally)labels(with \"System\" and \"User\" role text automatically ignored for loss computation).Your contributionI can try building a proof-of-concept for a \"standard\" workflow and Draft PR; I think there'd need to be a few discussions about the actual implementation details though!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_187.txt:\n",
      "Title: SequenceBiasLogitsProcessorparameter cannot be specified in the the json config file\n",
      "URL: https://github.com/huggingface/transformers/issues/32416\n",
      "Body:\n",
      "System InfoGeneral issueWho can help?@zucchini-nlp@ganteThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSequenceBiasLogitsProcessor'sinput argument parametersequence_bias (Dict[Tuple[int], float])can not be specified in the generation_config.json.This is because the dictionary withtupleas keys are not supported in json (not json serializable).Expected behaviorWe should be able to specify thesequence_biasparameter in the config file Iike any other generation config parameters.One way to support this is by changing the input argument type formDict[Tuple[int], float]toList[Tuple[int], float]\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_813.txt:\n",
      "Title: Add GPT-2-climate\n",
      "URL: https://github.com/huggingface/transformers/issues/20747\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_807.txt:\n",
      "Title: Add support for BLIP and GIT in image-to-text and VQA pipelines\n",
      "URL: https://github.com/huggingface/transformers/issues/21110\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_185.txt:\n",
      "Title: IDEFICS-9B output NAN with transformer 4.33.2\n",
      "URL: https://github.com/huggingface/transformers/issues/32426\n",
      "Body:\n",
      "System Infotransformersversion: 4.33.2Platform: Linux-4.15.0-76-generic-x86_64-with-glibc2.27Python version: 3.10.14Huggingface_hub version: 0.21.4Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config:    not foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@amyeroberts@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI run the official code in my machine, but get a invalid ouput.importtorchfromtransformersimportIdeficsForVisionText2Text,AutoProcessordevice=\"cuda\"iftorch.cuda.is_available()else\"cpu\"checkpoint=\"HuggingFaceM4/idefics-9b\"model=IdeficsForVisionText2Text.from_pretrained(checkpoint,torch_dtype=torch.bfloat16).to(device)processor=AutoProcessor.from_pretrained(checkpoint)# We feed to the model an arbitrary sequence of text strings and images. Images can be either URLs or PIL Images.prompts=[\n",
      "    [\"https://upload.wikimedia.org/wikipedia/commons/8/86/Id%C3%A9fix.JPG\",\"In this picture from Asterix and Obelix, we can see\"],\n",
      "]# --batched modeinputs=processor(prompts,return_tensors=\"pt\").to(device)# --single sample mode# inputs = processor(prompts[0], return_tensors=\"pt\").to(device)# Generation argsbad_words_ids=processor.tokenizer([\"<image>\",\"<fake_token_around_image>\"],add_special_tokens=False).input_idsgenerated_ids=model.generate(**inputs,bad_words_ids=bad_words_ids,max_length=100)generated_text=processor.batch_decode(generated_ids,skip_special_tokens=True)fori,tinenumerate(generated_text):print(f\"{i}:\\n{t}\\n\")output is below:(['In this picture from Asterix and Obelix, we can see'],\n",
      " tensor([[    1, 32000, 32001, 32000,   512,   445,  7623,   515,   319,  2475,\n",
      "            861,   322,  4250,   295,   861, 29892,   591,   508,  1074,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
      "        device='cuda:6'))I try to do forward and find the logits are all NAN.But, when I upgrade my transformers package to 4.38.2 (Only upgrade the transformers). The result will be correct.Expected behaviorDifferent version of Transformers should generate the same output.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_608.txt:\n",
      "Title: [2023-12-04 11:52:08,378] [INFO] [autotuner.py:1110:run_after_tuning] No optimal DeepSpeed configuration found by autotuning.\n",
      "URL: https://github.com/huggingface/transformers/issues/27830\n",
      "Body:\n",
      "System Infodocker image:huggingface/transformers-pytorch-deepspeed-latest-gpu:latesttransformersversion: 4.36.0.dev0Platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.29Python version: 3.8.10Huggingface_hub version: 0.19.4Safetensors version: 0.4.1Accelerate version: 0.25.0.dev0Accelerate config:    not foundPyTorch version (GPU?): 2.1.0+cu118 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: rtx4060ti 16gUsing distributed or parallel set-up in script?: noWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionrun:deepspeed  --autotuning run \\\n",
      "./script/run_classification.py \\\n",
      "--model_name_or_path ckip-joint/bloom-1b1-zh \\\n",
      "--do_train \\\n",
      "--do_eval \\\n",
      "--output_dir ./bloom \\\n",
      "--train_file ./data/train.csv \\\n",
      "--validation_file ./data/test.csv \\\n",
      "--text_column_names sentence \\\n",
      "--label_column_name label \\\n",
      "--overwrite_output_dir \\\n",
      "--fp16 \\\n",
      "--torch_compile \\\n",
      "--deepspeed cfg/auto.jsoncfg/auto.json:{\n",
      "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
      "    \"autotuning\": {\n",
      "      \"enabled\": true,\n",
      "      \"fast\": false\n",
      "    }\n",
      "}the error:[2023-12-04 11:51:42,325] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-12-04 11:51:43,363] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-12-04 11:51:43,363] [INFO] [autotuner.py:71:__init__] Created autotuning experiments directory: autotuning_exps\n",
      "[2023-12-04 11:51:43,364] [INFO] [autotuner.py:84:__init__] Created autotuning results directory: autotuning_exps\n",
      "[2023-12-04 11:51:43,364] [INFO] [autotuner.py:200:_get_resource_manager] active_resources = OrderedDict([('localhost', [0])])\n",
      "[2023-12-04 11:51:43,364] [INFO] [runner.py:362:run_autotuning] [Start] Running autotuning\n",
      "[2023-12-04 11:51:43,364] [INFO] [autotuner.py:669:model_info_profile_run] Starting model info profile run.\n",
      "  0%|                                                                                                                                             | 0/1 [00:00<?, ?it/s][2023-12-04 11:51:43,366] [INFO] [scheduler.py:344:run_experiment] Scheduler wrote ds_config to autotuning_results/profile_model_info/ds_config.json, /workspaces/hf/autotuning_results/profile_model_info/ds_config.json\n",
      "[2023-12-04 11:51:43,367] [INFO] [scheduler.py:351:run_experiment] Scheduler wrote exp to autotuning_results/profile_model_info/exp.json, /workspaces/hf/autotuning_results/profile_model_info/exp.json\n",
      "[2023-12-04 11:51:43,367] [INFO] [scheduler.py:378:run_experiment] Launching exp_id = 0, exp_name = profile_model_info, with resource = localhost:0, and ds_config = /workspaces/hf/autotuning_results/profile_model_info/ds_config.json\n",
      "localhost: ssh: connect to host localhost port 22: Cannot assign requested address\n",
      "pdsh@b97c1584d47d: localhost: ssh exited with exit code 255\n",
      "[2023-12-04 11:51:59,057] [INFO] [scheduler.py:430:clean_up] Done cleaning up exp_id = 0 on the following workers: localhost\n",
      "[2023-12-04 11:51:59,057] [INFO] [scheduler.py:393:run_experiment] Done running exp_id = 0, exp_name = profile_model_info, with resource = localhost:0\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:25<00:00, 25.01s/it]\n",
      "[2023-12-04 11:52:08,378] [ERROR] [autotuner.py:699:model_info_profile_run] The model is not runnable with DeepSpeed with error = (\n",
      "\n",
      "[2023-12-04 11:52:08,378] [INFO] [runner.py:367:run_autotuning] [End] Running autotuning\n",
      "[2023-12-04 11:52:08,378] [INFO] [autotuner.py:1110:run_after_tuning] No optimal DeepSpeed configuration found by autotuning.Expected behaviortrain successfully\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_152.txt:\n",
      "Title: Llama3 Tokenizer Decode Removing Space Character\n",
      "URL: https://github.com/huggingface/transformers/issues/32575\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: macOS-14.6.1-arm64-arm-64bitPython version: 3.12.3Huggingface_hub version: 0.24.2Safetensors version: 0.4.3Accelerate version: 0.32.1Accelerate config:    not foundPyTorch version (GPU?): 2.4.0 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction>>>importtransformers>>>tok=transformers.AutoTokenizer.from_pretrained('baseten/Meta-Llama-3-tokenizer')>>>tok.decode([1232,364])\"': '\">>>tok.decode([364,1874])\"'search\">>>tok.decode([1232,364,1874])\"':'search\"Expected behaviorOutput should be': 'search; the space between the colon and quote character should be kept\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_146.txt:\n",
      "Title: Significant Performance Discrepancy Between Single-GPU and Multi-GPU Training with BERT\n",
      "URL: https://github.com/huggingface/transformers/issues/32614\n",
      "Body:\n",
      "System Infotransformers>=4.43.2accelerate==0.33.0GPUs: 2 x A100Who can help?@ArthurZucker@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI am currently using the transformers library (version >= 4.43.2) for training BERT models. I have observed a significant performance discrepancy when training with a single GPU compared to training with multiple GPUs. Specifically, the performance difference can reach up to 10 percentage points.Additionally, I have noticed that this issue persists in lower versions of the library, such as 4.42.1.Expected behaviorSame performance between single gpu and multi-gpu.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_22.txt:\n",
      "Title: The forced adoption of low_cpu_mem_usage behavior ruins models that require absolute position embedding.\n",
      "URL: https://github.com/huggingface/transformers/issues/33326\n",
      "Body:\n",
      "The loading of Hugging Face models adheres to the following logic: models using device_map must forcibly enable low_cpu_mem_usage. (Or a user might have manually enabled low_cpu_mem_usage in from_pretrained without realizing its consequence)transformers/src/transformers/modeling_utils.pyLines 3321 to 3325\n",
      "      in47b0964ifdevice_mapisnotNone:iflow_cpu_mem_usageisNone:low_cpu_mem_usage=Trueelifnotlow_cpu_mem_usage:raiseValueError(\"Passing along a `device_map` requires `low_cpu_mem_usage=True`\")transformers/src/transformers/modeling_utils.pyLines 3837 to 3838\n",
      "      in47b0964eliflow_cpu_mem_usage:init_contexts.append(init_empty_weights())This behavior harms some models that require absolute positional encoding, such asthe 'fsmt' model（likewmt19 winner model） which bridgesFairseqand Hugging Face.Because tensors created on the meta device are all meaningless empty tensors, this renders the following initialization code completely ineffective.transformers/src/transformers/models/fsmt/modeling_fsmt.pyLines 1343 to 1360\n",
      "      in47b0964defget_embedding(num_embeddings,embedding_dim,padding_idx):\"\"\"Build sinusoidal embeddings.This matches the implementation in tensor2tensor, but differs slightly from the description in Section 3.5 of\"Attention Is All You Need\".\"\"\"half_dim=embedding_dim//2emb=math.log(10000)/(half_dim-1)emb=torch.exp(torch.arange(half_dim,dtype=torch.int64).float()*-emb)emb=torch.arange(num_embeddings,dtype=torch.int64).float().unsqueeze(1)*emb.unsqueeze(0)emb=torch.cat([torch.sin(emb),torch.cos(emb)],dim=1).view(num_embeddings,-1)ifembedding_dim%2==1:# zero pademb=torch.cat([emb,torch.zeros(num_embeddings,1)],dim=1)ifpadding_idxisnotNone:emb[padding_idx, :]=0returnembInterestingly, this behavior does not affect embed_tokens or other weights, as they are overwritten by the state_dict.transformers/src/transformers/modeling_utils.pyLine 818\n",
      "      in47b0964def_load_state_dict_into_meta_model(So far, a optional approach is to include the weight matrix for embed_positions in the checkpoint converted from Fairseq, even though it can be calculated by rule. Given that this parameter count is negligible for today's large models, I believe it should have no adverse effects.We just need to change the following one line.transformers/src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.pyLine 250\n",
      "      inc6d2848model_new.load_state_dict(model_state_dict,strict=False)If needed, I can make a PR to solve this.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_393.txt:\n",
      "Title: Significant performance degradation with multi-GPU training on newer torch/transformers\n",
      "URL: https://github.com/huggingface/transformers/issues/30840\n",
      "Body:\n",
      "System Info#Env 1-`Accelerate`version: 0.30.1\n",
      "- Platform: Linux-5.15.0-1058-aws-x86_64-with-glibc2.31\n",
      "-`accelerate`bash location: /home/ubuntu/miniconda3/envs/train/bin/accelerate\n",
      "- Python version: 3.10.14\n",
      "- Numpy version: 1.26.4\n",
      "- PyTorch version (GPU?): 2.3.0+cu121 (True)\n",
      "- Transformers version: 4.40.2\n",
      "- PyTorch XPU available: False\n",
      "- PyTorch NPU available: False\n",
      "- PyTorch MLU available: False\n",
      "- System RAM: 186.70 GB\n",
      "- GPU type: NVIDIA A10G\n",
      "-`Accelerate`default config:\n",
      "        Not found#Env 2-`Accelerate`version: 0.20.3\n",
      "- Platform: Linux-5.15.0-1058-aws-x86_64-with-glibc2.31\n",
      "- Python version: 3.10.14\n",
      "- Numpy version: 1.26.4\n",
      "- PyTorch version (GPU?): 2.0.1+cu117 (True)\n",
      "- Transformers version: 4.30.2\n",
      "- PyTorch XPU available: False\n",
      "- System RAM: 186.70 GB\n",
      "- GPU type: NVIDIA A10G\n",
      "-`Accelerate`default config:\n",
      "        Not foundInformationThe official example scriptsMy own modified scriptsTasksOne of the scripts in the examples/ folder of Accelerate or an officially supportedno_trainerscript in theexamplesfolder of thetransformersrepo (such asrun_no_trainer_glue.py)My own task or dataset (give details below)ReproductionI am using ag5.12xlargeEC2 instance for this test but I observed this issue on other machines as well. This is just a minimum example to demonstrate the issue. In my actual usage, the degradation is even worse.Createenv1and install:pip install transformers torch accelerate.Createenv2and install:pip install transformers==4.30.2 torch==2.0.1 accelerate==0.20.3.Run the following script usingtorchrun --nproc-per-node=4 test.py.fromtypingimportIteratorimporttorchfromtransformersimportT5ForConditionalGeneration,Trainer,TrainingArgumentsfromtorch.utils.dataimportIterableDatasetclassDummyDataset(IterableDataset):def__iter__(self)->Iterator:whileTrue:yield{\"input_ids\":torch.randint(4000,size=(512,)),\"labels\":torch.randint(4000,size=(64,)),\n",
      "            }if__name__==\"__main__\":model=T5ForConditionalGeneration.from_pretrained(\"google/t5-efficient-small\")dataset=DummyDataset()training_args=TrainingArguments(output_dir=\"./output/\",max_steps=1000_000,per_device_train_batch_size=16,\n",
      "    )trainer=Trainer(model=model,train_dataset=dataset,args=training_args)trainer.train()ObservationsOnenv1GPU0 utilization keeps fluctuating and the estimated training time is shown as ~82hrs.Onenv2all GPUs have utilization maxed out and the estimated training time is shown as ~66hrs.Expected behaviorBoth environments should have similar training time.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_387.txt:\n",
      "Title: Title: CUDA RuntimeError: Unspecified Launch Failure during Training\n",
      "URL: https://github.com/huggingface/transformers/issues/30913\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.0Platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.35Python version: 3.10.6Huggingface_hub version: 0.23.0Safetensors version: 0.4.3Accelerate version: 0.27.2Accelerate config:    not foundPyTorch version (GPU?): 2.1.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@ArthurZucker@younesbelkada@muellerzrWhy does this error occur when passing a custom device_map? The map I wrote only differs from the auto-generated map in device order. Why does this cause an error? Does the device order affect the execution results?InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionimport torchfrom torch import nnfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, LlamaForCausalLMfrom transformers import DataCollatorForLanguageModeling, DataCollatorWithPaddingfrom transformers.utils.fx import symbolic_traceimport argparseimport numpy as npfrom datasets import load_metric, load_datasetdef compute_metrics(eval_preds):metric = load_metric(\"glue\", \"mrpc\")logits, labels = eval_predspredictions = np.argmax(logits, axis=-1)return metric.compute(predictions=predictions, references=labels)def tokenize_function(example):return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)ifname== \"main\":parser = argparse.ArgumentParser()parser.add_argument('--gpus', type=int, help='the number of gpus', default=8)parser.add_argument('--modelName', type=str, help=\"the name of model\", default='Llama2')parser.add_argument('--bs', type=int, help=\"the name of bs\", default=4)args = parser.parse_args()\n",
      "\n",
      "# Step 1: Define the model\n",
      "tokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Atom-7B-Chat')\n",
      "if tokenizer.pad_token is None:\n",
      "    tokenizer.pad_token = tokenizer.eos_token\n",
      "\n",
      "device_map = {\n",
      "    'model.embed_tokens': 6,\n",
      "    'model.layers.0': 6,\n",
      "    'model.layers.1': 4,\n",
      "    'model.layers.2': 1,\n",
      "    'model.layers.3': 1,\n",
      "    'model.layers.4': 1,\n",
      "    'model.layers.5': 0,\n",
      "    'model.layers.6': 0,\n",
      "    'model.layers.7': 0,\n",
      "    'model.layers.8': 0,\n",
      "    'model.layers.9': 0,\n",
      "    'model.layers.10': 6,\n",
      "    'model.layers.11': 5,\n",
      "    'model.layers.12': 5,\n",
      "    'model.layers.13': 5,\n",
      "    'model.layers.14': 5,\n",
      "    'model.layers.15': 5,\n",
      "    'model.layers.16': 4,\n",
      "    'model.layers.17': 4,\n",
      "    'model.layers.18': 4,\n",
      "    'model.layers.19': 4,\n",
      "    'model.layers.20': 3,\n",
      "    'model.layers.21': 3,\n",
      "    'model.layers.22': 3,\n",
      "    'model.layers.23': 3,\n",
      "    'model.layers.24': 3,\n",
      "    'model.layers.25': 2,\n",
      "    'model.layers.26': 2,\n",
      "    'model.layers.27': 2,\n",
      "    'model.layers.28': 2,\n",
      "    'model.layers.29': 2,\n",
      "    'model.layers.30': 1,\n",
      "    'model.layers.31': 1,\n",
      "    \"model.norm.weight\": 1,\n",
      "    \"lm_head\": 6,\n",
      "}\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained('FlagAlpha/Atom-7B-Chat', device_map=device_map, num_labels=2)\n",
      "\n",
      "print(model)\n",
      "print(model.hf_device_map)\n",
      "\n",
      "print(\"gpt start train\")\n",
      "\n",
      "# Step 4: Load the dataset\n",
      "data_files = {\n",
      "    'train': '/mnt/glue_mrpc/train.jsonl',\n",
      "    'test': '/mnt/glue_mrpc/test.jsonl',\n",
      "    'validation': '/mnt/glue_mrpc/validation.jsonl'\n",
      "}\n",
      "raw_datasets = load_dataset('json', data_files=data_files)\n",
      "\n",
      "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
      "tokenized_datasets = tokenized_datasets.rename_column(\"label\", 'labels')\n",
      "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
      "\n",
      "# Step 5: Train the model\n",
      "training_args = TrainingArguments(\n",
      "    output_dir='./results',\n",
      "    num_train_epochs=5,\n",
      "    per_device_train_batch_size=args.bs,\n",
      ")\n",
      "\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=tokenized_datasets[\"train\"],\n",
      "    eval_dataset=tokenized_datasets[\"validation\"],\n",
      "    data_collator=data_collator,\n",
      "    tokenizer=tokenizer,\n",
      "    compute_metrics=compute_metrics,\n",
      ")\n",
      "\n",
      "print('start train')\n",
      "trainer.train()Expected behaviorI want to know if the device order in the device_map affects the results.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_378.txt:\n",
      "Title: Add option to only install AutoTokenizer for production environment\n",
      "URL: https://github.com/huggingface/transformers/issues/31043\n",
      "Body:\n",
      "Feature requestAdd option to only install AutoTokenizer for production environment, so we can minimize the dependencies and footprint in a production system.MotivationWhen we deploy LLM models to a Triton inference server, we often convert the model to a optimized engine for a specific backend (ONNX, TensorRT-LLM etc.), these optimized engine does not require the transformers library anymore.However, we are still required to install the full transformers library, as we need the AutoTokenizer to handle pre/post processing steps to handle tokenization.Your contributionNot sure\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_350.txt:\n",
      "Title: convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py works for data2vec 1.0 checkpoint but not data2vec 2.0\n",
      "URL: https://github.com/huggingface/transformers/issues/31346\n",
      "Body:\n",
      "System Infotransformers version: 4.41Platform: Red Hat Enterprise Linux 8.4 (Ootpa)Python version: 3.10.14PyTorch version  2.3+cu118Who can help?@sanchit-gandhi@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRunconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.pyAttributeError: 'ModuleList' object has no attribute 'mlp'Expected behaviorExpected behavior is that model weights must be loaded via this function: recursively_load_weights(model, hf_wav2vec, not is_finetuned)but error is:\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_344.txt:\n",
      "Title: transformers unable to run whisper on MPS from version 4.40.0 onwards\n",
      "URL: https://github.com/huggingface/transformers/issues/31408\n",
      "Body:\n",
      "System Infosetup with crashtransformersversion: 4.41.2Platform: macOS-14.5-arm64-arm-64bitPython version: 3.12.3Huggingface_hub version: 0.23.3Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.0 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yes mpsUsing distributed or parallel set-up in script?: noworking setuptransformersversion: 4.39.0Platform: macOS-14.5-arm64-arm-64bitPython version: 3.12.3Huggingface_hub version: 0.23.3Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.0 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yes mpsUsing distributed or parallel set-up in script?: noWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionconda install pytorch transformers accelerate to use their latest releases. (the same issue arrises when using pip)run:importtorchfromtransformersimportAutoModelForSpeechSeq2Seq,AutoProcessor,pipelinedevice=\"mps\"torch_dtype=torch.float32model_id=\"openai/whisper-tiny\"model=AutoModelForSpeechSeq2Seq.from_pretrained(model_id,torch_dtype=torch_dtype,low_cpu_mem_usage=True,use_safetensors=True,\n",
      ")model.to(device)processor=AutoProcessor.from_pretrained(model_id)pipe=pipeline(\"automatic-speech-recognition\",model=model,tokenizer=processor.tokenizer,feature_extractor=processor.feature_extractor,max_new_tokens=128,chunk_length_s=30,batch_size=16,return_timestamps=True,torch_dtype=torch_dtype,device=device,\n",
      ")result=pipe(\"some_audio.wav\",generate_kwargs={\"task\":\"transcribe\"})# same issue arises when using mp3 fileprint(result[\"text\"])results in the following crash:Traceback (most recent call last):\n",
      "  File \"/Users/hugo/programming/music_genre_classification/src/bug.py\", line 32, in <module>\n",
      "    result = pipe(\"../../whisper/thomas.mp3\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 285, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 1235, in __call__\n",
      "    return next(\n",
      "           ^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py\", line 269, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 1150, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 508, in _forward\n",
      "    tokens = self.model.generate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py\", line 578, in generate\n",
      "    outputs = super().generate(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/generation/utils.py\", line 1758, in generate\n",
      "    result = self._sample(\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2410, in _sample\n",
      "    next_token_scores = logits_processor(input_ids, next_token_logits)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/generation/logits_process.py\", line 98, in __call__\n",
      "    scores = processor(input_ids, scores)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hugo/programming/music_genre_classification/env/lib/python3.12/site-packages/transformers/generation/logits_process.py\", line 1784, in __call__\n",
      "    suppress_token_mask = torch.isin(vocab_tensor, self.begin_suppress_tokens)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "NotImplementedError: The operator 'aten::isin.Tensor_Tensor_out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.Expected behaviorThe expected output is the following sentence, followed by a transcription of the soundfile.Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_422.txt:\n",
      "Title: (Willing to PR) Maketokenizer.padding_sidean argument instead of only being a field\n",
      "URL: https://github.com/huggingface/transformers/issues/30447\n",
      "Body:\n",
      "Feature requestHi thanks for the library! When using tokenizer, for example, for batch-generation with GPT2 (inhttps://discuss.huggingface.co/t/batch-generation-with-gpt2/1517), it seems that currently I have to do something like:tokenizer.padding_side = 'left'\n",
      "data = tokenizer(['sentence one', 'another'])\n",
      "tokenizer.padding_side = 'right'Therefore, it would be great to have:data = tokenizer(['sentence one', 'another'], padding_side = 'left')just like what we do today for many options likepadding_strategyetc.Motivation(see above)Your contributionYes, I am willing to PR\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_595.txt:\n",
      "Title: Exposegradient_as_bucket_viewas training argument forDDP\n",
      "URL: https://github.com/huggingface/transformers/issues/28079\n",
      "Body:\n",
      "Feature requestAs the title says, addgradient_as_bucket_viewas the training argument (default False)MotivationI have been experimenting with qlora fine-tuning LLMs on multiple A10 GPUs and I am leveraging DDP. I was going through the torch docs andhttps://pytorch.org/docs/2.1/generated/torch.nn.parallel.DistributedDataParallel.htmland it seems thegradient_as_bucket_viewargument can save a little bit of memory. It would be great to have it added as accelerate's DDP plugin already supports it.I am already experimenting with it to test it outclassHFTrainer(Trainer):def_wrap_model(self,model,training=True,dataloader=None):outputs=super()._wrap_model(model,training,dataloader)ifself.args.parallel_mode==ParallelMode.DISTRIBUTEDandself.accelerator.ddp_handler:self.accelerator.ddp_handler.gradient_as_bucket_view=TruereturnoutputsYour contributionLet me know, I can also work on a PR for this as the change is relatively small\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_542.txt:\n",
      "Title: BertTokenizer and BertTokenizerFast have different behavior when requested \"return_overflowing_tokens\"\n",
      "URL: https://github.com/huggingface/transformers/issues/28900\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.2Platform: Linux-6.5.5-arch1-1-x86_64-with-glibc2.38Python version: 3.11.5Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.2.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionfromtransformersimportBertTokenizer,BertTokenizerFast,BatchEncodingn_tok=BertTokenizer.from_pretrained(\"bert-base-uncased\")f_tok=BertTokenizerFast.from_pretrained(\"bert-base-uncased\")text=\"hello my name is nikola and i debug transformers now\"n_inputs:BatchEncoding=n_tok(text=text,add_special_tokens=True,max_length=6,truncation=True,padding='max_length',return_overflowing_tokens=True)o=n_inputs.get(\"overflowing_tokens\")print(f'Overflowing{o}')n_inputs['input_ids']f_inputs:BatchEncoding=f_tok(text=text,add_special_tokens=True,max_length=6,truncation=True,padding='max_length',return_overflowing_tokens=True)o=f_inputs.get(\"overflowing_tokens\")print(f'Overflowing{o}')f_inputs['input_ids']Expected behaviorFor then_inputs['input_ids']we get[101, 7592, 2026, 2171, 2003, 102], andfor thef_inputs['input_ids']we get[[101, 7592, 2026, 2171, 2003, 102], [101, 24794, 1998, 1045, 2139, 102], [101, 8569, 2290, 19081, 2085, 102]].Outputs should be the same.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_224.txt:\n",
      "Title: Incorrect scores returned in Whisper withnum_beams>1\n",
      "URL: https://github.com/huggingface/transformers/issues/32246\n",
      "Body:\n",
      "TL;DR: Scores corresponding to the wrong sequence in the batch/beam are returned.System Infotransformersversion: 4.43.2Platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.31Python version: 3.9.18Huggingface_hub version: 0.24.2Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config:    not foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noUsing GPU in script?: yesGPU type: NVIDIA RTX A6000Who can help?@sanchit-gandhi@ylacombe@patrickvonplatenInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionfromdatasetsimportAudio,load_datasetfromtransformersimportWhisperForConditionalGeneration,AutoProcessorimporttorchimportnumpyasnpmodel=WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\",torch_dtype=torch.float16)processor=AutoProcessor.from_pretrained(\"openai/whisper-tiny\")model.cuda()ds=load_dataset(\"distil-whisper/meanwhile\",\"default\")[\"test\"]ds=ds.cast_column(\"audio\",Audio(sampling_rate=16000))audio=ds[0][\"audio\"][\"array\"].astype(np.float32)inputs=processor(\n",
      "    [audio],return_tensors=\"pt\",truncation=False,padding=\"longest\",sampling_rate=16_000,\n",
      ")inputs=inputs.to(model.device,torch.float16)generation_output=model.generate(**inputs,language=\"en\",return_timestamps=True,return_segments=True,output_scores=True,num_beams=2,temperature=0.0,logprob_threshold=0.0,compression_ratio_threshold=2.4,no_speech_threshold=0.6,\n",
      ")# Print each token along with its log-probability and beam indexsegment=generation_output[\"segments\"][0][0]tokens=segment[\"result\"][\"sequences\"]scores=segment[\"result\"][\"scores\"]beam_indices=segment[\"result\"][\"beam_indices\"]logprobs=torch.as_tensor([s.log_softmax(-1)[t]fors,tinzip(scores,segment[\"tokens\"])])print(*[(processor.tokenizer.decode([t],decode_with_timestamps=True),s.item(),b.item())fors,t,binzip(logprobs,tokens,beam_indices)],sep=\"\\n\")('<|0.00|>',-0.061553955078125,0)\n",
      "(' Folks',-1.9033203125,0)\n",
      "(',',-0.406005859375,0)\n",
      "(' if',-0.038330078125,0)\n",
      "(' you',-0.0019512176513671875,0)\n",
      "(' watch',-0.1451416015625,0)\n",
      "(' the',-0.1986083984375,0)\n",
      "(' show',-0.0019512176513671875,0)\n",
      "(',',-0.28076171875,0)\n",
      "(' you',-0.291015625,0)\n",
      "(' know',-0.026031494140625,0)\n",
      "(' I',-1.05859375,0)\n",
      "(' spent',-10.125,1)\n",
      "(' a',-12.5625,1)\n",
      "(' lot',-8.1796875,1)\n",
      "(' of',-11.8359375,1)\n",
      "(' time',-5.4296875,1)\n",
      "(' right',-14.109375,1)\n",
      "(' over',-inf,1)\n",
      "(' there',-0.0307769775390625,0)\n",
      "('.',-0.236328125,0)\n",
      "('<|4.00|>',-3.7109375,0)# Now run a forward pass with the generated tokensinputs_forward={k:v[..., :3000].cuda()fork,vininputs.items()}inputs_forward[\"decoder_input_ids\"]=torch.cat(\n",
      "    [torch.as_tensor(processor.tokenizer.encode(\"<|startoftranscript|><|en|><|transcribe|>\",add_special_tokens=False)),tokens,\n",
      "    ],\n",
      ")[None].cuda()withtorch.inference_mode():output_forward=model(**inputs_forward)# Print each token along with its log-probabilityprint(*[(processor.tokenizer.decode([t],decode_with_timestamps=True),s[t].item())fors,tinzip(output_forward.logits.squeeze(0).log_softmax(-1),inputs_forward[\"decoder_input_ids\"].squeeze(0)[1:])],sep=\"\\n\")('<|en|>',-0.3857421875)\n",
      "('<|transcribe|>',-6.556510925292969e-06)\n",
      "('<|0.00|>',-0.1917724609375)\n",
      "(' Folks',-1.939453125)\n",
      "(',',-0.40966796875)\n",
      "(' if',-0.038818359375)\n",
      "(' you',-0.0020503997802734375)\n",
      "(' watch',-0.1458740234375)\n",
      "(' the',-0.204345703125)\n",
      "(' show',-0.00235748291015625)\n",
      "(',',-0.276123046875)\n",
      "(' you',-0.299560546875)\n",
      "(' know',-0.0259552001953125)\n",
      "(' I',-1.06640625)\n",
      "(' spent',-0.5)\n",
      "(' a',-0.0234832763671875)\n",
      "(' lot',-0.023773193359375)\n",
      "(' of',-0.0233001708984375)\n",
      "(' time',-0.01520538330078125)\n",
      "(' right',-1.12109375)\n",
      "(' over',-0.0208892822265625)\n",
      "(' there',-0.0306854248046875)\n",
      "('.',-0.2406005859375)\n",
      "('<|4.00|>',-3.798828125)\n",
      "...We can see that the scores returned bygenerate()are similar (though not identical) when the beam index is 0, but are much lower, and even-inf, when the beam index is 1, suggesting that we are getting scores from the wrong sequence in the beam. (I guess a small difference in the scores in the vicinity of timestamps might be explained by the logits processor, but the score of the generated token should clearly never be-inf.)The bug seems to behere in_postprocess_outputs. This works fine withnum_beams==1, but withnum_beams>1, the shape of the items inseek_outputs[\"scores\"]will be[num_beams * batch_size, vocab_size], while the code expects it to be[batch_size, vocab_size]. Therefore, instead of choosing the correct sequence in the beam/batch, this code will incorrectly combine scores from different sequences.Expected behaviorThe scores returned fromgenerate()should be the same as in the forward pass.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_556.txt:\n",
      "Title: Tokenizerencode/decodemethods are inconsistent, TypeError: argument 'ids': 'list' object cannot be interpreted as an integer\n",
      "URL: https://github.com/huggingface/transformers/issues/28635\n",
      "Body:\n",
      "System Infotransformersversion: 4.35.2Platform: Linux-6.5.0-14-generic-x86_64-with-glibc2.35Python version: 3.11.6Huggingface_hub version: 0.20.2Safetensors version: 0.4.1Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.0.1+cu117 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun the following code:fromtransformersimportAutoTokenizertext=\"test\"tokenizer=AutoTokenizer.from_pretrained(\"gpt2\")encoded=tokenizer.encode(text,return_tensors='pt')result_text=tokenizer.decode(encoded,skip_special_tokens=True)print(text)Will raise exception:Traceback (most recent call last):\n",
      "  File \"main.py\", line 8, in <module>\n",
      "    tokenizer.decode(encoded, skip_special_tokens=True)\n",
      "  File \"/home/scruel/mambaforge/envs/vae/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 3748, in decode\n",
      "    return self._decode(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/scruel/mambaforge/envs/vae/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 625, in _decode\n",
      "    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: argument 'ids': 'list' object cannot be interpreted as an integerExpected behaviorShould be able to print the original text\"test\", rather than raise an exception(TypeError).\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_967.txt:\n",
      "Title: ImportSPIECE_UNDERLINEfromfile_utilsinstead of WET definition\n",
      "URL: https://github.com/huggingface/transformers/issues/11732\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_783.txt:\n",
      "Title: Unable to import VGG16 model transformers\n",
      "URL: https://github.com/huggingface/transformers/issues/22567\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_797.txt:\n",
      "Title: Add SpikeGPT model\n",
      "URL: https://github.com/huggingface/transformers/issues/21875\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_998.txt:\n",
      "Title: Multilabel Sequence Classification in trainer\n",
      "URL: https://github.com/huggingface/transformers/issues/10232\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_754.txt:\n",
      "Title: BART-fusion\n",
      "URL: https://github.com/huggingface/transformers/issues/23781\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_741.txt:\n",
      "Title: Add training support for EnCodec\n",
      "URL: https://github.com/huggingface/transformers/issues/24295\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_796.txt:\n",
      "Title: Add X-Decoder Model\n",
      "URL: https://github.com/huggingface/transformers/issues/22003\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_782.txt:\n",
      "Title: Add MobileViT v2\n",
      "URL: https://github.com/huggingface/transformers/issues/22570\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_972.txt:\n",
      "Title: [resume optimization] skip loading pretrained weights on resume\n",
      "URL: https://github.com/huggingface/transformers/issues/11465\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_966.txt:\n",
      "Title: Request addition of 'GPT2ForwardBackward' models\n",
      "URL: https://github.com/huggingface/transformers/issues/11847\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_231.txt:\n",
      "Title: get error when running the chatglm3:  'GenerationConfig' object has no attribute '_eos_token_tensor'\n",
      "URL: https://github.com/huggingface/transformers/issues/32207\n",
      "Body:\n",
      "System Infoplatform == ubuntu 22.04transformers == 4.32.2python == 3.10.12Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductiongit clonehttps://github.com/THUDM/ChatGLM3cd ChatGLM3pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-urlhttps://download.pytorch.org/whl/cu118pip install -r requirements.txtgit clonehttps://huggingface.co/THUDM/chatglm3-6bcd basic_demopython3 cli_demo.pyExpected behaviorI hope to run the command : python3 cli_demo.py successfully without any error\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_543.txt:\n",
      "Title: Returning history prompt from BarkModel.generate()\n",
      "URL: https://github.com/huggingface/transformers/issues/28890\n",
      "Body:\n",
      "Feature requestHi,I have noticed that the original implementation of Bark (https://github.com/suno-ai/bark) has added a feature where one can get the history_prompt for the audio being currently generated using the parameter  output_full.history_prompt, out_arr = generate_audio(text_prompt, output_full=True)where history_prompt is a dict object with semantic_prompt, coarse_prompt, and fine_prompt as its keys.But the generate method of the  huggingface version of Bark (BarkModel) doesn't support this parameter. I tried to modify the code by creating a dict of these under the generate method but the prompts in the output prompt don't meet the criteria of a valid history_prompt to be used next time because of the mismatch in ndarray.Even the ndarray shape is also different for semantic, coarse, and fine prompts are different in the original implementation and the HuggingFace implementation.Can you please help me in fixing it?MotivationI want to generate a continous long-form audio for an audiobook for a better experience. I believe this will help in helping the Suno/Bark decide the tone based on the last sentence which can not be achieved using it at a sentence level  based on a single fixed history_prompt.Your contributionI need to go through and understand why there is a difference in the shape of different prompts. If that's achieved, I can contribute with a PR.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_225.txt:\n",
      "Title: phi3 model is not running in cpu\n",
      "URL: https://github.com/huggingface/transformers/issues/32243\n",
      "Body:\n",
      "microsoft/Phi-3-small-128k-instruct is not running on cpu  (ICELAKE or Graviton)the script which i was using :Error I got for Graviton:-Error for ICELAKE :-microsoft/Phi-3-mini-128k-instruct and other mini models are running on cpu but showing warning that You are not running the flash-attention implementation, expect numerical differences.but the implementation of Flash attention for cpu is available in pytorch aten native is there any flag I need to enable in order to use the flash attention for cpu.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_219.txt:\n",
      "Title: Finish short form / long from generation integration in Whisper\n",
      "URL: https://github.com/huggingface/transformers/issues/32263\n",
      "Body:\n",
      "Feature requestUnify the output format to Whisper Generate method for short form/long form generation.MotivationIn PR#30984, short-form and long-form generation in Whisper were unified so that both benefit from generation with fallback.However, the output to the generate method's format still varies depending on whether we're doing short form or long form generation, as we can see in thisline.Forshort form generationthe output format can be either a torch tensor containing the sequence of token ids or an instance ofModelOutputwith additional information (attention masks, hidden states, ...) ifreturn_dict_in_generateis set to True (we can now also usereturn_segmentswith short form generation).Forlong form generationthe output is either a torch tensor with the sequence of token ids, or a dict containing thesequencesof token ids and a list of allsegmentsifreturn_segmentsis set to True. Note that if bothreturn_dict_in_generateandreturn_segmentsare set to true, the additional information (attention masks, hidden states) will be contained insegments. However, at the moment we can't get an instance ofModelOutputas output with long form generation.Should we work on this ?Ideally, we should also unify the output format for the Whispergeneratemethod so that users don't have to distinguish between short and long form audio.  They should only have to specify wether they want to perform sequential generation (non chunked) or parallel generation (chunked) with the pipeline.The aim of PR#30984was to implement all the modifications to allow generation with fallback for short form audios without breakingBackward Compatibilityon main. If we further unify the output format, we would break backward compatibility and have to adapt several tests.cc@sanchit-gandhi@ArthurZuckerDo you think we should complete the unification of Whisper Generation by unifying the output format?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_580.txt:\n",
      "Title: Add Mixture of Tokens model\n",
      "URL: https://github.com/huggingface/transformers/issues/28285\n",
      "Body:\n",
      "Model descriptionMixture of Tokens is a new architecture / technique proposed inMixture of Tokens: Efficient LLMs through Cross-Example Aggregationand accompanyingblogby Szymon Antoniak, Sebastian Jaszczur et al.It builds on expert-choice MoE, aggregating across sequences in a batch rather than positions in a sequence, and doing so in a continuous fashion. This full differentiability is its main advantage, bringing training stability and even expert utilization.In collaboration with the authors, we (me + 3 others) would like to add a PyTorch implementation matching the architecture from the paper to HF transformers and later publish corresponding checkpoints. We believe this will make it significantly easier for the community to experiment with this approach, as the original implementation is quite dense and contained in an active research repo.We believe a good approach is to start from the GPT2 HF model. We will have the assistance of the original authors for making sure the details match.Please advise:If you have any general suggestions at this stageWhat kinds of tests you would like to see in the finalized implementation for this case, where the exact snapshot corresponding to the paper's implementation and the checkpoints were not  previously published.If you have general suggestions regarding contributing methods that are potentially applicable to multiple base models  (like MoE and MoT).As we understand, the next step is for us to create a template withhttps://github.com/huggingface/transformers/tree/main/templates/adding_a_new_modeland get coding.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationhttps://github.com/llm-random/llm-randomhttps://github.com/sebastianjaszczurhttps://llm-random.github.io/posts/mixture_of_tokens/\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_437.txt:\n",
      "Title: modeling_t5 incompatible with multiprocessing\n",
      "URL: https://github.com/huggingface/transformers/issues/30280\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.0.dev0Platform: Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.17Python version: 3.10.13Huggingface_hub version: 0.21.4Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config:    - compute_environment: LOCAL_MACHINE- distributed_type: DEEPSPEED- mixed_precision: bf16- use_cpu: False- debug: False- num_processes: 8- machine_rank: 0- num_machines: 1- rdzv_backend: static- same_network: True- main_training_function: main- deepspeed_config: {'gradient_accumulation_steps': 16, 'zero3_init_flag': False, 'zero_stage': 0}- downcast_bf16: no- tpu_use_cluster: False- tpu_use_sudo: False- tpu_env: []PyTorch version (GPU?): 2.2.1 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?Hi,@ArthurZuckerand@younesbelkada. I'm trying to split a dataset automatically to multi gpu (a bit like data parallel) forinference. But strange things happen when using t5 model in hf while other models work correctly(i.e. bart), so I guess here exist some problem related to t5 implementation, would you like help checking it out?    ：）Although it has been mentioned online that the error below may be related to OOM, I am certain that it is not. The following code only allows rank0 to obtain normal output, while other ranks will report the following error.Traceback (most recent call last):\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/multiprocessing/process.py\", line 314,in_bootstrapself.run()\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/multiprocessing/process.py\", line 108,inrun\n",
      "    self._target(*self._args,**self._kwargs)\n",
      "  File\"/data/ruanjh/NiuInference/NiuInference.py\", line 97,inget_pred\n",
      "    output = model.generate(\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115,indecorate_contextreturnfunc(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1388,ingenerate\n",
      "    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/generation/utils.py\", line 503,in_prepare_encoder_decoder_kwargs_for_generation\n",
      "    model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520,in_call_implreturnforward_call(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 1115,inforward\n",
      "    layer_outputs = layer_module(\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520,in_call_implreturnforward_call(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 695,inforward\n",
      "    self_attention_outputs = self.layer[0](\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520,in_call_implreturnforward_call(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 602,inforward\n",
      "    attention_output = self.SelfAttention(\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520,in_call_implreturnforward_call(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 521,inforward\n",
      "    query_states = shape(self.q(hidden_states))#(batch_size, n_heads, seq_length, dim_per_head)File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520,in_call_implreturnforward_call(*args,**kwargs)\n",
      "  File\"/data/ruanjh/miniconda3/envs/mamba/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 116,inforwardreturnF.linear(input, self.weight, self.bias)\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling`cublasCreate(handle)`InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe following code should be quite easy to reproduce. All you need to do is replace the model_dir in the main function with a specific model, such asGoogle/t5-v1_1-large, and make sure CUDA VISIBLE DEVICES >1 .importtorchfromtorchimportbfloat16importtorch.distributedasdistimporttorch.multiprocessingasmpfromtorch.utils.dataimportDataset,DataLoaderimportfunctoolsfromtransformersimportAutoTokenizer,DefaultDataCollator,GenerationConfig,PreTrainedModel,AutoModelForSeq2SeqLM,AutoModelForCausalLM,AutoConfig,DataCollatorWithPaddingimportloggingfromtransformers.models.auto.modeling_autoimportMODEL_FOR_CAUSAL_LM_MAPPING_NAMESfromtqdmimporttqdm# from accelerate import find_executable_batch_sizelogging.basicConfig(level=logging.INFO,format='%(asctime)s - %(levelname)s - %(message)s')logger=logging.getLogger(__name__)classDefaultDataset(Dataset):def__init__(self,data,tokenizer):self.data=tokenizer(data,return_tensors='pt',padding=True)def__getitem__(self,idx):return{'input_ids':self.data['input_ids'][idx]}def__len__(self):returnself.data['input_ids'].size(0)classNiuInference:def__init__(self,model_dir,data,dtype=bfloat16,dataset=None,data_collator=None,output_path='niuinference.out',auto_batch_size=True,batch_size=1,generation_config=None):self.model_dir=model_dirself.dtype=dtypeself.data=dataself.dataset=datasetself.data_collator=data_collatorself.output_path=output_pathself.batch_size=batch_sizeself.auto_batch_size=auto_batch_sizeself.generation_config=generation_configdef_load_model_and_tokenizer(self,device):print(self.dtype)config=AutoConfig.from_pretrained(self.model_dir)ifconfig.model_typeinMODEL_FOR_CAUSAL_LM_MAPPING_NAMES:model=AutoModelForCausalLM.from_pretrained(self.model_dir,torch_dtype=self.dtype)else:model=AutoModelForSeq2SeqLM.from_pretrained(self.model_dir,torch_dtype=self.dtype)model.to(device)tokenizer=AutoTokenizer.from_pretrained(self.model_dir)returnmodel,tokenizer# @find_executable_batch_size(starting_batch_size=1)# def auto_get_pred(batch_size):defget_pred(self,rank,out_path,data,dict):batch_size=2try:device=torch.device(f'cuda:{rank}')model,tokenizer=self._load_model_and_tokenizer(device)ifself.datasetisnotNone:dataset=self.dataset(data=data,tokenizer=tokenizer)else:dataset=DefaultDataset(data=data,tokenizer=tokenizer)ifself.data_collatorisnotNone:collator=self.data_collator(tokenizer,model=model,padding=True)else:collator=DataCollatorWithPadding(tokenizer)dataloader=DataLoader(dataset,batch_size,collate_fn=collator,pin_memory=True,num_workers=0)result=[]forinputintqdm(dataloader):input.to(device)print(input)output=model.generate(input_ids=input['input_ids'],attention_mask=input['attention_mask'],num_beams=5,do_sample=False,temperature=1.0,max_new_tokens=512,\n",
      "                        )pred=tokenizer.batch_decode(output,skip_special_tokens=True)print(pred)result+=preddict[f'{rank}']=resultexceptExceptionase:print('error',device)raisedefsplit_list(self,lst,n):avg=len(lst)/float(n)return[lst[int(avg*i):int(avg*(i+1))]foriinrange(n)]defrun(self,):world_size=min(torch.cuda.device_count(),len(self.data))# corner case， data<available GPU numdata_subsets=self.split_list(self.data,world_size)print(data_subsets)processes=[]manager=mp.Manager()record_dict=manager.dict()forrankinrange(world_size):p=mp.Process(target=self.get_pred,args=(rank,self.output_path,data_subsets[rank],record_dict))p.start()processes.append(p)forpinprocesses:p.join()withopen(self.output_path,\"w\",encoding=\"utf-8\")asf:forrankinrange(world_size):forrinrecord_dict[f'{rank}']:f.write(r.replace('\\n','\\\\n')+'\\n')if__name__=='__main__':mp.set_start_method('spawn')i=NiuInference(model_dir=**replaceheretot5orbart**,data=['hello,how is your day','my wish is that you happy','from scratch',])i.run()Expected behaviort5 model can inference in multiprocessing.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_392.txt:\n",
      "Title: Cache problem while runing on multiple nodes with GPU\n",
      "URL: https://github.com/huggingface/transformers/issues/30859\n",
      "Body:\n",
      "System InfoHi,I am currently trying to use the script run_mlm_wwm.py to perform a continual pretrianing  on the Whole Word mazking task, on a Bert model, my problem occured when I am trying to use multpiles GPU, when the number of GPU force my server to use severals nodes then I get an error.I think there's a locking problem linked to the parallel file system when I go over several nodes, I would have to set a different cache for each process (for example by adding the global rank) or for each process on the same node but i don't succeed to do it so farIs there the error message i get :`Loading pytorch-gpu/py3/2.1.1Loading requirement: cuda/11.8.0 nccl/2.18.5-1-cuda cudnn/8.7.0.84-cudagcc/8.5.0 openmpi/4.1.5-cuda intel-mkl/2020.4 magma/2.7.1-cuda sox/14.4.2sparsehash/2.0.3 libjpeg-turbo/2.1.3 ffmpeg/4.4.4export OMP_NUM_THREADS=10OMP_NUM_THREADS=10export CUDA_LAUNCH_BLOCKING=1CUDA_LAUNCH_BLOCKING=1export NCCL_ASYNC_ERROR_HANDLING=1NCCL_ASYNC_ERROR_HANDLING=1export TRAIN_FILE=/textes_shuffle_txt/train_data_shuffle_2020a2022.txtTRAIN_FILE=/textes_shuffle_txt/train_data_shuffle_2020a2022.txtexport VALIDATION_FILE=/textes_shuffle_txt/test_data_shuffle_2020a2022.txtVALIDATION_FILE=/textes_shuffle_txt/test_data_shuffle_2020a2022.txtexport OUTPUT_DIR=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwmOUTPUT_DIR=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwmsrun -l python -u run_mlm_wwm.py --model_name_or_path my_path_to_files/camembert-base --train_file my_path_to_files/textes_shuffle_txt/train_data_shuffle_2020a2022.txt --validation_file my_path_to_files/textes_shuffle_txt/test_data_shuffle_2020a2022.txt --per_device_train_batch_size=96 --do_train --warmup_steps=10000 --overwrite_output_dir --max_seq_length=512 --logging_steps=500 --report_to=tensorboard --save_strategy=epoch --skip_memory_metrics=False --log_level=info --logging_first_step=True --learning_rate 1e-4 --num_train_epochs 6.0 --fp16 --output_dir /gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm --do_eval --pad_to_max_length True --preprocessing_num_workers 8 --ddp_timeout=600 --ddp_find_unused_parameters=Falsesrun: warning: can't honor --ntasks-per-node set to 8 which doesn't match the requested tasks 18 with the number of requested nodes 3. Ignoring --ntasks-per-node.0: comet_ml is installed butCOMET_API_KEYis not set.6: comet_ml is installed butCOMET_API_KEYis not set.7: comet_ml is installed butCOMET_API_KEYis not set.8: comet_ml is installed butCOMET_API_KEYis not set.9: comet_ml is installed butCOMET_API_KEYis not set.10: comet_ml is installed butCOMET_API_KEYis not set.11: comet_ml is installed butCOMET_API_KEYis not set.12: comet_ml is installed butCOMET_API_KEYis not set.13: comet_ml is installed butCOMET_API_KEYis not set.14: comet_ml is installed butCOMET_API_KEYis not set.15: comet_ml is installed butCOMET_API_KEYis not set.16: comet_ml is installed butCOMET_API_KEYis not set.17: comet_ml is installed butCOMET_API_KEYis not set.3: comet_ml is installed butCOMET_API_KEYis not set.4: comet_ml is installed butCOMET_API_KEYis not set.5: comet_ml is installed butCOMET_API_KEYis not set.1: comet_ml is installed butCOMET_API_KEYis not set.2: comet_ml is installed butCOMET_API_KEYis not set.2: 05/04/2024 06:14:12 - WARNING -main- Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True1: 05/04/2024 06:14:12 - WARNING -main- Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True3: 05/04/2024 06:14:12 - WARNING -main- Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True4: 05/04/2024 06:14:12 - WARNING -main- Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True5: 05/04/2024 06:14:12 - WARNING -main- Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True14: 05/04/2024 06:14:12 - WARNING -main- Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True15: 05/04/2024 06:14:12 - WARNING -main- Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True16: 05/04/2024 06:14:12 - WARNING -main- Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True12: 05/04/2024 06:14:12 - WARNING -main- Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True13: 05/04/2024 06:14:12 - WARNING -main- Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True17: 05/04/2024 06:14:12 - WARNING -main- Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True12: 05/04/2024 06:14:12 - INFO -main- Training/evaluation parameters TrainingArguments(12: _n_gpu=1,12: adafactor=False,12: adam_beta1=0.9,12: adam_beta2=0.999,12: adam_epsilon=1e-08,12: auto_find_batch_size=False,12: bf16=False,12: bf16_full_eval=False,12: data_seed=None,12: dataloader_drop_last=False,12: dataloader_num_workers=0,12: dataloader_pin_memory=True,12: ddp_backend=None,12: ddp_broadcast_buffers=None,12: ddp_bucket_cap_mb=None,12: ddp_find_unused_parameters=False,12: ddp_timeout=600,12: debug=[],12: deepspeed=None,12: disable_tqdm=False,12: dispatch_batches=None,12: do_eval=True,12: do_predict=False,12: do_train=True,12: eval_accumulation_steps=None,12: eval_delay=0,12: eval_steps=None,12: evaluation_strategy=IntervalStrategy.NO,12: fp16=True,12: fp16_backend=auto,12: fp16_full_eval=False,12: fp16_opt_level=O1,12: fsdp=[],12: fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},12: fsdp_min_num_params=0,12: fsdp_transformer_layer_cls_to_wrap=None,12: full_determinism=False,12: gradient_accumulation_steps=1,12: gradient_checkpointing=False,12: gradient_checkpointing_kwargs=None,12: greater_is_better=None,12: group_by_length=False,12: half_precision_backend=auto,12: hub_always_push=False,12: hub_model_id=None,12: hub_private_repo=False,12: hub_strategy=HubStrategy.EVERY_SAVE,12: hub_token=<HUB_TOKEN>,12: ignore_data_skip=False,12: include_inputs_for_metrics=False,12: include_tokens_per_second=False,12: jit_mode_eval=False,12: label_names=None,12: label_smoothing_factor=0.0,12: learning_rate=0.0001,12: length_column_name=length,12: load_best_model_at_end=False,12: local_rank=0,12: log_level=info,12: log_level_replica=warning,12: log_on_each_node=True,12: logging_dir=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm/runs/May04_06-14-12_jean-zay-iam07,12: logging_first_step=True,12: logging_nan_inf_filter=True,12: logging_steps=500,12: logging_strategy=IntervalStrategy.STEPS,12: lr_scheduler_type=SchedulerType.LINEAR,12: max_grad_norm=1.0,12: max_steps=-1,12: metric_for_best_model=None,12: mp_parameters=,12: neftune_noise_alpha=None,12: no_cuda=False,12: num_train_epochs=6.0,12: optim=OptimizerNames.ADAMW_TORCH,12: optim_args=None,12: output_dir=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm,12: overwrite_output_dir=True,12: past_index=-1,12: per_device_eval_batch_size=8,12: per_device_train_batch_size=96,12: prediction_loss_only=False,12: push_to_hub=False,12: push_to_hub_model_id=None,12: push_to_hub_organization=None,12: push_to_hub_token=<PUSH_TO_HUB_TOKEN>,12: ray_scope=last,12: remove_unused_columns=True,12: report_to=['tensorboard'],12: resume_from_checkpoint=None,12: run_name=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm,12: save_on_each_node=False,12: save_safetensors=True,12: save_steps=500,12: save_strategy=IntervalStrategy.EPOCH,12: save_total_limit=None,12: seed=42,12: skip_memory_metrics=False,12: split_batches=False,12: tf32=None,12: torch_compile=False,12: torch_compile_backend=None,12: torch_compile_mode=None,12: torchdynamo=None,12: tpu_metrics_debug=False,12: tpu_num_cores=None,12: use_cpu=False,12: use_ipex=False,12: use_legacy_prediction_loop=False,12: use_mps_device=False,12: warmup_ratio=0.0,12: warmup_steps=10000,12: weight_decay=0.0,12: )Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 21454.24it/s]Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 11259.88it/s]0: 05/04/2024 06:14:13 - WARNING -main- Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True0: 05/04/2024 06:14:13 - INFO -main- Training/evaluation parameters TrainingArguments(0: _n_gpu=1,0: adafactor=False,0: adam_beta1=0.9,0: adam_beta2=0.999,0: adam_epsilon=1e-08,0: auto_find_batch_size=False,0: bf16=False,0: bf16_full_eval=False,0: data_seed=None,0: dataloader_drop_last=False,0: dataloader_num_workers=0,0: dataloader_pin_memory=True,0: ddp_backend=None,0: ddp_broadcast_buffers=None,0: ddp_bucket_cap_mb=None,0: ddp_find_unused_parameters=False,0: ddp_timeout=600,0: debug=[],0: deepspeed=None,0: disable_tqdm=False,0: dispatch_batches=None,0: do_eval=True,0: do_predict=False,0: do_train=True,0: eval_accumulation_steps=None,0: eval_delay=0,0: eval_steps=None,0: evaluation_strategy=IntervalStrategy.NO,0: fp16=True,0: fp16_backend=auto,0: fp16_full_eval=False,0: fp16_opt_level=O1,0: fsdp=[],0: fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},0: fsdp_min_num_params=0,0: fsdp_transformer_layer_cls_to_wrap=None,0: full_determinism=False,0: gradient_accumulation_steps=1,0: gradient_checkpointing=False,0: gradient_checkpointing_kwargs=None,0: greater_is_better=None,0: group_by_length=False,0: half_precision_backend=auto,0: hub_always_push=False,0: hub_model_id=None,0: hub_private_repo=False,0: hub_strategy=HubStrategy.EVERY_SAVE,0: hub_token=<HUB_TOKEN>,0: ignore_data_skip=False,0: include_inputs_for_metrics=False,0: include_tokens_per_second=False,0: jit_mode_eval=False,0: label_names=None,0: label_smoothing_factor=0.0,0: learning_rate=0.0001,0: length_column_name=length,0: load_best_model_at_end=False,0: local_rank=0,0: log_level=info,0: log_level_replica=warning,0: log_on_each_node=True,0: logging_dir=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm/runs/May04_06-14-13_jean-zay-iam05,0: logging_first_step=True,0: logging_nan_inf_filter=True,0: logging_steps=500,0: logging_strategy=IntervalStrategy.STEPS,0: lr_scheduler_type=SchedulerType.LINEAR,0: max_grad_norm=1.0,0: max_steps=-1,0: metric_for_best_model=None,0: mp_parameters=,0: neftune_noise_alpha=None,0: no_cuda=False,0: num_train_epochs=6.0,0: optim=OptimizerNames.ADAMW_TORCH,0: optim_args=None,0: output_dir=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm,0: overwrite_output_dir=True,0: past_index=-1,0: per_device_eval_batch_size=8,0: per_device_train_batch_size=96,0: prediction_loss_only=False,0: push_to_hub=False,0: push_to_hub_model_id=None,0: push_to_hub_organization=None,0: push_to_hub_token=<PUSH_TO_HUB_TOKEN>,0: ray_scope=last,0: remove_unused_columns=True,0: report_to=['tensorboard'],0: resume_from_checkpoint=None,0: run_name=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm,0: save_on_each_node=False,0: save_safetensors=True,0: save_steps=500,0: save_strategy=IntervalStrategy.EPOCH,0: save_total_limit=None,0: seed=42,0: skip_memory_metrics=False,0: split_batches=False,0: tf32=None,0: torch_compile=False,0: torch_compile_backend=None,0: torch_compile_mode=None,0: torchdynamo=None,0: tpu_metrics_debug=False,0: tpu_num_cores=None,0: use_cpu=False,0: use_ipex=False,0: use_legacy_prediction_loop=False,0: use_mps_device=False,0: warmup_ratio=0.0,0: warmup_steps=10000,0: weight_decay=0.0,0: )7: 05/04/2024 06:14:13 - WARNING -main- Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True10: 05/04/2024 06:14:13 - WARNING -main- Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True9: 05/04/2024 06:14:13 - WARNING -main- Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True11: 05/04/2024 06:14:13 - WARNING -main- Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True6: 05/04/2024 06:14:13 - WARNING -main- Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True8: 05/04/2024 06:14:13 - WARNING -main- Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True6: 05/04/2024 06:14:13 - INFO -main- Training/evaluation parameters TrainingArguments(6: _n_gpu=1,6: adafactor=False,6: adam_beta1=0.9,6: adam_beta2=0.999,6: adam_epsilon=1e-08,6: auto_find_batch_size=False,6: bf16=False,6: bf16_full_eval=False,6: data_seed=None,6: dataloader_drop_last=False,6: dataloader_num_workers=0,6: dataloader_pin_memory=True,6: ddp_backend=None,6: ddp_broadcast_buffers=None,6: ddp_bucket_cap_mb=None,6: ddp_find_unused_parameters=False,6: ddp_timeout=600,6: debug=[],6: deepspeed=None,6: disable_tqdm=False,6: dispatch_batches=None,6: do_eval=True,6: do_predict=False,6: do_train=True,6: eval_accumulation_steps=None,6: eval_delay=0,6: eval_steps=None,6: evaluation_strategy=IntervalStrategy.NO,6: fp16=True,6: fp16_backend=auto,6: fp16_full_eval=False,6: fp16_opt_level=O1,6: fsdp=[],6: fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},6: fsdp_min_num_params=0,6: fsdp_transformer_layer_cls_to_wrap=None,6: full_determinism=False,6: gradient_accumulation_steps=1,6: gradient_checkpointing=False,6: gradient_checkpointing_kwargs=None,6: greater_is_better=None,6: group_by_length=False,6: half_precision_backend=auto,6: hub_always_push=False,6: hub_model_id=None,6: hub_private_repo=False,6: hub_strategy=HubStrategy.EVERY_SAVE,6: hub_token=<HUB_TOKEN>,6: ignore_data_skip=False,6: include_inputs_for_metrics=False,6: include_tokens_per_second=False,6: jit_mode_eval=False,6: label_names=None,6: label_smoothing_factor=0.0,6: learning_rate=0.0001,6: length_column_name=length,6: load_best_model_at_end=False,6: local_rank=0,6: log_level=info,6: log_level_replica=warning,6: log_on_each_node=True,6: logging_dir=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm/runs/May04_06-14-13_jean-zay-iam06,6: logging_first_step=True,6: logging_nan_inf_filter=True,6: logging_steps=500,6: logging_strategy=IntervalStrategy.STEPS,6: lr_scheduler_type=SchedulerType.LINEAR,6: max_grad_norm=1.0,6: max_steps=-1,6: metric_for_best_model=None,6: mp_parameters=,6: neftune_noise_alpha=None,6: no_cuda=False,6: num_train_epochs=6.0,6: optim=OptimizerNames.ADAMW_TORCH,6: optim_args=None,6: output_dir=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm,6: overwrite_output_dir=True,6: past_index=-1,6: per_device_eval_batch_size=8,6: per_device_train_batch_size=96,6: prediction_loss_only=False,6: push_to_hub=False,6: push_to_hub_model_id=None,6: push_to_hub_organization=None,6: push_to_hub_token=<PUSH_TO_HUB_TOKEN>,6: ray_scope=last,6: remove_unused_columns=True,6: report_to=['tensorboard'],6: resume_from_checkpoint=None,6: run_name=/gpfsscratch/rech/khy/uvb95lb/test-mlm-wwm,6: save_on_each_node=False,6: save_safetensors=True,6: save_steps=500,6: save_strategy=IntervalStrategy.EPOCH,6: save_total_limit=None,6: seed=42,6: skip_memory_metrics=False,6: split_batches=False,6: tf32=None,6: torch_compile=False,6: torch_compile_backend=None,6: torch_compile_mode=None,6: torchdynamo=None,6: tpu_metrics_debug=False,6: tpu_num_cores=None,6: use_cpu=False,6: use_ipex=False,6: use_legacy_prediction_loop=False,6: use_mps_device=False,6: warmup_ratio=0.0,6: warmup_steps=10000,6: weight_decay=0.0,6: )Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 21183.35it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00,  7.98it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00,  7.37it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 26.94it/s]Generating train split: 735691 examples [00:02, 368808.23 examples/s]Generating train split: 674152 examples [00:02, 277129.32 examples/s]Generating train split: 674152 examples [00:03, 247846.23 examples/s]Generating train split:1593705examples [00:04, 420185.94 examples/s]Generating train split: 1501884 examples [00:05, 283821.86 examples/s]Generating train split: 1410002 examples [00:05, 375326.74 examples/s]Generating train split: 2513358 examples [00:06, 437244.14 examples/s]Generating train split: 2421559 examples [00:07, 378112.29 examples/s]Generating train split: 2329470 examples [00:08, 326257.49 examples/s]Generating train split:3370011examples [00:08, 448849.21 examples/s]Generating train split: 3278022 examples [00:10, 358206.71 examples/s]Generating train split: 3155413 examples [00:10, 341364.95 examples/s]Generating train split:4290261examples [00:11, 447049.42 examples/s]Generating train split: 4198175 examples [00:12, 373207.24 examples/s]Generating train split: 4074904 examples [00:12, 353087.99 examples/s]Generating train split: 5147893 examples [00:13, 417013.34 examples/s]Generating train split:5056374examples [00:14, 376059.59 examples/s]Generating train split: 6067697 examples [00:15, 448228.75 examples/s]s/s]Generating train split: 4933690 examples [00:15, 421004.81 examples/s]Generating train split:6926173examples [00:17, 450815.07 examples/s]Generating train split:5975531examples [00:17, 383399.07 examples/s]/s]Generating train split: 5852750 examples [00:17, 365422.88 examples/s]s]Generating train split: 7783637 examples [00:19, 467585.48 examples/s]Generating train split: 6712504 examples [00:19, 422087.08 examples/s]Generating train split: 6835032 examples [00:19, 295981.16 examples/s]Generating train split: 8703429 examples [00:21, 438801.32 examples/s]Generating train split:7569889examples [00:22, 405311.56 examples/s]Generating train split: 7691972 examples [00:22, 384627.19 examples/s]Generating train split: 9561456 examples [00:23, 430769.79 examples/s]Generating train split: 8611880 examples [00:24, 368744.37 examples/s]Generating train split: 8488889 examples [00:24, 309105.36 examples/s]Generating train split: 10481374 examples [00:25, 434503.57 examples/s]Generating train split: 9469794 examples [00:26, 366982.35 examples/s]Generating train split: 9347416 examples [00:26, 439195.31 examples/s]Generating train split: 11340042 examples [00:27, 446102.44 examples/s]Generating train split: 12198580 examples [00:29, 434616.45 examples/s]Generating train split: 10389743 examples [00:29, 398051.80 examples/s]Generating train split: 10267411 examples [00:29, 368192.47 examples/s]Generating train split: 13056612 examples [00:31, 439929.51 examples/s]Generating train split: 11247710 examples [00:31, 398261.78 examples/s]Generating train split: 11125153 examples [00:31, 419988.52 examples/s]Generating train split: 13913907 examples [00:33, 458949.37 examples/s]Generating train split: 12106665 examples [00:33, 380881.35 examples/s]Generating train split: 11983873 examples [00:33, 358349.17 examples/s]Generating train split: 14833307 examples [00:35, 423504.66 examples/s]Generating train split: 12964439 examples [00:35, 400086.35 examples/s]Generating train split: 12841924 examples [00:36, 400414.04 examples/s]Generating train split: 15690649 examples [00:37, 438872.86 examples/s]Generating train split: 13822319 examples [00:37, 398832.34 examples/s]Generating train split: 13769754 examples [00:38, 425513.63 examples/s]Generating train split: 16548238 examples [00:39, 450448.68 examples/s]Generating train split: 14741800 examples [00:40, 390752.76 examples/s]Generating train split: 14619130 examples [00:40, 395537.82 examples/s]Generating train split: 17405436 examples [00:41, 442940.32 examples/s]Generating train split: 15598726 examples [00:42, 356492.27 examples/s]Generating train split: 15476523 examples [00:42, 369237.01 examples/s]Generating train split: 18323721 examples [00:43, 459883.94 examples/s]Generating train split: 16456225 examples [00:44, 403418.79 examples/s]Generating train split: 19181845 examples [00:45, 444280.63 examples/s]Generating train split: 16334014 examples [00:44, 431093.34 examples/s]Generating train split: 20038448 examples [00:47, 438563.69 examples/s]Generating train split: 17312794 examples [00:47, 402221.80 examples/s]/s]Generating train split: 20130863 examples [00:47, 425298.18 examples/s]Generating train split: 17190731 examples [00:47, 352356.97 examples/s]Generating train split: 18231895 examples [00:49, 438039.86 examples/s]Generating validation split: 795560 examples [00:01, 450074.53 examples/s]Generating train split: 18109337 examples [00:49, 387274.11 examples/s]Generating train split: 19090119 examples [00:51, 431144.93 examples/s]Generating validation split:1592633examples [00:03, 447538.35 examples/s]Generating train split: 18967698 examples [00:51, 400959.93 examples/s]Generating train split: 19946306 examples [00:53, 444335.85 examples/s]Generating validation split: 2450319 examples [00:05, 437344.97 examples/s]Generating train split: 20130863 examples [00:53, 376271.19 examples/s]14: Traceback (most recent call last):14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/builder.py\", line 902, in incomplete_dirGenerating train split: 19823979 examples [00:53, 427450.54 examples/s]14:     yield tmp_dir14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/builder.py\", line 948, in download_and_prepare14:     self._download_and_prepare(14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/builder.py\", line 1045, in _download_and_prepare14:     raise OSError(14: OSError: Cannot find data file.14: Original error:14: [Errno 2] No such file or directory: '/scrip_continual_pretraining/Cache_mlm/text/default-d0870639fca1403e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34.incomplete/text-train-00000-00000-of-NNNNN.arrow'14:14: During handling of the above exception, another exception occurred:14:14: Traceback (most recent call last):14:   File \"/gpfsdswork/projects/rech/khy/uvb95lb/scrip_continual_pretraining/run_mlm_wwm.py\", line 450, in14:     main()14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/init.py\", line 346, in wrapper14:     return f(*args, **kwargs)14:            ^^^^^^^^^^^^^^^^^^14:   File \"/gpfsdswork/projects/rech/khy/uvb95lb/scrip_continual_pretraining/run_mlm_wwm.py\", line 294, in main14:     datasets = load_dataset(extension, data_files=data_files, cache_dir=\"/scrip_continual_pretraining/Cache_mlm\")14:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/load.py\", line 2152, in load_datasetDownloading data files: 100%|██████████| 2/2 [00:00<00:00, 18117.94it/s]14:     builder_instance.download_and_prepare(14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/builder.py\", line 928, in download_and_prepare14:     with incomplete_dir(self._output_dir) as tmp_output_dir:14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/contextlib.py\", line 155, inexitExtracting data files: 100%|██████████| 2/2 [00:00<00:00, 72.83it/s]14:     self.gen.throw(typ, value, traceback)14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/builder.py\", line 909, in incomplete_dir14:     shutil.rmtree(tmp_dir)14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/shutil.py\", line 738, in rmtree14:     onerror(os.rmdir, path, sys.exc_info())14:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/shutil.py\", line 736, in rmtree14:     os.rmdir(path, dir_fd=dir_fd)14: OSError: [Errno 39] Directory not empty: '/scrip_continual_pretraining/Cache_mlm/text/default-d0870639fca1403e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34.incomplete'Generating train split: 20130863 examples [00:54, 368921.36 examples/s]9: Traceback (most recent call last):9:   File \"/gpfsdswork/projects/rech/khy/uvb95lb/scrip_continual_pretraining/run_mlm_wwm.py\", line 450, in9:     main()9:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/init.py\", line 346, in wrapper9:     return f(*args, **kwargs)9:            ^^^^^^^^^^^^^^^^^^9:   File \"/gpfsdswork/projects/rech/khy/uvb95lb/scrip_continual_pretraining/run_mlm_wwm.py\", line 294, in main9:     datasets = load_dataset(extension, data_files=data_files, cache_dir=\"/scrip_continual_pretraining/Cache_mlm\")9:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^9:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/load.py\", line 2152, in load_dataset9:     builder_instance.download_and_prepare(9:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/builder.py\", line 948, in download_and_prepare9:     self._download_and_prepare(9:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/builder.py\", line 1045, in _download_and_prepare9:     raise OSError(9: OSError: Cannot find data file.9: Original error:9: [Errno 2] No such file or directory: '/scrip_continual_pretraining/Cache_mlm/text/default-d0870639fca1403e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34.incomplete/text-train-00000-00001-of-NNNNN.arrow'Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 17962.76it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 59.23it/s]Generating validation split: 3246757 examples [00:07, 410442.68 examples/s]srun: error: jean-zay-iam07: task 14: Exited with exit code 1srun: Terminating StepId=1741717.00: slurmstepd: error: *** STEP1741717.0 ON jean-zay-iam05 CANCELLED AT 2024-05-04T06:15:08 ***Generating train split: 122206 examples [00:00, 396783.29 examples/s]2: split:3308013examples [00:07, 424316.76 examples/s]srun: error: jean-zay-iam07: tasks 12-13,15-16: Terminatedsrun: error: jean-zay-iam05: tasks 0-2,4-5: Terminatedsrun: error: jean-zay-iam06: tasks 7-11: TerminatedGenerating train split: 489938 examples [00:01, 440569.16 examples/s]srun: error: jean-zay-iam07: task 17: Terminatedsrun: error: jean-zay-iam05: task 3: Terminatedsrun: error: jean-zay-iam06: task 6: Terminatedsrun: Force Terminated StepId=1741717.0`Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction14: OSError: [Errno 39] Directory not empty: '/scrip_continual_pretraining/Cache_mlm/text/default-d0870639fca1403e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34.incomplete'Generating train split: 20130863 examples [00:54, 368921.36 examples/s]9: Traceback (most recent call last):9:   File \"/gpfsdswork/projects/rech/khy/uvb95lb/scrip_continual_pretraining/run_mlm_wwm.py\", line 450, in9:     main()9:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/init.py\", line 346, in wrapper9:     return f(*args, **kwargs)9:            ^^^^^^^^^^^^^^^^^^9:   File \"/scrip_continual_pretraining/run_mlm_wwm.py\", line 294, in main9:     datasets = load_dataset(extension, data_files=data_files, cache_dir=\"/scrip_continual_pretraining/Cache_mlm\")9:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^9:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/load.py\", line 2152, in load_dataset9:     builder_instance.download_and_prepare(9:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/builder.py\", line 948, in download_and_prepare9:     self._download_and_prepare(9:   File \"/gpfslocalsup/pub/anaconda-py3/2023.09/envs/pytorch-gpu-2.1.1+py3.11.5/lib/python3.11/site-packages/datasets/builder.py\", line 1045, in _download_and_prepare9:     raise OSError(9: OSError: Cannot find data file.9: Original error:9: [Errno 2] No such file or directory: '/scrip_continual_pretraining/Cache_mlm/text/default-d0870639fca1403e/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34.incomplete/text-train-00000-00001-of-NNNNN.arrow'Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 17962.76it/s]Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 59.23it/s]Generating validation split: 3246757 examples [00:07, 410442.68 examples/s]srun: error: jean-zay-iam07: task 14: Exited with exit code 1srun: Terminating StepId=1741717.00: slurmstepd: error: *** STEP1741717.0 ON jean-zay-iam05 CANCELLED AT 2024-05-04T06:15:08 ***Generating train split: 122206 examples [00:00, 396783.29 examples/s]2: split:3308013examples [00:07, 424316.76 examples/s]srun: error: jean-zay-iam07: tasks 12-13,15-16: Terminatedsrun: error: jean-zay-iam05: tasks 0-2,4-5: TerminatedExpected behaviorI expect that my training succed to run at least one epoch\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_23.txt:\n",
      "Title: Phi-3's tokenizer.convert_ids_to_tokensreturns bytes instead of a string\n",
      "URL: https://github.com/huggingface/transformers/issues/33322\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0 and 4.44.2Platform: Linux-5.4.0-193-generic-x86_64-with-glibc2.31Python version: 3.10.4Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noUsing GPU in script?: noGPU type: NVIDIA RTX A5000Who can help?@arthuInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductioninitialize tokenizerfromtransformersimportAutoTokenizer.tokenizer=AutoTokenizer.from_pretrained(\"microsoft/Phi-3-small-8k-instruct\")callconvert_ids_to_tokenson a random token idtokenizer.convert_ids_to_tokens(tokenizer.encode(\" a\",add_special_tokens=False)[0])>b' a'Observe that the returned object is a byte (array), instead of the documented string (array).Expected behaviorThe returned object is a byte (array), instead of thedocumented string (array). For most models it is actually a string (array), so this would need some conditional conversion logic to call.decode(\"utf-8\").\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_37.txt:\n",
      "Title: latest 44.4.2 doesn't support falcon_mamba\n",
      "URL: https://github.com/huggingface/transformers/issues/33262\n",
      "Body:\n",
      "System Infopython 3.10rocky linux 9Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionpip install transformers==4.44.2Traceback (most recent call last):\n",
      "  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 993, in from_pretrained\n",
      "    config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n",
      "  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 695, in __getitem__\n",
      "    raise KeyError(key)\n",
      "KeyError: 'falcon_mamba'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/text-generation-webui/modules/ui_model_menu.py\", line 231, in load_model_wrapper\n",
      "    shared.model, shared.tokenizer = load_model(selected_model, loader)\n",
      "  File \"/home/user/text-generation-webui/modules/models.py\", line 93, in load_model\n",
      "    output = load_func_map[loader](model_name)\n",
      "  File \"/home/user/text-generation-webui/modules/models.py\", line 155, in huggingface_loader\n",
      "    config = AutoConfig.from_pretrained(path_to_model, trust_remote_code=shared.args.trust_remote_code)\n",
      "  File \"/home/user/miniconda3/envs/textgen/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\", line 995, in from_pretrained\n",
      "    raise ValueError(\n",
      "ValueError: The checkpoint you are trying to load has model type `falcon_mamba` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.Expected behaviormamba falcon loads\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_621.txt:\n",
      "Title: Nougat-small VisionEncoderDecoderModel failed when max_new_tokens > 3584,  Index out of range in self\n",
      "URL: https://github.com/huggingface/transformers/issues/27614\n",
      "Body:\n",
      "System InfoSystem Infotransformers == 4.35.0torch == 2.0.1+cu117Error shows on both cpu and cuda.TaskUse nougat to parse text from large documents.IssueNougatTokenizerFast. model_max_length = 3584 . Setting max_new_tokens=3585 (or larger) in model.generate works well on most pages, but fails on pages which output should be '[MISSING_PAGE_POST]'.Once the nougat fails, parsing all the following pages will fail unless kernel restart.ErrorIndexError: index out of range in selfWhat I TriedSettingnougat_model.config.decoder.pad_token_id = nougat_image_processor.tokenizer.eos_token_iddoesn't work.I suspect one special token is generated outside of vocabulary, which prevents from end-of-senescence detection.Nougat_base can generate an empty string with max_new_tokens=3585 on the same page, without errors.Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSample DocumentCodenougat_image_processor = NougatProcessor.from_pretrained(\"facebook/nougat-small\")\n",
      "nougat_model = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-small\")\n",
      "\n",
      "pixel_values = nougat_image_processor(image, return_tensors=\"pt\").pixel_values\n",
      "\n",
      "device = 'cpu'#\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "if device == 'cuda':\n",
      "    pixel_values = pixel_values.cuda()\n",
      "    nougat_model = nougat_model.cuda()\n",
      "    \n",
      "outputs = nougat_model.generate(\n",
      "    pixel_values.to(device),\n",
      "    min_length=1,\n",
      "    max_new_tokens=3585,\n",
      "    bad_words_ids=[[nougat_image_processor.tokenizer.unk_token_id]],\n",
      ")\n",
      "generated = nougat_image_processor.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
      "generated = nougat_image_processor.post_process_generation(generated, fix_markdown=False)\n",
      "print(generated)Expected behaviorExpected Output'[MISSING_PAGE_POST]' or '  '.Observed OutputIndexError                                Traceback (most recent call last)/tmp/ipykernel_11659/2341160804.py in6     nougat_model = nougat_model.cuda()7----> 8 outputs = nougat_model.generate(9     pixel_values.to(device),10     min_length=1,~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/utils/_contextlib.py in decorate_context(*args, **kwargs)113     def decorate_context(*args, **kwargs):114         with ctx_factory():--> 115             return func(*args, **kwargs)116117     return decorate_context~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/generation/utils.py in generate(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)1671         if generation_mode == GenerationMode.GREEDY_SEARCH:1672             # 11. run greedy search-> 1673             return self.greedy_search(1674                 input_ids,1675                 logits_processor=logits_processor,~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/generation/utils.py in greedy_search(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)25192520             # forward pass to get next token-> 2521             outputs = self(2522                 **model_inputs,2523                 return_dict=True,~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)1499                 or _global_backward_pre_hooks or _global_backward_hooks1500                 or _global_forward_hooks or _global_forward_pre_hooks):-> 1501             return forward_call(*args, **kwargs)1502         # Do not call functions when jit is used1503         full_backward_hooks, non_full_backward_hooks = [], []~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py in forward(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)602603         # Decode--> 604         decoder_outputs = self.decoder(605             input_ids=decoder_input_ids,606             attention_mask=decoder_attention_mask,~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)1499                 or _global_backward_pre_hooks or _global_backward_hooks1500                 or _global_forward_hooks or _global_forward_pre_hooks):-> 1501             return forward_call(*args, **kwargs)1502         # Do not call functions when jit is used1503         full_backward_hooks, non_full_backward_hooks = [], []~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)20462047         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)-> 2048         outputs = self.model.decoder(2049             input_ids=input_ids,2050             attention_mask=attention_mask,~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)1499                 or _global_backward_pre_hooks or _global_backward_hooks1500                 or _global_forward_hooks or _global_forward_pre_hooks):-> 1501             return forward_call(*args, **kwargs)1502         # Do not call functions when jit is used1503         full_backward_hooks, non_full_backward_hooks = [], []~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)12361237         # embed positions-> 1238         positions = self.embed_positions(input, past_key_values_length)12391240         hidden_states = inputs_embeds + positions.to(inputs_embeds.device)~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)1499                 or _global_backward_pre_hooks or _global_backward_hooks1500                 or _global_forward_hooks or _global_forward_pre_hooks):-> 1501             return forward_call(*args, **kwargs)1502         # Do not call functions when jit is used1503         full_backward_hooks, non_full_backward_hooks = [], []~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py in forward(self, input_ids, past_key_values_length)120         ).expand(bsz, -1)121--> 122         return super().forward(positions + self.offset)123124~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/modules/sparse.py in forward(self, input)160161     def forward(self, input: Tensor) -> Tensor:--> 162         return F.embedding(163             input, self.weight, self.padding_idx, self.max_norm,164             self.norm_type, self.scale_grad_by_freq, self.sparse)~/.conda/envs/py-chunker-client-ipython/lib/python3.9/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)2208         # remove once script supports set_grad_enabled2209no_grad_embedding_renorm(weight, input, max_norm, norm_type)-> 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)22112212IndexError: index out of range in self\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_147.txt:\n",
      "Title: [RT-DETR] No norm freezing for R18\n",
      "URL: https://github.com/huggingface/transformers/issues/32604\n",
      "Body:\n",
      "System InfoIssueIn the original implementation of the code, RT-DETR R18doesn't have its norms frozenI was wondering if this was normal or not and if I can submit a PR to parameterize this.I don't know if this was intentional because for the same config, the author also excplicitly removes the weight decay with anoptimizer group. And since it is hard to, by default, force the user to use a param group in transformers (at least I believe) freezing the norm would prevent the user from manually specifying it.Specstransformersversion: 4.43.3Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35Python version: 3.11.9Huggingface_hub version: 0.24.3Safetensors version: 0.4.3Accelerate version: 0.32.1Accelerate config:    - compute_environment: LOCAL_MACHINE- distributed_type: DEEPSPEED- mixed_precision: fp16- use_cpu: False- debug: False- num_processes: 2- machine_rank: 0- num_machines: 1- rdzv_backend: static- same_network: True- main_training_function: main- enable_cpu_affinity: False- deepspeed_config: {'gradient_accumulation_steps': 1, 'offload_optimizer_device': 'cpu', 'offload_param_device': 'cpu', 'zero3_init_flag': False, 'zero_stage': 2}- downcast_bf16: no- tpu_use_cluster: False- tpu_use_sudo: False- tpu_env: []- dynamo_config: {'dynamo_backend': 'INDUCTOR'}PyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA GeForce RTX 3090Who can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionprint(RTDetrConfig.from_pretrained(\"PekingU/rtdetr_r18vd_coco_o365\"))# model config...Expected behaviorprint(RTDetrConfig.from_pretrained(\"PekingU/rtdetr_r18vd_coco_o365\"))# model config with `freeze_norm` parameter\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_153.txt:\n",
      "Title: [whisper] settingprompt_condition_type=\"all-segments\"results in generation errors whenprompt_idsis set\n",
      "URL: https://github.com/huggingface/transformers/issues/32571\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: Linux-6.9.12-201.fsync.fc40.x86_64-x86_64-with-glibc2.35Python version: 3.12.4Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA GeForce RTX 3080Who can help?@ArthurZucker@SanchInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionimporttorchfromtransformersimportAutoProcessor,WhisperForConditionalGenerationfromdatasetsimportload_dataset,Audioprocessor=AutoProcessor.from_pretrained(\"openai/whisper-large-v3\",attn_implementation=\"flash_attention_2\",low_cpu_mem_usage=True,use_safetensors=True,\n",
      ")model=WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")model.to(\"cuda\",torch.float16)# load audios > 30 secondsds=load_dataset(\"distil-whisper/meanwhile\",\"default\")[\"test\"]# resample to 16kHzds=ds.cast_column(\"audio\",Audio(sampling_rate=16000))# take first 8 audios and retrieve arrayaudio=ds[:8][\"audio\"]audio=[x[\"array\"]forxinaudio]vocab=[\"aberration\",\"accolade\",\"acumen\",\"alacrity\",\"anomaly\",\"antithesis\",\"aplomb\",\"apocryphal\",\"arduous\",\"assiduous\",\"austere\",\"bellicose\",\"benevolent\",\"brevity\",\"cacophony\",\"capricious\",\"cogent\",\"colloquial\",\"conundrum\",\"credence\",\"cryptic\",\"deference\",\"delineate\",\"demagogue\",\"diatribe\",\"dichotomy\",\"disparage\",\"ebullient\",\"eclectic\",\"efficacy\",\"egregious\",\"elucidate\",\"ephemeral\",\"equivocate\",\"esoteric\",\"euphemism\",\"exacerbate\",\"exonerate\",\"expedite\",\"facetious\",\"fallacy\",\"fervent\",\"fortuitous\",\"gregarious\",\"harbinger\",\"idiosyncrasy\",\"imminent\",\"inchoate\",\"ineffable\",\"inimical\"]prompt=f\"Glossary:{'; '.join(vocab)}.\"prompt_ids=processor.get_prompt_ids(prompt,return_tensors=\"pt\")prompt_ids=prompt_ids.to(\"cuda\")# make sure to NOT truncate the input audio, to return the `attention_mask` and to pad to the longest audioinputs=processor(audio,return_tensors=\"pt\",truncation=False,padding=\"longest\",return_attention_mask=True,sampling_rate=16_000)inputs=inputs.to(\"cuda\",torch.float16)# transcribe audio to idsgenerated_ids=model.generate(**inputs,prompt_ids=prompt_ids,condition_on_prev_tokens=True,temperature=[0.0,0.2,0.4,0.6,0.8,1.0],logprob_threshold=-1.0,compression_ratio_threshold=2.4,no_speech_threshold=None,## COMMENT OUT TO TESTprompt_condition_type=\"all-segments\",\n",
      ")transcription=processor.batch_decode(generated_ids,skip_special_tokens=True)transcription[0]Expected result:\" Folks, if you watch the show, you know I spend a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chess set of the day's biggest stories. Developing the central headline pawns. Deftly maneuvering an oh-so-topical knight to F6. Feigning a classic Sicilian Neidorf variation on the news. All the while seeing eight moves deep and patiently marshalling the latest press releases into a Fischer-Schosen-Lipnitsky attack that culminates in the elegant, lethal, slow-played en passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I... Sometimes I startle awake upside down in the monkey bars of a condemned playground on a Superfund site. Get all hepped up on goofballs rummaged through a discarded tag bag of defective toys. Yank out a fistful of disembodied doll limbs. Toss them on a stained kid's placemat from a defunct dentist. Set up a table inside a rusty cargo container down by the wharf. And challenge toothless drifters to the godless bug house blitz of tournament that is my segment... Meanwhile...\"Actual results (withprompt_condition_type=\"all-segments\"):\" Folks, if you watch the show, you know I spend a lot of time right over there, patiently and astutely scrutinizing the boxwood and mahogany chess set of the day's biggest stories, developing the central headline pawns, deftly maneuvering an oh-so-topical knight to f6, feigning a classic Sicilian Neidorf variation on the news, all the while seeing eight moves deep and patiently marshaling the latest press releases into a Fischer-Chosin-Lipnitsky attack that culminates in the elegant, lethal, slow-played en passant chess. I'm not a chess-mack mate that is my nightly monologue, but sometimes, sometimes, folks, I... sometimes I... startle awake upside down in the last blitz of tournament that is my segment...\"Expected behaviorSettingprompt_condition_type=\"all-segments\"results in a significant chunk of the transcription being left off the end.Semi-relatedly, I was experiencing other generation errors when settingprompt_condition_type=\"all-segments\"in my own pipeline, but I haven't been able to reproduce those in an example. The errors I'm receiving are actually when I'm processing a shortform audio segment of about 2 seconds. I'm using a similar prompt as the above example, and the pipeline uses the same generation parameters. This is the error I get:Input length of decoder_input_ids is 448, butmax_lengthis set to 448. This can lead to unexpected behavior. You should consider increasingmax_lengthor, better yet, settingmax_new_tokens.However, settingmax_new_tokensto something like124or even as low as62results in this error on other audio segments:The length ofdecoder_input_idsequalprompt_idsplus special start tokens is 342, and themax_new_tokensis 448. Thus, the combined length ofdecoder_input_idsandmax_new_tokensis: 790. This exceeds themax_target_positionsof the Whisper model: 448. You should either reduce the length of your prompt, or reduce the value ofmax_new_tokens, so that their combined length is less than 448.Settingprompt_condition_type=\"first-segment\"allows it to process all of the audio segments just fine without error without having to setmax_new_tokens.Is this just a side effect ofprompt_condition_type? Ideally, I want to use this setting since many of the spoken vocab words do not appear in the first 30 seconds of speech.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_609.txt:\n",
      "Title: Add LayoutLMProcessor\n",
      "URL: https://github.com/huggingface/transformers/issues/27826\n",
      "Body:\n",
      "Feature requestAdd processor for LayoutLM. I'm not sure why v2 and v3 have their respective processors, but the original v1 doesn't. It should be almost identical their v2 and v3 counterparts (apply tesseract OCR + call the tokenizer appropriately), without returning the resized image (pixel_values), since LayoutLMv1 is text-only.This would also simplifydocument-question-answeringpipeline, since right now the pipeline repeats the above logic for LayoutLM.MotivationMake LayoutLM feature-parity with its v2 and v3.Your contributionI can submit a PR to add LayoutLMProcessor. It should be almost identical to v2 and v3, so the task should be straight-forward.Updatingdocument-question-answeringpipeline to use the new processor would be too complex since I'm not familiar with the codebase.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_184.txt:\n",
      "Title: error need either state_dict or a save folder\n",
      "URL: https://github.com/huggingface/transformers/issues/32427\n",
      "Body:\n",
      "System Infotransformers version: 4.43.3Platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.35Python version: 3.11.7Huggingface_hub version: 0.24.5Safetensors version: 0.4.3Accelerate version: 0.21.0Accelerate config:   not foundPyTorch version (GPU?): 2.4.0+cu121 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Who can help?This is codeimport transformers\n",
      "import torch\n",
      "\n",
      "\n",
      "def Response(text):\n",
      "    model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
      "    pipeline = transformers.pipeline(\n",
      "    \"text-generation\", model=model_id, \n",
      "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
      "    device_map=\"auto\",\n",
      "    offload_folder=\"save_folder\" # the error will show up with this code or without this code \n",
      "    )\n",
      "\n",
      "    output = pipeline(text)\n",
      "\n",
      "    return output[0]['generated_text']\n",
      "\n",
      "print(Response(\"hi\"))This is errori got this error on llama3.1/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.warn(\"The installed version of bitsandbytes was compiled without GPU support. \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32Loading checkpoint shards: 100%|██████████████████| 4/4 [00:00<00:00, 15.72it/s]Loading checkpoint shards: 100%|██████████████████| 4/4 [00:00<00:00, 13.49it/s]Traceback (most recent call last):File \"/home/aboud/Desktop/testLlama.py\", line 20, inprint(Response(\"hi\"))^^^^^^^^^^^^^^File \"/home/aboud/Desktop/testLlama.py\", line 8, in Responsepipeline = transformers.pipeline(^^^^^^^^^^^^^^^^^^^^^^File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/transformers/pipelines/init.py\", line 895, in pipelineframework, model = infer_framework_load_model(^^^^^^^^^^^^^^^^^^^^^^^^^^^File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 296, in infer_framework_load_modelraise ValueError(ValueError: Could not load model meta-llama/Meta-Llama-3.1-8B with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>). See the original errors:while loading with AutoModelForCausalLM, an error is thrown:Traceback (most recent call last):File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_modelmodel = model_class.from_pretrained(model, **kwargs)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrainedreturn model_class.from_pretrained(^^^^^^^^^^^^^^^^^^^^^^^^^^^^File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3990, in from_pretraineddispatch_model(model, **device_map_kwargs)File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/accelerate/big_modeling.py\", line 364, in dispatch_modelweights_map = OffloadedWeightsLoader(^^^^^^^^^^^^^^^^^^^^^^^File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/accelerate/utils/offload.py\", line 150, in initraise ValueError(\"Need either a state_dict or a save_folder containing offloaded weights.\")ValueError: Need either a state_dict or a save_folder containing offloaded weights.while loading with LlamaForCausalLM, an error is thrown:Traceback (most recent call last):File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_modelmodel = model_class.from_pretrained(model, **kwargs)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 3990, in from_pretraineddispatch_model(model, **device_map_kwargs)File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/accelerate/big_modeling.py\", line 364, in dispatch_modelweights_map = OffloadedWeightsLoader(^^^^^^^^^^^^^^^^^^^^^^^File \"/home/aboud/anaconda3/envs/pt/lib/python3.11/site-packages/accelerate/utils/offload.py\", line 150, in initraise ValueError(\"Need either a state_dict or a save_folder containing offloaded weights.\")ValueError: Need either a state_dict or a save_folder containing offloaded weights.the only way that it fix it by change device_map to cpu and comment the offload# fix it code\n",
      "import transformers\n",
      "import torch\n",
      "\n",
      "\n",
      "def Response(text):\n",
      "    model_id = \"meta-llama/Meta-Llama-3.1-8B\"\n",
      "    pipeline = transformers.pipeline(\n",
      "    \"text-generation\", model=model_id, \n",
      "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
      "    device_map=\"cpu\", # just change it to cpu \n",
      "    # offload_folder=\"save_folder\" \n",
      "    )\n",
      "\n",
      "    output = pipeline(text)\n",
      "    \n",
      "    return output[0]['generated_text']\n",
      "\n",
      "print(Response(\"hi\"))i dont know  why my gpu is integrated intel 3000 hd so it should ignore iti try it on linux arm64 system and it work well after 10 mintues of waitingand device_map set on autoInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI just run the codeExpected behaviorIt should give a clear error about what should i doI know it was because my gpucause i run the same code on my linux arm64 system and its work if i wasn't knowidk what i will do nextI will continue searching on this errorAnd tyvm\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_190.txt:\n",
      "Title: Potential error in num_patches calculation in src/transformers/models/vit_mae/modeling_vit_mae.py\n",
      "URL: https://github.com/huggingface/transformers/issues/32410\n",
      "Body:\n",
      "System InfoI've been examining theinterpolate_pos_encodingmethod in the ViTMAE implementation, and I've discovered what appears to be an error in the calculation ofnum_patches. This error seems to have unintended consequences on the method's behavior especially when the input image has the same size as the pretrained image. Here are my findings:Current implementation:def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
      "    num_patches = embeddings.shape[1] - 1\n",
      "    num_positions = self.position_embeddings.shape[1] - 1\n",
      "    \n",
      "    if num_patches == num_positions and height == width:\n",
      "        return self.position_embeddings\n",
      "    # ... rest of the methodIssue withnum_patchescalculation:The embeddings passed to this method are created inViTMAEPatchEmbeddings.forward(),x = self.projection(pixel_values).flatten(2).transpose(1, 2)For a 224x224 input image (same as pretrained size), x.shape is torch.Size([1, 196, 768]). This 196 represents the number of patches (14*14)without including a CLS token.For another higher resolution input image: 480x640:embeddings.shape= [1, 1200, 768].Current:num_patches= 1200 - 1 = 1199 which shouldn't be the case, since the num_patchesshould be 30*40 = 1200.Consequences of the error:I suppose in the current implementation,num_patches = embeddings.shape[1] - 1the -1 is for removing the cls token?Withnum_patches = embeddings.shape[1] - 1, we getnum_patches= 196 - 1 = 195.However, num_positions = self.position_embeddings.shape[1] - 1 = 197 - 1 = 196.This causesnum_patches != num_positions, even when using the pre-trained image size.Unintended behavior:Due to this discrepancy, the conditionnum_patches == num_positions and height == widthis never true.As a result, the method never returnsself.position_embeddingsdirectly, even when it should (i.e., when the input image size matches the pretrained size).This forces unnecessary interpolation even wheninterpolate_pos_encoding = Trueand the image size matches the pretrained size.Proposed fix:Changenum_patches = embeddings.shape[1] - 1tonum_patches = embeddings.shape[1]Can you confirm that this is indeed an error in the implementation? Are there any considerations I might be missing? Thank you for your time and for maintaining this amazing project!Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductiondef interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
      "    **num_patches = embeddings.shape[1] - 1**\n",
      "    num_positions = self.position_embeddings.shape[1] - 1\n",
      "    \n",
      "    if num_patches == num_positions and height == width:\n",
      "        return self.position_embeddings\n",
      "    # ... rest of the methodExpected behaviordef interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
      "    **num_patches = embeddings.shape[1]**\n",
      "    num_positions = self.position_embeddings.shape[1] - 1\n",
      "    \n",
      "    if num_patches == num_positions and height == width:\n",
      "        return self.position_embeddings\n",
      "    # ... rest of the method\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_806.txt:\n",
      "Title: Megatron-11B\n",
      "URL: https://github.com/huggingface/transformers/issues/21353\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_812.txt:\n",
      "Title: Cannot export Deberta to TorchScript\n",
      "URL: https://github.com/huggingface/transformers/issues/20815\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_837.txt:\n",
      "Title: add Unified-IO\n",
      "URL: https://github.com/huggingface/transformers/issues/19081\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_610.txt:\n",
      "Title: Could we Add Linear Projection Layer in pre-trained model?\n",
      "URL: https://github.com/huggingface/transformers/issues/27824\n",
      "Body:\n",
      "Model descriptionAs you know, if our model is too high positional_embedding size, then input vectors become too sparse data, so it leads to a performance decrease.However, I have an idea to overcome it.Before positional embedding, we add a new trainable linear projection layer to reduce its dimension.For example, If our model’s original positional embedding size is 4096, then we enlarge our input size as 8192.Then trainable linear projection layer projects 8192 to 4096 and then we perform positional embedding.let LP is a trainable Linear Projection Layer, PE(X) is positional Embedding.Then our calculation formula is as follows.\"Original Structure\"X(R^4096)-> PE(X, where X is in R^4096)-> model(PE(X))\"Proposed Structure\"X(R^8192)-> LP(X, where X^8192) -> X'(R^4096) -> PE(X') -> model(PE(X'))Can we run this with the current library? If not, can you add?Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationNo response\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_176.txt:\n",
      "Title: Additional options for include_num_input_tokens_seen in Trainer\n",
      "URL: https://github.com/huggingface/transformers/issues/32469\n",
      "Body:\n",
      "Feature requestTrack only the training, avoiding the count of padding tokensMotivationIt appears that this metric also includes padding tokens. If one would use example packing, then it really tracks the “correct” number of tokens seen by the model.However, I can think of two cases where this will not be accurate:In cases where packing is not used, training examples are padded to the longest sequence in the batchFor SFT training on completions onlyFor the first case, a more accurate calculation would be to sum the attention mask.For the second case, I'm not sure how this should be regarded. However, we can consider counting only label tokens !=-100Your contributionReplace lines 2248-2258 in trainer.py (v4.43.4) with the following:self.state.num_input_tokens_seen+=(torch.sum(self.accelerator.gather(torch.tensor(inputs['attention_mask'].sum(),device=self.args.device,dtype=torch.int64)\n",
      "        )\n",
      "    )\n",
      "    .cpu()\n",
      "    .item()\n",
      ")\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_162.txt:\n",
      "Title: TrOCR is ExecuTorch compatible\n",
      "URL: https://github.com/huggingface/transformers/issues/32509\n",
      "Body:\n",
      "Feature requestEnable TrOCR to\"Export to ExecuTorch\"workflowMotivationSee details in#32253Your contributionTBD\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_604.txt:\n",
      "Title: dMoE support\n",
      "URL: https://github.com/huggingface/transformers/issues/27915\n",
      "Body:\n",
      "Feature requestMistralAI recentlyreleased their new model, a Mixture of Experts based onmegablocks, a type of dropless Mixture of Experts.MotivationIt's very likely that the future of open source LLMs will be MoEs. Having it in HF transformers would allow us to use the built-in trainer, as it's unwieldy to use Megatron-LM for the average user who's only ever done QLoRA.Your contributionNo clue for now.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_638.txt:\n",
      "Title: Deepspeed integration: support batch sizes that are less than the number of gpus/ranks\n",
      "URL: https://github.com/huggingface/transformers/issues/27299\n",
      "Body:\n",
      "Feature requestCurrently when training using trainer and the deepspeed zero integration the minimum total effective batch size is one per gpu/rank, thus on a 8 gpu machine the minimum effective batch size is 8.As far as i know there is no fundamental requirement with zero style shading for there to be one batch per GPU.MotivationHaving one batch per gpu may be undesirable, for instance when there is insufficient memory.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_12.txt:\n",
      "Title: Accelerate x Trainer issue tracker:\n",
      "URL: https://github.com/huggingface/transformers/issues/33345\n",
      "Body:\n",
      "A bunch of issues are a bit stale, and@SunMarc+@muellerzrare a bit short on bandwidth!Thus we would love to have community support to solve the following:It's an AlignModel or Deepspeed Zero3 bug.#28808[2023-12-04 11:52:08,378] [INFO] [autotuner.py:1110:run_after_tuning] No optimal DeepSpeed configuration found by autotuning.#27830Training hangs at the first gradient syncing of an MoE model while using deepspeed#30911[Trainer.train] learning rate logging inconsistency: learning rate for the future step is logged#28124Memory leak when using CLIPTextModel#31439resume_from_checkpointfunction fails because \"There seems to be not a single sample in your epoch_iterator\"#26413Resuming from checkpoint runs into OOM#30822DeepSpeed ZeRO stage3+Qwen2/Qwen2-57B-A14B-Instruct: RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)#32312Title: CUDA RuntimeError: Unspecified Launch Failure during Training#30913DDP error with load_best_model_at_end enabled#30702ERROR in run_hp_search_optuna when trying to use multi-GPU#27487Trainer do not move the model to GPU when doing evaluation with FSDP#30239Please correct the following DeepSpeed config values that mismatch TrainingArguments values: scheduler.params.total_num_steps=0 vs hf num_training_steps (calculated)= 260#29348Issues occuring during parallel evaluation (using Trainer.evaluate)#30767Failed to load universal_checkpoint with deepspeed integreation#33157Multi-GPU setup: indices should be either on cpu or on the same device as the indexed tensor (cuda:1)#33147Cannot find the best model after training#31734\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_374.txt:\n",
      "Title: Log multiple losses used along with the combined losses when a model returns a dictionary of losses.\n",
      "URL: https://github.com/huggingface/transformers/issues/31081\n",
      "Body:\n",
      "Feature requestAble to log individual losses returned as dict.MotivationI have multiple losses that are being added to form a combined loss. I want to log all these individual losses to observe the trend of the individual losses. SemanticSegmenterOutput accepts single loss at the moment and logs the loss in the SemanticSegmenterOutput.Your contributionI have modified the Trainer class and SemanticSegmenter output as below but it is not working as expected. I have added a few print statements to check if the on_log part is being accessed or not but that code is not even being accessed.class CustomTrainer(Trainer):definit(self, *args, **kwargs):super().init(*args, **kwargs)self.additional_losses = {}def training_step(self, model, inputs):\n",
      "    outputs = model(**inputs)\n",
      "    \n",
      "    # Extract the additional losses from the model output\n",
      "    self.additional_losses = outputs.additional_losses\n",
      "    \n",
      "    # Continue with usual training step process\n",
      "    return super().training_step(model, inputs)\n",
      "\n",
      "def on_log(self, global_step, state, control, logs, **kwargs):\n",
      "    loss_dict = logs.get(\"loss_dict\", {})\n",
      "    \n",
      "    if 'wandb' in self.args.report_to:\n",
      "        wandb.log({k: v for k, v in loss_dict.items()})\n",
      "    \n",
      "    if 'tensorboard' in self.args.report_to:\n",
      "        tensorboard_writer = self.state.log_history.get('tensorboard')\n",
      "        if tensorboard_writer is not None:\n",
      "            tensorboard_writer.add_scalars('additional_losses', loss_dict, global_step)\n",
      "    \n",
      "    # Call the parent's on_log method\n",
      "    super().on_log(global_step, state, control, logs, **kwargs)\n",
      "\n",
      "def on_train_end(self):\n",
      "    tensorboard_writer = self.state.log_history.get('tensorboard')\n",
      "    if tensorboard_writer is not None:\n",
      "        tensorboard_writer.close()@DataClassclass CustomSemanticSegmenterOutput(SemanticSegmenterOutput):additional_losses: Optional[Dict[str, torch.FloatTensor]] = None\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_412.txt:\n",
      "Title: Add Prismatic VLMs to Transformers\n",
      "URL: https://github.com/huggingface/transformers/issues/30638\n",
      "Body:\n",
      "Model descriptionHi! I'm the author of\"Prismatic VLMs\", our upcoming ICML paper that introduces and ablates design choices of visually-conditioned language models that are similar to LLaVa or InstructBLIP, introducing ~50 new VLMs at the 3B/7B/13B scale that are trained with:Different Visual Representations (CLIP,SigLIP,DINOv2, fusions thereof likeSigLIP + DINOv2)Different LLM Backbones (LLaMa2,Vicuña v1.5,Mistral v0.1,Mistral v0.1 Instruct,Phi-2, etc.)Different Data (e.g., the LLaVa v1.5 Data, LVIS-Instruct-4V, and more upcoming!)Our best models outperform LLaVa v1.5 given the same data/same scale on a wide spectrum of different evaluation tasks; furthermore, we're seeing a lot of folks adopt our code for their research into new data mixtures, scaling to different LLM/Vision backbones, new projection mechanisms, and more.I think it'd be awesome to support these intransformers-- especially to tap into existing tooling for loading quantized versions of models, usingPEFTand other tools in the HF ecosystem for adaptation/fine-tuning, and general usability of our trained models.While we have 50+ checkpoints (all open-sourced, and loadable in our library), all currents models share a pretty common interface of using some pretrained visual extractor fromtimm, aXXXForCausalLMfromtransformers, and a lightweightnn.Moduleto project visual features into the LLM embedding space. As such, I'm hoping to contribute a generalmodeling_prismatic.pyclass that implementsPrismaticPretrainedModelandPrismaticForConditionalGenerationthat properly instantiates the appropriate VLM instance using the dependencies already intransformers.I'm happy to get started with this, following theinstructions here, but would love help/advice on clean ways to support all the various image backbones / LLM backbones / preprocessing schemes, and verifying compatibility with existing HF tooling!Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationPrismatic Authors:@siddk,@ashwin-balakrishna96(Potentially) Relevant Folks at HF:@merveenoyan@NielsRoggeUseful Links:Loading Pretrained Models (in Prismatic)Lightweight Generation REPL12 Task Evaluation Suite from Paper (for unit-testing, fully reproduces all results from our paper)Existing Checkpoints on HF Hub\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_406.txt:\n",
      "Title: Mixtral past_key_values and output_router_logits incompatible\n",
      "URL: https://github.com/huggingface/transformers/issues/30731\n",
      "Body:\n",
      "System Infotransformers==4.40.2Python 3.11.8Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionfromtransformersimportMixtralConfig,MixtralForCausalLM,AutoTokenizerimporttorch# Initializing a smaller version of Mixtral for faster executionconfiguration=MixtralConfig(hidden_size=256,intermediate_size=896,num_hidden_layers=8,num_attention_heads=8,num_key_value_heads=8,num_local_experts=4,num_experts_per_tok=1,\n",
      ")model=MixtralForCausalLM(configuration)tokenizer=AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")prompt=\"This is a test\"tokenized=tokenizer(prompt,return_tensors=\"pt\")output=model(**tokenized,output_router_logits=True)key_values=output.past_key_valueslogits=output.logitsnext_token_logits=logits[...,-1, :]# Softmaxsoftmaxed=torch.nn.functional.softmax(next_token_logits,dim=-1)# Samplesampled=torch.multinomial(softmaxed.squeeze(),num_samples=1)ids=sampled.item()attention_mask=torch.cat([tokenized[\"attention_mask\"],torch.tensor([[1]])],dim=-1)next_output=model(torch.tensor([[ids]]),attention_mask=attention_mask,past_key_values=key_values,output_router_logits=True)Expected behaviorIt seems that this is the same underlying issue as in#29087- I would expectpast_key_valuesto work withoutput_router_logits.So what happens?Without past key values (and with multiple input ids) theall_router_logitshas the proper sequence length, thus inload_balancing_loss_functhisnum_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)correctly evaluates the number of hidden layers.If past key values are used,all_router_logitshas a sequence length of 1, but since the attention mask is still the whole sequence (from which thesequence_lengthis inferred) the hidden layers evaluate to a small value or 0, leading to the same error as inMixtral inference breaks whenoutput_router_logits=True#29087Instead, I would like theload_balancing_loss_functo be able to deal with a case where thegate_logitspassed are of shape[batch_size X 1, num_experts]instead of[batch_size X sequence_length, num_experts].\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_348.txt:\n",
      "Title: Object Detection Pipeline only outputs first element when batching\n",
      "URL: https://github.com/huggingface/transformers/issues/31356\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-5.4.0-182-generic-x86_64-with-glibc2.31Python version: 3.11.8Huggingface_hub version: 0.23.0Safetensors version: 0.4.2Accelerate version: 0.30.1Accelerate config:    not foundPyTorch version (GPU?): 2.2.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:When running the ObjectDetectionPipeline in a batch, the output will only be the bounding boxes of the first input image due to ObjectDetectionPipeline.py accessing element [0] in postprocessing  and not looping over all outputs.transformers/src/transformers/pipelines/object_detection.pyLine 150\n",
      "      ina4e1a1draw_annotation=raw_annotations[0]This accesses only and always the first element, instead of looping over all outputs.Only the first element is accessed in postprocessingWho can help?@NarsilInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionCreate object detection Pipelinepipe = pipeline(\"object-detection\", model=model_name, image_processor=preprocessor_name, device=device)use batch inference with batch_size > 2.for out in tqdm(pipe(dataset, batch_size=batch_size)):Expected behaviorExpected Output: 2 Elements with each x items (bboxes).Actual Output, only bboxes of the first input element.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_599.txt:\n",
      "Title: Open to contribution: addingtorch.nn.functional.scaled_dot_product_attentionsupport for more architectures\n",
      "URL: https://github.com/huggingface/transformers/issues/28005\n",
      "Body:\n",
      "Feature requestInTransformers 4.36, we started adding native support oftorch.nn.functional.scaled_dot_product_attention(SDPA), enabled by default in Transformers:https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattentionSDPA allows to dispatch to memory-efficient attention, flash attention on supported GPUs (currently NVIDIA-only), and even onIntel CPUs.For the record, here's a benchmark on some currently supported models:Training benchmark, run on A100-SXM4-80GB.ModelBatch sizeSequence lengthTime per batch (\"eager\", s)Time per batch (\"sdpa\", s)SpeedupPeak memory (\"eager\", MB)Peak memory (\"sdpa\", MB)Memory savingsllama2 7b410241.0650.9019.4%73878.2845977.8160.7%llama2 7b42048OOM1.87/OOM78394.58SDPA does not OOMllama2 7b120480.640.4832.0%55557.0129795.6386.4%llama2 7b13072OOM0.75/OOM37916.08SDPA does not OOMllama2 7b14096OOM1.03/OOM46028.14SDPA does not OOMllama2 7b24096OOM2.05/OOM78428.14SDPA does not OOMInference benchmark, run on A100-SXM4-80GB.ModelBatch sizePrompt lengthNum new tokensPer token latency\"eager\"(ms)Per token latency\"sdpa\"(ms)Speedupllama2 13b110241 (prefill)178.66159.3612.11%llama2 13b110010040.3537.627.28%llama2 13b810010040.5538.066.53%Whisper v3 large1/6220.0518.906.10%Whisper v3 large8/7725.4224.772.59%Whisper v3 large16/7728.5126.328.34%Previously, we had a partial support of SDPA inOptimum BetterTransformerbut we are now looking to slowly deprecate it in favor of upstream support of SDPA directly in Transformers.Here are the architectures for which support has been requested:Codegen (BetterTransformer not supporting CodeGen2optimum#1050)LLAVA (Can optimum.bettertransformer supports LLAVA model?optimum#1592)Marian (BetterTransforer not Support Marianoptimum#1142)Mistral (Add support for mistral  type Model to use Mistral  and Zephyroptimum#1553)LongT5 (longT5 BetterTransformer implementationoptimum#1506)ViT (Add support for mistral  type Model to use Mistral  and Zephyroptimum#1553)The integration could take inspiration fromhttps://github.com/huggingface/optimum/blob/main/optimum/bettertransformer/models/decoder_models.py&https://github.com/huggingface/optimum/blob/main/optimum/bettertransformer/models/attention.pyMotivationFaster training & inference, lower memory requirementYour contributionI may work on some at some point, but contributions are most welcome.You should refer to#26572to add the support of SDPA for a model, roughly following these steps:Create aXxxSdpaAttentionclass inheriting fromXxxAttentionand implement the attention logic using SDPAUse_prepare_4d_causal_attention_mask_for_sdpainstead of_prepare_4d_causal_attention_maskfor SDPAUse_prepare_4d_attention_mask_for_sdpainstead of_prepare_4d_attention_maskfor SDPAAdd_supports_sdpa = TruetoXxxPreTrainedModelAdd\"sdpa\"key toXXX_ATTENTION_CLASSESin the model modeling file\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_200.txt:\n",
      "Title: Some Whisper beam search output (sequences_scores, etc.) is lost in _stack_split_outputs\n",
      "URL: https://github.com/huggingface/transformers/issues/32373\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.3Platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.23.2Safetensors version: 0.4.3Accelerate version: 0.30.1Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noUsing GPU in script?: yesGPU type: NVIDIA GeForce RTX 4090 Laptop GPUWho can help?@sanchit-gandhi@kamilakesbiInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionIn generating short form output (<30 sec):# inputs is from the processorgen_kwargs={\"max_new_tokens\":400,\"num_beams\":5,\"temperature\":None,\"return_timestamps\":False,\"return_dict_in_generate\":True,\"num_return_sequences\":1,\"output_scores\":True,\"language\":\"english\"}pred_ids=self.model.generate(inputs,**gen_kwargs)print(pred_ids.__class__)print(dict((k,type(v))fork,vinvars(pred_ids).items()))Expected behaviorGenerateBeamEncoderDecoderOutputseems to lose some fields in a recent version. (Maybe other output forms are also affected, haven't checked.)Bisecting transformers versions, in 4.42.4 the output looked like:<class 'transformers.generation.utils.GenerateBeamEncoderDecoderOutput'>{'sequences': <class 'torch.Tensor'>,'sequences_scores': <class 'torch.Tensor'>, 'scores': <class 'tuple'>, 'logits': <class 'NoneType'>,'beam_indices': <class 'torch.Tensor'>, 'encoder_attentions': <class 'NoneType'>, 'encoder_hidden_states': <class 'NoneType'>, 'decoder_attentions': <class 'NoneType'>, 'cross_attentions': <class 'NoneType'>, 'decoder_hidden_states': <class 'NoneType'>, 'past_key_values': <class 'tuple'>}In 4.43.0 and aftersequences_scoresandbeam_indicesbecameNone:<class 'transformers.generation.utils.GenerateBeamEncoderDecoderOutput'>{'sequences': <class 'torch.Tensor'>,'sequences_scores': <class 'NoneType'>, 'scores': <class 'tuple'>, 'logits': <class 'NoneType'>,'beam_indices': <class 'NoneType'>, 'encoder_attentions': <class 'NoneType'>, 'encoder_hidden_states': <class 'NoneType'>, 'decoder_attentions': <class 'NoneType'>, 'cross_attentions': <class 'NoneType'>, 'decoder_hidden_states': <class 'NoneType'>, 'past_key_values': <class 'tuple'>}It looks like these get removed in postprocessing, potential culprit in_stack_split_outputsattransformers/src/transformers/models/whisper/generation_whisper.pyLine 946\n",
      "      in9451a38def_stack_split_outputs(self,seek_outputs,model_output_type,device,kwargs):Which looks like it changed in#30984.Hacking inif key in [\"sequences\", \"beam_indices\", \"sequences_scores\"]:, for example, fixes it, although I'm not sure what's intended to be handled as tensors vs. tuples, so will defer as to the best way to fix.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_566.txt:\n",
      "Title: Adding mixtral attention_bias in style of llama modeling\n",
      "URL: https://github.com/huggingface/transformers/issues/28440\n",
      "Body:\n",
      "Feature requestSystem Infotransformers version: 4.36.2Who can help?don't have a clue about thisInformationRefer to llama2 modeling code, I want to add attention bias option in mixtral model and configuration for flexibility of experiments.If this changes seems appropriate, I will make a PR for itExpected behaviorAfter changes, attention bias option of model is added in config.Can be controlled like example below(default config value is false)from transformers import AutoConfig\n",
      "config = AutoConfig.from_pretrained(\"variant_of_mixtral\")\n",
      "config.attention_bias = TrueMotivationRefer to llama2 modeling code, I want to add attention bias option in mixtral model and configuration for flexibility of experiments.Your contributionI have created a fix branch. I can make a PR of itrefer tolink\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_572.txt:\n",
      "Title: [Flax] Migration from frozen to regular dicts with v0.7.1+\n",
      "URL: https://github.com/huggingface/transformers/issues/28368\n",
      "Body:\n",
      "Feature requestAs of version 0.7.1, Flax defaults to returningregular dictionarieswith the methods.initand.apply, notfrozen dictionariesas was the case before:google/flax#3191The.initmethod is called in the Transformers methodmodel.init_weights, where we randomly initialised the model's parameters:transformers/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.pyLine 370\n",
      "      in4ab5fb8random_params=self.module.init(rngs,input_ids,attention_mask,position_ids,return_dict=False)[\"params\"]Therefore, this Flax update is a breaking change for Transformers: previously, callingmodel.init_weightsreturned a frozen dict of params, whereas now it returns a regular dict. However, blindly reverting to using frozen dicts might cause issues for Flax users, since they will get regular dicts of params from Flax, but get frozen ones from Transformers.This leaves us with two options:Update themodel.init_weightsmethod to always return a frozen dict, even in themodule.initreturns a standard dict. This mitigates the breaking change and reverts to the behaviour we had beforeFollow the Flax behaviour and return regular dicts of params with v0.7.1+. This would keep Transformers in-line with the latest Flax philosophy, at the expense of a breaking changeA PR to implement 1 is in#28367: it is a single line change for each of the Flax modelling files. To implement 2, we would need to check if therandom_paramsreturn by themodule.initmethod are frozen or not, and match the dictionary type on the returned outputs.Note that the change in behaviour will only really affect users who are initialising parameters themselves (with_do_init=False). These are typically advanced users who are familiar with the Flax library, and want an easy way of dropping-in Transformers Flax modules into other Flax scripts. Therefore, I would be in favour of 2, in order to maintain equivalence between the Flax and Transformers libraries. For users who rely on automatic init (_do_init=True), there's unlikely to be any friction, since they tend not to access the model params anyway.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_214.txt:\n",
      "Title: Qwen2_moe: Avoid zero tokens fowarding for some experts\n",
      "URL: https://github.com/huggingface/transformers/issues/32283\n",
      "Body:\n",
      "System Infotransformers=4.43.3python=3.8LinuxWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionUnder Auto-GPTQ with Triton kernel, it would use math.log2() function in Line 96While for the Implementation of MoE,transformers/src/transformers/models/qwen2_moe/modeling_qwen2_moe.pyLines 667 to 675\n",
      "      inf739687forexpert_idxinrange(self.num_experts):expert_layer=self.experts[expert_idx]idx,top_x=torch.where(expert_mask[expert_idx])# Index the correct hidden states and compute the expert hidden state for# the current expert. We need to make sure to multiply the output hidden# states by `routing_weights` on the corresponding tokens (top-1 and top-2)current_state=hidden_states[None,top_x].reshape(-1,hidden_dim)current_hidden_states=expert_layer(current_state)*routing_weights[top_x,idx,None]some experts directly forward with zero tokens and therefore the input shape is like [0, seq_length, hidden_states], and fails on log2()The issue could be solved by checking the number of tokens before Line 675if current_state.shape[0] == 0: continueExpected behaviorNo more forwarding with zero tokens.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_228.txt:\n",
      "Title: flashattention3\n",
      "URL: https://github.com/huggingface/transformers/issues/32219\n",
      "Body:\n",
      "Feature requestflashattention3 has been released, when will it be supported flashattention3？MotivationuseYour contributionpr\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_770.txt:\n",
      "Title: Need support for Sentence Similarity Pipeline\n",
      "URL: https://github.com/huggingface/transformers/issues/22923\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_995.txt:\n",
      "Title: tokenizer.Tokenizer compatibility with Inference API or Auto* classes\n",
      "URL: https://github.com/huggingface/transformers/issues/10340\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_759.txt:\n",
      "Title: Wandb sweeps integraition: custom objective\n",
      "URL: https://github.com/huggingface/transformers/issues/23647\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_229.txt:\n",
      "Title: Parallel inference on generative models throws an exception\n",
      "URL: https://github.com/huggingface/transformers/issues/32217\n",
      "Body:\n",
      "System InfoLinux (ubuntu 20.04), transformers==4.40.0 onwards (including latest version), A100 GPULlama3 8B Instruct and LLama3.1 8B InstructWho can help?@zucchini-nlp@ArthurZucker@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionStepsInstall transformers==4.40.0 or newerGet Llama2 8B InstructLoad model using  AutoModelForCausalLM.from_pretrained onto GPU (I am using A100)Try and use the same model from two threads (I have a web app that allow more than one request to use a single instance of a model)Note that each thread has its own tokenizer as I know these are not thread safeI am using the TextIteratorStreamer but it also blows up calling generate and waiting for the whole completionThe fault is:File \"/home/AiimiAdmin/aiimi/InsightMaker.Python/AIModelService/venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 1989, in generateself._target(*self._args, **self._kwargs)File \"/home/AiimiAdmin/aiimi/InsightMaker.Python/AIModelService/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_contextreturn func(*args, **kwargs)^^^^^^^^^^^^^^^^^^^^^File \"/home/AiimiAdmin/aiimi/InsightMaker.Python/AIModelService/venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 1989, in generateresult = self._sample(^^^^^^^^^^^^^File \"/home/AiimiAdmin/aiimi/InsightMaker.Python/AIModelService/venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2969, in _sampleresult = self._sample(^^^^^^^^^^^^^File \"/home/AiimiAdmin/aiimi/InsightMaker.Python/AIModelService/venv/lib/python3.12/site-packages/transformers/generation/utils.py\", line 2969, in _samplenext_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^RuntimeError: probability tensor contains eitherinf,nanor element < 0next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^RuntimeError: probability tensor contains eitherinf,nanor element < 0Note that this works fine if I use transformers==4.39.0 or older.Seems like the logic has changed to use a shared data structure or there is something more fundamental with torch.Expected behaviorInference runs, albeit slower as you are running more than one generate task with the same model and GPU.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_215.txt:\n",
      "Title: During the first evaluation after training, an OOM (Out of Memory) error occurs.\n",
      "URL: https://github.com/huggingface/transformers/issues/32282\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.3Platform: Linux-5.15.0-46-generic-x86_64-with-glibc2.31Python version: 3.9.19Huggingface_hub version: 0.24.2Safetensors version: 0.4.3Accelerate version: 0.33.0PyTorch version (GPU?): 2.4.0 (True)GPU type: NVIDIA TITAN VWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductiontrain_args = TrainingArguments(\n",
      " \n",
      "    learning_rate=args.lr,\n",
      "    weight_decay=args.weight_decay,\n",
      "    per_device_train_batch_size=args.batch_size #4,\n",
      "    per_device_eval_batch_size=args.batch_size #4,\n",
      "    log_level=args.log_level,\n",
      "    logging_steps = 1 ,\n",
      "    evaluation_strategy=args.evaluation_strategy, # steps\n",
      "    eval_steps=args.eval_steps,# 2\n",
      "    eval_accumulation_steps=args.eval_accumulation_steps,#1\n",
      ")\n",
      "\n",
      "lora_config = LoraConfig(\n",
      "    r=args.r,\n",
      "    use_dora=True,\n",
      "    bias=\"none\",\n",
      "    lora_alpha=args.lora_alpha,\n",
      "    target_modules=(\n",
      "        args.lora_target_modules.split(\",\")\n",
      "        if args.lora_target_modules\n",
      "        else [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
      "    )\n",
      ")\n",
      "peft_model = get_peft_model(ehr_model, lora_config)\n",
      "trainer = Trainer(\n",
      "    model=peft_model,\n",
      "    args=train_args,\n",
      "    train_dataset=train_data,\n",
      "    eval_dataset=dev_data,\n",
      "    tokenizer=tokenizer,\n",
      "    data_collator=data_collator,\n",
      "    compute_metrics=compute_metrics,\n",
      "    callbacks=[info_callback]\n",
      ")Expected behaviorIf no evaluation is performed during training, no OOM (Out of Memory) error occurs. However, I want to conduct evaluations after training for a few epochs. But every time after an evaluation is completed and the next training epoch begins, an OOM error occurs. It seems like the GPU memory usage suddenly increases after backpropagation. What could be the reason?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_201.txt:\n",
      "Title: Load Phi3 small 8k Instruct without Flash attention\n",
      "URL: https://github.com/huggingface/transformers/issues/32365\n",
      "Body:\n",
      "System InfoPhi3 small, flash attention, gpuWho can help?@ArthurZucker@muellerzr@stevhliuInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionHi, I'm trying to load a Phi 3 small Instruct 8k model.Link:https://huggingface.co/microsoft/Phi-3-small-8k-instructI want to use it further for fine tuning, but I can't load it with flash attention because I have a V100 graphics cards and those are incompatibleSo I am trying to load it without flash attention using attn_implementation=\"eager\":model = AutoModelForSequenceClassification.from_pretrained(\"path\", num_labels=num_labels ,attn_implementation=\"eager\" )But I still get this error:AssertionError: Flash Attention is not available, but is needed for dense attentionIs there any way to load the model without flash attention?I am using the latest version oftransformers 4.43.3\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_567.txt:\n",
      "Title: GQA Llama 13B slower than Llama 13B without GQA\n",
      "URL: https://github.com/huggingface/transformers/issues/28425\n",
      "Body:\n",
      "Feature requestIt would be nice if when I choose different key_value_heads (key_value_heads < attention_heads) on config's model, automatically the attn weights were computed by mean pooling. Right now, if I do this, it gives me the next error.key_value_heads = 4MotivationMake models faster, e.g Llama  2 13B, Llama 7B, Mistral 7B etc.Your contributionI tried to do a simple implementation. But it gives me inconsistent results. GQA model is slower than No GQA model.from transformers import LlamaConfig\n",
      "from transformers.models.llama.modeling_llama import LlamaAttention, LlamaSdpaAttention\n",
      "from copy import deepcopy\n",
      "import torch\n",
      "\n",
      "def split_attention_to_heads(input_tensor, num_splits):\n",
      "    # Get the shape of the input tensor\n",
      "    rows, cols = input_tensor.shape\n",
      "\n",
      "    # Check if the number of rows is divisible by the number of splits\n",
      "    if rows % num_splits != 0:\n",
      "        raise ValueError(\"Number of rows is not divisible by the number of splits\")\n",
      "\n",
      "    # Calculate the number of rows in each split\n",
      "\n",
      "    # Use chunk to split the tensor along the rows\n",
      "    split_tensors = input_tensor.chunk(num_splits, dim=0)\n",
      "\n",
      "    return split_tensors\n",
      "\n",
      "def average_heads(tensor_tuple, group_size, dtype):\n",
      "    # Initialize an empty list to store the averaged tensors\n",
      "    averaged_tensors = []\n",
      "\n",
      "    # Iterate through the tuple and average consecutive groups\n",
      "    for i in range(0, len(tensor_tuple), group_size):\n",
      "        # Take a group of tensors\n",
      "        tensor_group = tensor_tuple[i:i + group_size]\n",
      "\n",
      "        # Calculate the mean along dimension 0\n",
      "        averaged_tensor = torch.mean(torch.stack(tensor_group), dim=0, dtype=dtype)\n",
      "\n",
      "        # Append the averaged tensor to the list\n",
      "        averaged_tensors.append(averaged_tensor)\n",
      "\n",
      "    # Convert the list of averaged tensors to a tuple\n",
      "    averaged_tensors_tuple = tuple(averaged_tensors)\n",
      "\n",
      "    return averaged_tensors_tuple\n",
      "\n",
      "def convert_wts_to_gqa(attention_module: torch.nn.Module , model_configuration: LlamaConfig):\n",
      "    attentions_wts = attention_module.state_dict().copy()\n",
      "    num_heads = model_configuration.num_attention_heads\n",
      "    gqa_groups = num_heads // model_configuration.num_key_value_heads\n",
      "    for name_wts in list(attentions_wts.keys()):\n",
      "        if (\"k_proj\" in name_wts) or (\"v_proj\" in name_wts):\n",
      "            tensor_to_convert = attentions_wts[name_wts].clone()\n",
      "            torch_dtype = tensor_to_convert.dtype\n",
      "            attn_heads = split_attention_to_heads(tensor_to_convert, num_splits=num_heads)\n",
      "            gqa_tensors_grouped = average_heads(attn_heads, gqa_groups, dtype=torch_dtype)\n",
      "            gqa_tensors_grouped = torch.cat(gqa_tensors_grouped)\n",
      "            attentions_wts[name_wts] = gqa_tensors_grouped\n",
      "            del tensor_to_convert\n",
      "    return attentions_wts\n",
      "\n",
      "\n",
      "\n",
      "def convert_llama_to_gqa(module: torch.nn.Module, llama_config_from_hf: LlamaConfig, inplace: bool = False):\n",
      "    if isinstance(module, LlamaAttention):\n",
      "        wts_gqa = convert_wts_to_gqa(attention_module=module, model_configuration=llama_config_from_hf)\n",
      "        llama_atention_gqa = LlamaAttention(llama_config_from_hf, layer_idx=module.layer_idx)\n",
      "        llama_atention_gqa.half()\n",
      "        llama_atention_gqa.load_state_dict(wts_gqa)\n",
      "        return llama_atention_gqa\n",
      "\n",
      "    out = module if inplace else deepcopy(module)\n",
      "    for name, child in out.named_children():\n",
      "        out._modules[name] = convert_llama_to_gqa(child, llama_config_from_hf=llama_config_from_hf, inplace=True)\n",
      "    return out\n",
      "\n",
      "from transformers import AutoConfig\n",
      "\n",
      "configuration_llama = AutoConfig.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
      "configuration_llama.num_key_value_heads = 4\n",
      "\n",
      "llama_gqa = convert_llama_to_gqa(llama, configuration_llama)ResultsGQA LLAMANO GQA LLAMAI don't know if I'm misunderstanding something, please let me know if you can see something I can't\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_598.txt:\n",
      "Title: Mixtral: Reduce and Increase Expert Models\n",
      "URL: https://github.com/huggingface/transformers/issues/28058\n",
      "Body:\n",
      "Feature requestAdd methods to MixtralSparseMoeBlock, MixtralDecoderLayer, and MixtralModel for reducing (and enlarging) the number of expert models.Implement a mechanism to decrease the number of expert models by removing corresponding rows in gate weights. This should enable the removal expert models by id.transformers/src/transformers/models/mixtral/modeling_mixtral.pyLines 688 to 710\n",
      "      in1a585c1classMixtralSparseMoeBlock(nn.Module):\"\"\"This implementation isstrictly equivalent to standard MoE with full capacity (nodropped tokens). It's faster since it formulates MoE operationsin terms of block-sparse operations to accomodate imbalancedassignments of tokens to experts, whereas standard MoE either(1) drop tokens at the cost of reduced performance or (2) setcapacity factor to number of experts and thus waste computationand memory on padding.\"\"\"def__init__(self,config):super().__init__()self.hidden_dim=config.hidden_sizeself.ffn_dim=config.intermediate_sizeself.num_experts=config.num_local_expertsself.top_k=config.num_experts_per_tok# gatingself.gate=nn.Linear(self.hidden_dim,self.num_experts,bias=False)self.experts=nn.ModuleList([MixtralBLockSparseTop2MLP(config)for_inrange(self.num_experts)])MotivationThis will allow scaling down or up the model size from a pre-trained model while preserving existing weights, eliminating the need to retrain from scratch.Your contributionI am willing to contribute a pr, but need a few time.I would like to know if such a PR is likely to be accepted, before I start working on it.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_349.txt:\n",
      "Title: AttributeError: 'NllbTokenizerFast' object has no attribute 'lang_code_to_id'\n",
      "URL: https://github.com/huggingface/transformers/issues/31348\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.0.dev0Platform: Windows-10-10.0.20348-SP0Python version: 3.9.7Huggingface_hub version: 0.23.3Safetensors version: 0.4.3Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 1.13.0 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA RTX A6000Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizertokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\", src_lang=\"ron_Latn\",token=token)model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", token=token)article = \"Şeful ONU spune că nu există o soluţie militară în Siria\"inputs = tokenizer(article, return_tensors=\"pt\")translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"], max_length=30)tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]Expected behaviorIt should output translated text: UN-Chef sagt, es gibt keine militärische Lösung in SyrienComplete error:translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"deu_Latn\"], max_length=30)AttributeError: 'NllbTokenizerFast' object has no attribute 'lang_code_to_id'\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_375.txt:\n",
      "Title: Add ability to specify input device for ffmpeg_microphone()\n",
      "URL: https://github.com/huggingface/transformers/issues/31074\n",
      "Body:\n",
      "Feature requestThe functiontransformers.pipelines.audio_utils.ffmpeg_microphone()currently has the following code for setting the input device for ffmpeg:def ffmpeg_microphone(\n",
      "    sampling_rate: int,\n",
      "    chunk_length_s: float,\n",
      "    format_for_conversion: str = \"f32le\",\n",
      "):\n",
      "    \"\"\"\n",
      "    Helper function to read raw microphone data.\n",
      "    \"\"\"\n",
      "   <....>\n",
      "    system = platform.system()\n",
      "    if system == \"Linux\":\n",
      "        format_ = \"alsa\"\n",
      "        input_ = \"default\"\n",
      "    elif system == \"Darwin\":\n",
      "        format_ = \"avfoundation\"\n",
      "        input_ = \":0\"\n",
      "    elif system == \"Windows\":\n",
      "        format_ = \"dshow\"\n",
      "        input_ = _get_microphone_name()This makes it where the only option is to use default ALSA input device. If the user wants to select a different device, there is no way to change this, other than making the system-wide change of default device.What I would like to see instead is an optionalinput_device=...parameter added to theffmpeg_microphone()function that allows the user to specify a different input device.It would still behave the same as it currently does (use alsa default), if user does nothing, so the change wouldn't break existing code. But if they pass a different input device in function args, they can use it without having to change default input device system-wide.MotivationI want to use a different input device than the system default input. I do not want to make the change system-wide.Your contributionThis would be an extremely simple change to make.  Just change the function header from:def ffmpeg_microphone(\n",
      "    sampling_rate: int,\n",
      "    chunk_length_s: float,\n",
      "    format_for_conversion: str = \"f32le\",\n",
      "):to:def ffmpeg_microphone(\n",
      "    sampling_rate: int,\n",
      "    chunk_length_s: float,\n",
      "    format_for_conversion: str = \"f32le\",\n",
      "    input_device= None,\n",
      "):And make the same change to theffmpeg_microphone_live()function so that different device could be used there as well.Then, change this part:system = platform.system()\n",
      "    if system == \"Linux\":\n",
      "        format_ = \"alsa\"\n",
      "        input_ = \"default\"\n",
      "    elif system == \"Darwin\":\n",
      "        format_ = \"avfoundation\"\n",
      "        input_ = \":0\"\n",
      "    elif system == \"Windows\":\n",
      "        format_ = \"dshow\"\n",
      "        input_ = _get_microphone_name()\n",
      "\n",
      "    ffmpeg_command = [\n",
      "        \"ffmpeg\",\n",
      "        \"-f\",\n",
      "        format_,\n",
      "        \"-i\",\n",
      "        input_,\n",
      "        \"-ac\",\n",
      "        ac,\n",
      "        \"-ar\",\n",
      "        ar,\n",
      "        \"-f\",\n",
      "        format_for_conversion,\n",
      "        \"-fflags\",\n",
      "        \"nobuffer\",\n",
      "        \"-hide_banner\",\n",
      "        \"-loglevel\",\n",
      "        \"quiet\",\n",
      "        \"pipe:1\",\n",
      "    ]to use user-supplied format input if provided, and OS specific defaults otherwise:input_ = input_device\n",
      "\n",
      "system = platform.system()\n",
      "if system == \"Linux\":\n",
      "    format_ = \"alsa\"\n",
      "    if not input_:\n",
      "        input_ = \"default\"\n",
      "elif system == \"Darwin\":\n",
      "    format_ = \"avfoundation\"\n",
      "    if not input_:\n",
      "        input_ = \":0\"\n",
      "elif system == \"Windows\":\n",
      "    format_ = \"dshow\"\n",
      "    if not input_:\n",
      "        input_ = _get_microphone_name()\n",
      "\n",
      "ffmpeg_command = [\n",
      "    \"ffmpeg\",\n",
      "    \"-f\",\n",
      "    format_,\n",
      "    \"-i\",\n",
      "    input_,\n",
      "    \"-ac\",\n",
      "    ac,\n",
      "    \"-ar\",\n",
      "    ar,\n",
      "    \"-f\",\n",
      "    format_for_conversion,\n",
      "    \"-fflags\",\n",
      "    \"nobuffer\",\n",
      "    \"-hide_banner\",\n",
      "    \"-loglevel\",\n",
      "    \"quiet\",\n",
      "    \"pipe:1\",\n",
      "]\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_413.txt:\n",
      "Title: More memory consumption than litgpt\n",
      "URL: https://github.com/huggingface/transformers/issues/30629\n",
      "Body:\n",
      "System Infotransformers=4.40.1pytorch=2.2.1deepspeed=0.14.1accelerate=0.29.0Who can help?@pacman100Hello, I tried fine-tuning LLMs using transformers' trainer with deepspeed zero-3 or fsdp. While I find it is very easy to use, it seems to cause more memory consumption (e.g., than litgpt's fsdp).For example, when I fine-tune a 7b LLM (with bf16, flashattn, max context length=2048), with transformers' code (with zero-3 or fsdp), batch=2 (local) leads to OOM on a 8X40GB A100 node; while with litgpt, the (local) batch size can be set to 6 without OOM.My fine-tuning script is as follows:deepspeed --num_gpus=$gpu code/train.py --adam_beta2 0.99 --adam_epsilon 1e-8 --num_train_epochs $epoch --per_device_train_batch_size $batch --per_device_eval_batch_size $batch --gradient_accumulation_steps $accumulation_steps --gradient_checkpointing \\\n",
      "          --learning_rate $lr --warmup_steps 30  --max_grad_norm 2.0 --seed $seed --data_seed $seed --logging_steps 10 --save_strategy 'epoch' --evaluation_strategy 'epoch' \\\n",
      "          --bf16 --output_dir $OUT_DIR --logging_dir $OUT_DIR --model_name_or_path $PRETRAINED_MODEL_PATH --tokenizer_name $TOKENIZER_PATH \\\n",
      "           --deepspeed zero3.jsonThe training code is also straightforward:def train_model(model, train_dataset, eval_dataset, training_args, data_collator=None):\n",
      "\n",
      "    last_checkpoint = None\n",
      "    if os.path.isdir(training_args.output_dir):\n",
      "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
      "        if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
      "            print(\n",
      "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
      "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
      "            )\n",
      "\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=eval_dataset,\n",
      "        data_collator=data_collator\n",
      "    )\n",
      "\n",
      "    checkpoint = None\n",
      "    if training_args.resume_from_checkpoint is not None:\n",
      "        checkpoint = training_args.resume_from_checkpoint\n",
      "    elif last_checkpoint is not None:\n",
      "        checkpoint = last_checkpoint\n",
      "\n",
      "    print(f\"Loaded from the checkpoint: {checkpoint}\")\n",
      "\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "    \n",
      "    trainer.save_model()\n",
      "    trainer.log_metrics(\"train\", train_result.metrics)\n",
      "    metrics = trainer.evaluate()\n",
      "    trainer.log_metrics(\"eval\", metrics)\n",
      "    trainer.save_metrics(\"eval\", metrics)model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, torch_dtype=bf16, use_flash_attention_2=True, resume_download=True)\n",
      "train_model(model, train_dataset, eval_dataset, training_args)Could I know whether there is something wrong with my code that leads to the additional memory cost?Thank youInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionIt can be easily run with my provided code and script.Expected behaviorlocal batch size can be increased to 6 without OOM\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_13.txt:\n",
      "Title: How to install transformers==4.45, two or three days I can install successfully, but today cannot.\n",
      "URL: https://github.com/huggingface/transformers/issues/33343\n",
      "Body:\n",
      "System Infotorch2.2Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionpip install git+https://github.com/huggingface/transformers.gitExpected behaviorHow to install the latest transformers\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1023.txt:\n",
      "Title: Converting all model Config classes to dataclasses\n",
      "URL: https://github.com/huggingface/transformers/issues/8775\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_163.txt:\n",
      "Title: Bart/Wav2Vec2 is ExecuTorch compatible\n",
      "URL: https://github.com/huggingface/transformers/issues/32508\n",
      "Body:\n",
      "Feature requestEnable Bart/Wav2Vec2 to\"Export to ExecuTorch\"workflowMotivationSee details in#32253Your contributionTBD\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_822.txt:\n",
      "Title: Convert LongT5 to ONNX\n",
      "URL: https://github.com/huggingface/transformers/issues/20275\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_836.txt:\n",
      "Title: Adding TensorFlow port of LeViT\n",
      "URL: https://github.com/huggingface/transformers/issues/19123\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_188.txt:\n",
      "Title: Incomplete memory allocation of dual GPU\n",
      "URL: https://github.com/huggingface/transformers/issues/32412\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.0.dev0Platform: Linux-5.15.0-117-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.2.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA GeForce RTX 3090 TiWho can help?text models:@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionIt occurs when using HF Transformers with Text-Generation WebUI (ticket opened here:oobabooga/text-generation-webui#6028) but was reproduced on a simple loading test in a Jupyter Notebook using the base conda environment, therefore I open a ticket here as it's probably a HF Transformers issue.The problem happens when a large model (Llama-3-70B with q4 parameters) gets loaded in a dual GPU (RTX 3090+RTX 3090 Ti); a large part of the VRAM isn't allocated (GPU0: 94%, GPU1: 66%).This is causing a torch.cuda.OutOfMemoryError for a modest length of text.17:34:28-952126 INFO     Loading \"Meta-Llama-3-70B-Instruct\"                    \n",
      "17:34:28-968657 INFO     TRANSFORMERS_PARAMS=                                   \n",
      "{   'low_cpu_mem_usage': True,\n",
      "    'torch_dtype': torch.bfloat16,\n",
      "    'use_flash_attention_2': True,\n",
      "    'device_map': 'auto',\n",
      "    'max_memory': {0: '24200MiB', 1: '24200MiB', 'cpu': '99GiB'},\n",
      "    'quantization_config': BitsAndBytesConfig {\n",
      "  \"_load_in_4bit\": true,\n",
      "  \"_load_in_8bit\": false,\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": true,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "}Expected behaviortorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 1 has a total capacity of 23.68 GiB of which 77.69 MiB is free. Including non-PyTorch memory, this process has 22.66 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 70.71 MiB is reserved by PyTorch but unallocated.There are two issues with this:It's a 24GB GPU therefore the total capacity shouldn't be 23.68GB (94% allocation)The other GPU is used at 66%, it shouldn't face any issue using full context length\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_820.txt:\n",
      "Title: Add FlexiBERT\n",
      "URL: https://github.com/huggingface/transformers/issues/20362\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_834.txt:\n",
      "Title: openai whisper ASR pytorch to tflite\n",
      "URL: https://github.com/huggingface/transformers/issues/19478\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_175.txt:\n",
      "Title: Enhancing Hugging Face Models with Tensor Parallelism for Large-Scale Model Support 🚀\n",
      "URL: https://github.com/huggingface/transformers/issues/32470\n",
      "Body:\n",
      "Feature requestDescriptionThis feature proposal aims to update Hugging Face's support for tensor parallelism (TP) to accommodate the increasing size and complexity of models such asLLaMA 3.1,Nemotron-4-340B-Instruct, and others, which have surpassed the capabilities of current training frameworks like TRL + DeepSpeed.Currently, the Hugging Face codebase is outdated concerning these advancements. Although parallelism requires careful customization based on hardware setup, dataset size, sequence length, and model size, implementing TP across many Hugging Face models is crucial.ProposalWith the introduction of tensor parallelism inPyTorch 2.0, the previous method of creating processes per device and model in theMegatronstyle is no longer efficient.Key Changes:Refactoring Code for TP:Remove the use ofkwargsin favor of more straightforward TP implementations, as PyTorch parallel plans do not accommodatekwargs.Refactor PyTorch models to incorporate TP effectively.Current Limitations:Existing implementations, such as inmodeling_llama, are not trainable and are incompatible withtorch.compilefor inference optimization.Future Integration:As models scale to large sizes, 8-way Tensor Parallel is becoming standard.This change would enableAccelerateto later support TP + FSDP (Fully Sharded Data Parallel), which many users could benefit from.Personal ContributionI have personally developed code that allows LLaMA to run entirely with TP, observing that it handles longer token sequences with less memory than FSDP. However, I have not submitted a pull request due to the need for comprehensive code refactoring.Call to ActionIf Hugging Face acknowledges this need, I am willing to contribute further if there is an overarching plan for abstraction and integration.MotivationMotivationThe motivation behind this proposal is to address the limitations and frustrations experienced when using Hugging Face with the current parallelism approaches, especially for large-scale models like LLaMA 3.1 and Nemotron-4-340B-Instruct. As models grow in complexity, existing frameworks struggle to support efficient training and inference.Current Issues with Existing Solutions:NVIDIA Megatron-LM: Lacks compiler-level optimization and is somewhat outdated.Tensor Parallel by BlackSamorez: Also lacks compiler-level optimization and is outdated.DeepSpeed: Primarily uses data parallelism (DP), with ZeRO closer to model parallelism (MP) rather than tensor parallelism (TP). It also has issues with ZeRO Stage 3.AWS Neuron Distributed: Potentially supports TP in distributed settings, though not tested extensively.PyTorch Lightning: Implements TP but is not applicable to Hugging Face models.NVIDIA NeMo: Uses PyTorch Lightning, underscoring the need for Hugging Face to adopt TP, including coding styles like avoidingkwargs.Implementing tensor parallelism (TP) in Hugging Face models is crucial to keep up with the trend towards larger models and to enhance compatibility with modern optimization techniques liketorch.compile.Your contributionContributionI am willing to contribute to implementing tensor parallelism (TP) within the Hugging Face ecosystem. To facilitate this, I would appreciate guidance on the following aspects:Integration Approach: Clarification on whether TP should be applied during model initialization, such as withAutoModelForCausalLM, or if it should be managed externally usingtorchrun.Automatic Initialization: Decide if the implementation should automatically initializetorch.distributedwithout requiring explicit commands from users.With a defined plan or abstraction level, I can work on refactoring the necessary code and submit a pull request to integrate TP effectively. My experience with TP, particularly with LLaMA, has demonstrated its efficiency in handling large models with reduced memory usage compared to current methods.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1035.txt:\n",
      "Title: Trying to add support for GPT2 as decoder in EncoderDecoder model\n",
      "URL: https://github.com/huggingface/transformers/issues/4483\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1021.txt:\n",
      "Title: Sparse Transormer\n",
      "URL: https://github.com/huggingface/transformers/issues/8945\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_149.txt:\n",
      "Title: [Bug] RT-DETR post-processing yields incorrect results when use_focal_loss=False\n",
      "URL: https://github.com/huggingface/transformers/issues/32578\n",
      "Body:\n",
      "System Infotransformersversion: 4.45.0.dev0Platform: Linux-5.19.0-1010-nvidia-lowlatency-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0a0+07cecf4168.nv24.05 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA GeForce RTX 4090Who can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe official usageexamplefor RT-DETR in the documentation produces incorrect results whenuse_focal_lossis set toFalsein thepost_process_object_detectionmethod.importtorchimportrequestsfromPILimportImagefromtransformersimportRTDetrForObjectDetection,RTDetrImageProcessorurl='http://images.cocodataset.org/val2017/000000039769.jpg'image=Image.open(requests.get(url,stream=True).raw)image_processor=RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")model=RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd\")inputs=image_processor(images=image,return_tensors=\"pt\")withtorch.no_grad():outputs=model(**inputs)# Here is changed# Default use_focal_loss=Trueresults=image_processor.post_process_object_detection(outputs,target_sizes=torch.tensor([image.size[::-1]]),threshold=0.3,use_focal_loss=False)forresultinresults:forscore,label_id,boxinzip(result[\"scores\"],result[\"labels\"],result[\"boxes\"]):score,label=score.item(),label_id.item()box=[round(i,2)foriinbox.tolist()]print(f\"{model.config.id2label[label]}:{score:.2f}{box}\")Expected behaviorWhenuse_focal_loss=Truesofa: 0.97 [0.14, 0.38, 640.13, 476.21]\n",
      "cat: 0.96 [343.38, 24.28, 640.14, 371.5]\n",
      "cat: 0.96 [13.23, 54.18, 318.98, 472.22]\n",
      "remote: 0.95 [40.11, 73.44, 175.96, 118.48]\n",
      "remote: 0.92 [333.73, 76.58, 369.97, 186.99]Whenuse_focal_loss=Falseperson: 1.00 [40.11, 73.44, 175.96, 118.48]\n",
      "person: 1.00 [343.38, 24.28, 640.14, 371.5]\n",
      "person: 1.00 [13.23, 54.18, 318.98, 472.22]\n",
      "person: 1.00 [333.73, 76.58, 369.97, 186.99]\n",
      "person: 1.00 [-10.18, -66.86, 629.35, 412.62]\n",
      "person: 1.00 [0.85, -69.13, 640.82, 410.67]\n",
      "person: 1.00 [4.19, -24.31, 644.17, 378.63]\n",
      "person: 1.00 [0.04, -62.35, 640.01, 417.49]\n",
      "person: 1.00 [-8.1, 136.34, 631.0, 385.72]\n",
      "person: 1.00 [11.32, 54.2, 317.21, 473.39]\n",
      "person: 1.00 [-5.81, 139.17, 620.73, 380.67]\n",
      "person: 1.00 [2.41, 18.83, 642.39, 378.26]\n",
      "person: 1.00 [-9.84, 21.47, 629.93, 322.49]\n",
      "person: 1.00 [0.36, -1.19, 640.35, 478.7]\n",
      "person: 1.00 [-44.53, -104.03, 595.36, 373.99]\n",
      "person: 1.00 [0.91, -39.17, 640.89, 440.75]\n",
      "person: 1.00 [-2.88, 216.51, 637.06, 434.71]\n",
      "person: 1.00 [1.13, -0.58, 331.65, 476.0]\n",
      "person: 1.00 [1.15, 58.44, 428.54, 471.67]\n",
      "person: 1.00 [5.91, -66.87, 645.79, 378.84]\n",
      "person: 1.00 [0.9, -44.8, 640.88, 435.03]\n",
      "person: 1.00 [-0.19, -1.78, 639.8, 476.89]\n",
      "person: 1.00 [1.72, 39.05, 641.1, 472.89]\n",
      "person: 1.00 [343.53, 24.69, 639.36, 370.48]\n",
      "person: 1.00 [1.8, 30.17, 640.73, 470.69]\n",
      "person: 1.00 [6.8, -69.75, 646.77, 389.73]\n",
      "person: 1.00 [0.6, 3.42, 640.59, 389.43]\n",
      "person: 1.00 [-45.64, -107.19, 594.27, 372.2]\n",
      "person: 1.00 [0.68, 0.79, 640.59, 314.32]\n",
      "person: 1.00 [1.17, 1.47, 641.16, 384.91]\n",
      "person: 1.00 [0.14, 0.38, 640.13, 476.21]\n",
      "person: 1.00 [15.19, 6.0, 655.03, 271.6]\n",
      "person: 1.00 [-35.92, -98.61, 602.61, 379.19]\n",
      "person: 1.00 [1.21, -27.17, 641.2, 452.78]\n",
      "person: 1.00 [-42.0, -93.72, 597.85, 385.56]\n",
      "person: 1.00 [11.33, 127.73, 481.84, 470.83]\n",
      "person: 1.00 [0.78, 2.29, 640.77, 375.53]\n",
      "person: 1.00 [-38.3, -109.69, 599.97, 369.88]\n",
      "person: 1.00 [-8.05, 198.77, 631.16, 385.42]\n",
      "person: 1.00 [28.85, 277.41, 638.55, 475.06]\n",
      "person: 1.00 [1.96, 262.19, 641.8, 475.22]\n",
      "person: 1.00 [0.15, 234.33, 639.66, 465.15]\n",
      "person: 1.00 [100.86, 25.73, 590.97, 369.99]\n",
      "person: 1.00 [16.91, -39.15, 198.13, 427.58]\n",
      "person: 1.00 [8.48, 56.71, 317.32, 377.75]\n",
      "person: 1.00 [-24.16, 44.59, 609.98, 324.35]\n",
      "person: 1.00 [1.67, -21.88, 641.65, 457.15]\n",
      "person: 1.00 [1.58, 47.78, 263.77, 316.35]\n",
      "person: 1.00 [66.05, 40.36, 641.24, 475.55]\n",
      "person: 1.00 [8.08, 34.69, 559.91, 368.08]\n",
      "person: 1.00 [0.03, 9.21, 185.75, 465.75]\n",
      "person: 1.00 [0.51, 3.66, 640.51, 381.63]\n",
      "person: 1.00 [-39.93, -109.05, 600.01, 370.51]\n",
      "person: 1.00 [69.6, 72.99, 172.19, 112.08]\n",
      "person: 1.00 [-38.78, -102.11, 600.59, 370.73]\n",
      "person: 1.00 [59.02, 29.3, 615.04, 463.61]\n",
      "person: 1.00 [37.15, 53.98, 308.09, 124.21]\n",
      "person: 1.00 [1.09, 221.75, 638.09, 448.28]\n",
      "person: 1.00 [-56.47, 27.89, 419.31, 455.42]\n",
      "person: 1.00 [0.78, -0.79, 217.89, 478.34]\n",
      "person: 1.00 [8.94, 25.11, 638.34, 472.57]\n",
      "person: 1.00 [18.73, 35.29, 619.38, 458.91]\n",
      "person: 1.00 [345.32, 23.68, 645.02, 269.5]\n",
      "person: 1.00 [28.8, -47.42, 631.41, 350.5]\n",
      "person: 1.00 [1.26, 42.63, 172.07, 298.25]\n",
      "person: 1.00 [-12.0, -0.47, 598.94, 346.74]\n",
      "person: 1.00 [-5.09, 173.18, 617.97, 383.35]\n",
      "person: 1.00 [1.47, -0.27, 641.42, 415.41]\n",
      "person: 1.00 [56.78, 13.05, 640.34, 473.68]\n",
      "person: 1.00 [0.19, 0.81, 640.18, 130.65]\n",
      "person: 1.00 [73.93, 121.67, 637.92, 474.11]\n",
      "person: 1.00 [1.52, 72.09, 329.32, 478.16]\n",
      "person: 1.00 [-8.07, -74.78, 594.32, 387.01]\n",
      "person: 1.00 [19.6, -17.51, 635.97, 320.83]\n",
      "person: 1.00 [-40.61, -73.53, 599.33, 371.17]\n",
      "person: 1.00 [243.94, 26.74, 594.06, 309.69]\n",
      "person: 1.00 [446.15, 18.67, 581.79, 310.19]\n",
      "person: 1.00 [4.56, 285.25, 644.46, 469.4]\n",
      "person: 1.00 [151.32, 62.36, 367.71, 326.84]\n",
      "person: 1.00 [25.66, 283.26, 322.72, 474.62]\n",
      "person: 1.00 [3.91, -25.77, 643.89, 399.53]\n",
      "person: 1.00 [0.46, 191.1, 225.5, 458.27]\n",
      "person: 1.00 [1.85, -17.32, 641.01, 460.72]\n",
      "person: 1.00 [162.58, 50.65, 598.12, 381.86]\n",
      "person: 1.00 [0.65, 236.97, 640.58, 449.68]\n",
      "person: 1.00 [528.98, -1.92, 638.65, 62.44]\n",
      "person: 1.00 [363.64, -0.13, 638.31, 82.46]\n",
      "person: 1.00 [41.2, 73.73, 175.34, 118.12]\n",
      "person: 1.00 [0.97, -1.93, 367.2, 477.25]\n",
      "person: 1.00 [487.96, -0.82, 639.21, 82.64]\n",
      "person: 1.00 [2.84, 37.55, 363.03, 340.35]\n",
      "person: 1.00 [1.45, 331.03, 641.42, 476.01]\n",
      "person: 1.00 [3.64, 66.66, 353.08, 363.45]\n",
      "person: 1.00 [346.94, 25.13, 639.3, 471.17]\n",
      "person: 1.00 [-0.81, -1.63, 336.41, 478.3]\n",
      "person: 1.00 [11.35, 60.53, 399.96, 474.83]\n",
      "person: 1.00 [202.02, 56.34, 609.75, 367.46]\n",
      "person: 1.00 [64.17, 301.09, 637.44, 475.32]\n",
      "person: 1.00 [43.04, 40.22, 549.0, 324.92]\n",
      "person: 1.00 [0.49, 0.75, 639.51, 123.66]\n",
      "person: 1.00 [2.91, 1.34, 496.53, 247.87]\n",
      "person: 1.00 [-43.47, -95.43, 596.13, 383.73]\n",
      "person: 1.00 [-0.32, 0.54, 639.64, 121.65]\n",
      "person: 1.00 [2.36, 1.03, 480.97, 123.27]\n",
      "person: 1.00 [1.69, 101.47, 625.26, 384.55]\n",
      "person: 1.00 [10.54, 169.03, 224.0, 460.98]\n",
      "person: 1.00 [-2.45, 46.63, 371.29, 441.16]\n",
      "person: 1.00 [-25.4, -99.08, 593.66, 379.06]\n",
      "person: 1.00 [490.38, 20.15, 639.81, 471.27]\n",
      "person: 1.00 [1.95, 2.5, 641.92, 319.84]\n",
      "person: 1.00 [0.54, 0.91, 640.43, 125.01]\n",
      "person: 1.00 [1.31, 130.35, 121.19, 469.66]\n",
      "person: 1.00 [-0.02, 0.68, 639.98, 124.31]\n",
      "person: 1.00 [344.29, 93.63, 576.88, 370.13]\n",
      "person: 1.00 [0.48, 37.4, 94.03, 470.18]\n",
      "person: 1.00 [71.13, -0.23, 639.15, 81.83]\n",
      "person: 1.00 [367.52, 37.94, 601.49, 230.9]\n",
      "person: 1.00 [-30.56, -102.78, 600.38, 376.28]\n",
      "person: 1.00 [5.34, 269.57, 643.09, 465.62]\n",
      "person: 1.00 [369.59, -0.1, 638.84, 72.72]\n",
      "person: 1.00 [173.63, 34.99, 579.36, 436.05]\n",
      "person: 1.00 [11.33, 25.64, 638.05, 424.99]\n",
      "person: 1.00 [4.19, 23.88, 644.11, 387.04]\n",
      "person: 1.00 [-0.95, -73.24, 637.02, 368.35]\n",
      "person: 1.00 [336.04, 77.28, 372.21, 257.29]\n",
      "person: 1.00 [14.94, -66.54, 647.64, 379.76]\n",
      "person: 1.00 [6.49, 132.64, 303.64, 466.5]\n",
      "person: 1.00 [223.85, 45.22, 607.61, 361.32]\n",
      "person: 1.00 [1.19, 0.52, 640.01, 122.91]\n",
      "person: 1.00 [12.19, 69.9, 319.91, 312.88]\n",
      "person: 1.00 [7.95, 7.4, 646.76, 442.46]\n",
      "person: 1.00 [2.6, -38.88, 642.58, 440.3]\n",
      "person: 1.00 [2.97, 127.14, 640.23, 475.11]\n",
      "person: 1.00 [4.5, 94.64, 311.44, 313.71]\n",
      "person: 1.00 [-0.41, -0.06, 639.58, 313.42]\n",
      "person: 1.00 [17.37, 57.79, 215.01, 412.27]\n",
      "person: 1.00 [10.01, 50.71, 269.38, 446.99]\n",
      "person: 1.00 [-37.33, -48.81, 601.75, 404.09]\n",
      "person: 1.00 [492.12, -0.21, 639.69, 70.05]\n",
      "person: 1.00 [-21.18, -73.4, 590.9, 401.15]\n",
      "person: 1.00 [408.74, 24.02, 643.94, 265.61]\n",
      "person: 1.00 [17.23, 186.59, 247.22, 472.83]\n",
      "person: 1.00 [151.44, 190.11, 297.58, 321.04]\n",
      "person: 1.00 [3.88, 302.06, 643.83, 475.11]\n",
      "person: 1.00 [0.27, 0.25, 640.26, 123.1]\n",
      "person: 1.00 [492.85, 16.35, 640.26, 471.57]\n",
      "person: 1.00 [14.62, 55.53, 316.26, 291.06]\n",
      "person: 1.00 [14.1, -0.33, 631.16, 74.81]\n",
      "person: 1.00 [334.63, 78.56, 470.02, 356.54]\n",
      "person: 1.00 [3.77, 18.34, 479.68, 473.46]\n",
      "person: 1.00 [344.6, 102.96, 638.63, 470.47]\n",
      "person: 1.00 [2.75, 8.23, 640.96, 339.47]\n",
      "person: 1.00 [108.61, 17.33, 635.49, 368.47]\n",
      "person: 1.00 [487.9, -0.24, 638.91, 461.87]\n",
      "person: 1.00 [158.74, 117.42, 639.63, 473.97]\n",
      "person: 1.00 [1.58, 19.88, 305.79, 360.04]\n",
      "person: 1.00 [498.59, 28.67, 640.16, 472.73]\n",
      "person: 1.00 [-2.19, 201.11, 637.61, 382.93]\n",
      "person: 1.00 [0.14, 49.01, 71.28, 435.95]\n",
      "person: 1.00 [-15.64, 102.72, 604.85, 368.72]\n",
      "person: 1.00 [494.15, 32.05, 628.72, 340.45]\n",
      "person: 1.00 [1.41, 0.66, 639.71, 297.89]\n",
      "person: 1.00 [2.69, 14.26, 640.87, 388.07]\n",
      "person: 1.00 [340.11, 0.93, 638.18, 194.54]\n",
      "person: 1.00 [0.94, 0.74, 639.67, 124.13]\n",
      "person: 1.00 [0.18, 0.32, 640.17, 122.62]\n",
      "person: 1.00 [500.6, 6.68, 639.57, 471.04]\n",
      "person: 1.00 [348.77, 204.83, 628.61, 413.85]\n",
      "person: 1.00 [-37.45, -96.32, 599.52, 380.37]\n",
      "person: 1.00 [346.7, 93.69, 578.03, 370.37]\n",
      "person: 1.00 [7.68, 56.58, 318.09, 370.17]\n",
      "person: 1.00 [-39.59, -92.42, 598.33, 386.99]\n",
      "person: 1.00 [1.42, 59.61, 315.34, 474.65]\n",
      "person: 1.00 [7.24, -22.12, 645.44, 405.42]\n",
      "person: 1.00 [148.65, 51.8, 632.96, 372.74]\n",
      "person: 1.00 [7.99, 57.89, 427.42, 468.69]\n",
      "person: 1.00 [256.47, 53.29, 602.2, 369.29]\n",
      "person: 1.00 [417.25, 45.64, 579.57, 289.73]\n",
      "person: 1.00 [8.96, 218.43, 623.48, 375.79]\n",
      "person: 1.00 [345.58, 24.0, 640.83, 212.95]\n",
      "person: 1.00 [2.6, 196.52, 221.6, 472.77]\n",
      "person: 1.00 [1.54, -0.22, 309.6, 129.08]\n",
      "person: 1.00 [252.79, 46.48, 636.74, 375.81]\n",
      "person: 1.00 [0.68, 0.06, 638.26, 121.15]\n",
      "person: 1.00 [0.6, -0.06, 346.16, 246.56]\n",
      "person: 1.00 [538.08, 52.16, 638.37, 173.23]\n",
      "person: 1.00 [491.73, 12.13, 638.8, 456.34]\n",
      "person: 1.00 [425.46, 25.33, 640.99, 373.4]\n",
      "person: 1.00 [0.28, 131.58, 104.88, 471.05]\n",
      "person: 1.00 [-28.17, -6.42, 603.09, 363.32]\n",
      "person: 1.00 [334.83, 92.94, 366.77, 247.36]\n",
      "person: 1.00 [1.08, 59.42, 316.2, 472.36]\n",
      "person: 1.00 [314.86, 296.47, 636.58, 475.68]\n",
      "person: 1.00 [-15.84, -83.0, 592.57, 396.41]\n",
      "person: 1.00 [336.01, 76.63, 396.59, 319.83]\n",
      "person: 1.00 [382.79, 72.34, 573.73, 280.94]\n",
      "person: 1.00 [128.7, 100.72, 482.46, 342.77]\n",
      "person: 1.00 [-0.86, 50.07, 608.39, 301.51]\n",
      "person: 1.00 [364.86, 73.13, 388.2, 106.67]\n",
      "person: 1.00 [1.21, 32.98, 179.92, 253.96]\n",
      "person: 1.00 [180.18, 54.78, 318.81, 239.11]\n",
      "person: 1.00 [9.61, 38.79, 639.13, 472.02]\n",
      "person: 1.00 [1.54, 51.77, 316.49, 325.46]\n",
      "person: 1.00 [340.6, 76.51, 370.03, 98.49]\n",
      "person: 1.00 [17.32, 215.85, 218.81, 472.48]\n",
      "person: 1.00 [1.8, 260.97, 74.21, 472.16]\n",
      "person: 1.00 [162.18, 101.62, 638.44, 474.12]\n",
      "person: 1.00 [0.21, -0.03, 640.19, 121.75]\n",
      "person: 1.00 [8.45, 243.09, 194.34, 468.54]\n",
      "person: 1.00 [0.37, 30.73, 83.28, 471.32]\n",
      "person: 1.00 [336.06, 77.69, 440.21, 353.98]\n",
      "person: 1.00 [343.71, 98.27, 585.57, 352.21]\n",
      "person: 1.00 [569.03, 54.91, 638.27, 87.99]\n",
      "person: 1.00 [349.85, 199.06, 580.5, 371.36]\n",
      "person: 1.00 [-5.68, 0.19, 615.74, 478.37]\n",
      "person: 1.00 [19.14, 66.82, 596.88, 306.63]\n",
      "person: 1.00 [-0.11, 57.04, 49.94, 226.28]\n",
      "person: 1.00 [-7.56, -21.22, 319.02, 381.06]\n",
      "person: 1.00 [-1.29, 53.2, 309.89, 473.24]\n",
      "person: 1.00 [12.82, 15.03, 248.0, 322.41]\n",
      "person: 1.00 [516.15, 119.23, 639.08, 474.0]\n",
      "person: 1.00 [18.63, 199.56, 221.1, 473.27]\n",
      "person: 1.00 [497.05, 117.41, 639.74, 473.1]\n",
      "person: 1.00 [9.79, 118.04, 297.19, 454.02]\n",
      "person: 1.00 [3.34, 358.19, 641.11, 477.52]\n",
      "person: 1.00 [172.61, 57.34, 320.07, 239.83]\n",
      "person: 1.00 [1.02, 54.99, 183.33, 473.55]\n",
      "person: 1.00 [55.08, 396.24, 425.23, 477.13]\n",
      "person: 1.00 [41.17, 76.11, 106.77, 116.85]\n",
      "person: 1.00 [6.81, 44.78, 39.15, 108.71]\n",
      "person: 1.00 [175.53, 55.39, 319.92, 238.32]\n",
      "person: 1.00 [143.4, 25.21, 636.62, 372.52]\n",
      "person: 1.00 [233.7, 53.43, 382.07, 192.45]\n",
      "person: 1.00 [348.71, 189.82, 580.6, 371.47]\n",
      "person: 1.00 [220.17, 54.97, 315.42, 141.13]\n",
      "person: 1.00 [0.11, 2.01, 213.73, 472.97]\n",
      "person: 1.00 [569.18, 105.69, 632.75, 223.87]\n",
      "person: 1.00 [347.64, 129.3, 581.54, 372.06]\n",
      "person: 1.00 [36.19, 57.23, 315.95, 240.78]\n",
      "person: 1.00 [333.96, 77.02, 369.67, 186.11]\n",
      "person: 1.00 [8.43, 161.46, 631.11, 443.6]\n",
      "person: 1.00 [349.32, 197.01, 585.32, 310.25]\n",
      "person: 1.00 [179.19, 9.84, 637.71, 314.29]\n",
      "person: 1.00 [567.7, 54.52, 638.05, 87.81]\n",
      "person: 1.00 [343.33, 76.47, 368.09, 116.57]\n",
      "person: 1.00 [334.01, 25.98, 640.51, 210.25]\n",
      "person: 1.00 [-0.24, -0.37, 476.16, 58.04]\n",
      "person: 1.00 [365.76, 72.45, 388.16, 97.67]\n",
      "person: 1.00 [5.58, 48.1, 556.11, 321.49]\n",
      "person: 1.00 [479.51, 0.24, 639.99, 138.34]\n",
      "person: 1.00 [112.97, 54.48, 320.09, 319.25]\n",
      "person: 1.00 [96.5, 170.52, 591.23, 385.25]\n",
      "person: 1.00 [18.35, 208.82, 634.48, 435.54]\n",
      "person: 1.00 [203.47, 54.36, 315.21, 181.25]\n",
      "person: 1.00 [6.79, 38.45, 95.96, 202.38]\n",
      "person: 1.00 [35.03, 83.94, 585.53, 336.66]\n",
      "person: 1.00 [-1.06, 46.68, 322.56, 431.87]\n",
      "person: 1.00 [333.98, 76.84, 369.62, 186.06]\n",
      "person: 1.00 [340.02, 76.48, 582.81, 371.45]\n",
      "person: 1.00 [115.9, 44.38, 574.69, 256.77]\n",
      "person: 1.00 [347.19, 182.82, 584.57, 326.97]\n",
      "person: 1.00 [292.2, 99.52, 312.06, 109.58]\n",
      "person: 1.00 [130.86, 25.02, 634.26, 374.85]\n",
      "person: 1.00 [20.14, -56.34, 569.14, 422.56]\n",
      "person: 1.00 [493.6, 23.8, 639.67, 203.12]\n",
      "person: 1.00 [340.95, 90.61, 366.48, 126.22]\n",
      "person: 1.00 [338.55, 76.9, 369.83, 129.09]\n",
      "person: 1.00 [0.4, -1.09, 397.33, 47.79]\n",
      "person: 1.00 [-5.2, 183.06, 631.77, 380.65]\n",
      "person: 1.00 [2.09, 53.29, 318.05, 266.63]\n",
      "person: 1.00 [-17.94, -90.57, 591.21, 388.28]\n",
      "person: 1.00 [494.47, 0.84, 640.16, 158.92]\n",
      "person: 1.00 [39.41, 20.58, 622.99, 320.53]\n",
      "person: 1.00 [349.8, 216.39, 586.93, 371.11]\n",
      "person: 1.00 [3.41, 72.11, 168.76, 463.4]\n",
      "person: 1.00 [333.96, 127.41, 354.65, 186.02]\n",
      "person: 1.00 [21.7, -50.29, 546.78, 413.27]\n",
      "person: 1.00 [349.67, 201.48, 579.65, 371.36]\n",
      "person: 1.00 [44.24, 73.55, 177.53, 102.62]\n",
      "person: 1.00 [260.45, 100.7, 309.76, 141.39]\n",
      "person: 1.00 [334.24, 127.85, 354.47, 185.53]\n",
      "person: 1.00 [11.41, 167.25, 587.53, 383.86]\n",
      "person: 1.00 [338.72, 76.53, 369.79, 121.35]\n",
      "person: 1.00 [334.4, 154.26, 351.33, 186.84]\n",
      "person: 1.00 [0.52, 0.42, 638.42, 86.97]\n",
      "person: 1.00 [336.65, 74.82, 380.12, 162.23]\n",
      "person: 1.00 [248.96, 27.01, 637.61, 474.17]\n",
      "person: 1.00 [313.42, 59.9, 500.07, 257.06]\n",
      "person: 1.00 [5.76, 6.61, 291.94, 327.65]\n",
      "person: 1.00 [113.73, 21.36, 633.36, 379.54]\n",
      "person: 1.00 [16.32, 82.46, 68.45, 241.11]\n",
      "person: 1.00 [516.34, -4.28, 593.78, 48.09]\n",
      "person: 1.00 [1.01, 0.37, 207.69, 239.27]\n",
      "person: 1.00 [-0.22, 52.25, 70.18, 130.13]\n",
      "person: 1.00 [26.83, 323.37, 73.32, 471.25]\n",
      "person: 1.00 [352.95, 25.03, 640.83, 201.57]\n",
      "person: 1.00 [-0.55, -1.35, 305.24, 42.25]\n",
      "person: 1.00 [358.3, 24.66, 640.36, 209.21]\n",
      "person: 1.00 [4.82, 60.29, 387.41, 474.41]\n",
      "person: 1.00 [442.45, 218.22, 493.46, 290.22]SolvedFixes a critical bug in thepost_process_object_detectionscores = torch.nn.functional.softmax(out_logits)[:, :, :-1]->scores = torch.nn.functional.softmax(out_logits,dim-1)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_39.txt:\n",
      "Title: Supprot for qwen2moe gguf models\n",
      "URL: https://github.com/huggingface/transformers/issues/33243\n",
      "Body:\n",
      "System Infotransformersversion: 4.45.0.dev0Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.35Python version: 3.10.14Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    - compute_environment: LOCAL_MACHINE- distributed_type: DEEPSPEED- use_cpu: False- debug: True- num_processes: 24- machine_rank: 2- num_machines: 3- main_process_ip: gpu007- main_process_port: 9901- rdzv_backend: static- same_network: True- main_training_function: main- enable_cpu_affinity: False- deepspeed_config: {'deepspeed_config_file': '/data/vayu/train/config/deepspeed/zero2.json', 'deepspeed_hostfile': '/data/vayu/train/config/hostfile', 'deepspeed_multinode_launcher': 'pdsh', 'zero3_init_flag': False}- downcast_bf16: no- tpu_use_cluster: False- tpu_use_sudo: False- tpu_env: []PyTorch version (GPU?): 2.4.0 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA A800-SXM4-80GBWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoModelForCausalLMmodel_id = \"./qwen2moe_4x1.5b/\"file_name = \"Qwen2-4x1.5B-reasoning-pro-Q4_K_M.gguf\" # local file, base on Qwen/Qwen2-1.5B-Instructmodel = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)Expected behaviorminiconda3/envs/vllm_cu12/lib/python3.10/site-packages/transformers/modeling_gguf_pytorch_utils.py\", line 100, in load_gguf_checkpointraise ValueError(f\"Architecture {architecture} not supported\")ValueError: Architecture qwen2moe not supported\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_388.txt:\n",
      "Title: Training hangs at the first gradient syncing of an MoE model while using deepspeed\n",
      "URL: https://github.com/huggingface/transformers/issues/30911\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.3Platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35Python version: 3.12.2Huggingface_hub version: 0.22.2Safetensors version: 0.4.2Accelerate version: 0.28.0PyTorch version (GPU?): 2.2.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI'm training a customized MoE language model using 8 GPUs on one node, and it works fine using Accelerate (without DeepSpeed). However, when I enable DeepSpeed (both ZerO 1 and 2), the training hangs at the first gradient syncing and will end up with an NCCL timeout after a few minutes.The MoE is built based on the XGLM architecture and the task as language modeling.DeepSpeed works fine when I train a dense model (not MoE).I also setdeepspeed_moe_layer_cls_namesto my MoE block, but it doesn't seem to work.I use theaccelerate launchcommand to run my experiments.I guess the problem with MoE is that not all GPUs use the same parameters (experts) in one forward pass, and that's why GPUs are waiting for each other to receive all gradients.Expected behaviorThe gradient syncing and training continue properly, and we are able to train an MoE (sparse network) using DeepSpeed.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_363.txt:\n",
      "Title: Add support for non-CUDA architectures at the same time Bitsandbytes is doing it\n",
      "URL: https://github.com/huggingface/transformers/issues/31248\n",
      "Body:\n",
      "Feature requestCurrently, the helper/setup functions explicitly check for CUDA support:transformers/src/transformers/quantizers/quantizer_bnb_4bit.pyLines 60 to 63\n",
      "      in8685b3cdefvalidate_environment(self,*args,**kwargs):ifnottorch.cuda.is_available():raiseRuntimeError(\"No GPU found. A GPU is needed for quantization.\")ifnot(is_accelerate_available()andis_bitsandbytes_available()):BNB is currently doing a project to enable support for other GPU backends:ALPHA TESTERS WANTEDMotivationApple MPS support is being added for so many major players, it'd be great for the biggest one of all to support it as its dependencies do. Also would be good to not hard-code this kind of limitation so that code updates aren't necessary as dependent libraries update themselves...Your contributionidea done\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_405.txt:\n",
      "Title: Special token handling breaks idempotency of sentencepiece due to extra spaces\n",
      "URL: https://github.com/huggingface/transformers/issues/31513\n",
      "Body:\n",
      "Sentenpiece tokenizers have the property thatDecode(Encode(Normalize(input))) == Normalize(input).. This property is very useful when combining and re-inferring prompts. However, when used throughtokenizerswith special tokens added for BOS/EOS etc,tokenizerswill inject an extra space around special tokens when decoding - i.e,<s>Awill become<s> A, which when encoded and decoded will become<s>  A,<s>   A, etc.A previous issue was raised about this but incorrectly closed as intended behavior/unfixable:huggingface/tokenizers#1237. Although not all tokenizers have this property, sentencepiece is very widely used now due to llama and mistral so it would make sense for this behavior to be preserved.There could be two fixes for this: either not add the extra space, or tokenize<s> Athe same as<s>A(i think could be accomplished by changing theAddedTokenparams for these tokens.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_411.txt:\n",
      "Title: Question about quantized model with zero3\n",
      "URL: https://github.com/huggingface/transformers/issues/30663\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.0.dev0Platform: Linux-5.15.0-92-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.21.4Safetensors version: 0.4.2Accelerate version: 0.28.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.2.0a0+81ea7a4 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionbnb_config = BitsAndBytesConfig(\n",
      "            load_in_4bit=True,\n",
      "            bnb_4bit_quant_type=\"nf4\",\n",
      "            bnb_4bit_compute_dtype=\"bfloat16\",\n",
      "            bnb_4bit_quant_storage=\"bfloat16\",\n",
      "        )\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "            args.model_name_or_path,\n",
      "            torch_dtype=torch.bfloat16,\n",
      "            trust_remote_code=True,\n",
      "            quantization_config=bnb_config,\n",
      "            attn_implementation=\"flash_attention_2\" if args.use_flash_attn else \"eager\",\n",
      "        )I found that when fine-tune a quantized model using trainer withZero3, the quantized model will  be loaded all to the GPU first, and then partitioning the parameters across data-parallel processes.What if there is not enough memory to load the whole quantized model?The code that load all quantized model is in the deepspeed/runtime/engine.py about line262:self._configure_distributed_model(model)It was entered from transformers trainer: inner_train_loop: about line1082:model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)Expected behaviorHow to partitioning the parameters during load from_pretrain instead of in trainer? like load float model.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_377.txt:\n",
      "Title: Whether the OutEffHop can support with Transfomers\n",
      "URL: https://github.com/huggingface/transformers/issues/31046\n",
      "Body:\n",
      "Feature requestWe request that the option \"OutEffHop\" be included in the BERT, OPT, and ViT models.MotivationI am the author of OutEffHop and we plan to release our OutEffHop-based model on HuggingFace. However, because we use a different activation function (not softmax) in attention, if we directly use the original structure, the model inference online will show a huge problem.Your contributionWe are able to supply the code for OutEffHop and assist with integrating the function into BERT and OPT. Additionally, if necessary, we can also help incorporate OutEffHop into ViT. My initial idea is to add an option in the model configuration that allows the user to choose whether to use OutEffHop. If they opt to use it, they can employ the new activation function.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_217.txt:\n",
      "Title: Unexpected results of the lm_head when averaging model parameters\n",
      "URL: https://github.com/huggingface/transformers/issues/32272\n",
      "Body:\n",
      "System Infotransformers==4.32.1torch=2.0.1Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionIn a recursive way of averaging model parameters using torch.distributed as dist.all_reduce(parameter, op=dist.ReduceOp.SUM) and then dividing the value by the number of workers, the result of the lm_head is actually the sum.This bug may not only exist in the distributed allreduce, but also in other sum-divide averaging situations, like SWA algorithm which is frequently used in optimization, or the federated learning.This bug is caused by lm_head parameters reusinghttps://discuss.huggingface.co/t/why-is-the-lm-head-layer-in-gpt2lmheadmodel-not-a-parameter/639.Specifically, the lm_head reuses the parameters of the transformer.wte. During the recursive all_reduce, thedist.all_reduce(avg_params[name], op=dist.ReduceOp.SUM)will sum both parameters of transformer.wte and the lm_head. However, when executingavg_params[name] = avg_params[name] / dist.get_world_size()of the transformer.wte, only the transformer.wte is averaged, because the number of references of avg_params[name] is at least two:avg_params[\"transformer.wte.weight\"]andavg_params[\"lm_head.weight\"].This results in the averaging error of lm_head, which actually is summation.One simplest way to avoid this problem is to use torch.distributed as dist.all_reduce(parameter, op=dist.ReduceOp.AVG). However, this limits many usage scenarios. Furthermore, this bug may cause other similar problems in the similar situation.The minimal and fast reproducible example. Please use torch.distributed with 4 workers to launch it:import torch\n",
      "import torch.distributed as dist\n",
      "import logging\n",
      "import os\n",
      "import socket\n",
      "\n",
      "from copy import deepcopy\n",
      "\n",
      "from transformers import (GPT2Config, \n",
      "                          AutoModelForCausalLM)\n",
      "\n",
      "dist.init_process_group(backend='nccl', init_method='env://')\n",
      "rank = dist.get_rank()\n",
      "print(f'os.environ[LOCAL_RANK]: {os.environ[\"LOCAL_RANK\"]}')\n",
      "hostname = socket.gethostname() \n",
      "logger = logging.getLogger(hostname)\n",
      "logger.setLevel(logging.INFO)\n",
      "\n",
      "strhdlr = logging.StreamHandler()\n",
      "logger.addHandler(strhdlr)\n",
      "formatter = logging.Formatter('%(asctime)s [%(filename)s:%(lineno)d] %(levelname)s %(message)s')\n",
      "strhdlr.setFormatter(formatter)\n",
      "\n",
      "selected_gpu = rank % 4\n",
      "torch.cuda.set_device(selected_gpu)\n",
      "\n",
      "dnn=\"gpt2\"\n",
      "model_dir=\"/data2/share/zhtang/gpt2\"\n",
      "\n",
      "config = GPT2Config.from_pretrained(dnn, cache_dir=model_dir)\n",
      "print(config)\n",
      "config.max_position_embeddings = 32\n",
      "config.num_hidden_layers = 2\n",
      "config.hidden_size = 32\n",
      "config.num_attention_heads = 2\n",
      "config.num_key_value_heads = 2\n",
      "net = AutoModelForCausalLM.from_config(config)\n",
      "# param = net.transformer.wpe\n",
      "param = net.lm_head\n",
      "\n",
      "net.to(selected_gpu)\n",
      "\n",
      "# Add 1 to make results more clear.\n",
      "for name, param in net.named_parameters():\n",
      "    shape = param.data.shape\n",
      "    param.data = param.data + torch.normal(mean=1.0, std=0.5, size=shape, device=param.data.device)\n",
      "\n",
      "def is_root():\n",
      "    return dist.get_rank() == 0 \n",
      "def allreduce_model_weights_SUM_DIV(model):\n",
      "    if isinstance(model, dict):\n",
      "        avg_params = deepcopy(model)\n",
      "    else:\n",
      "        state = model.state_dict()\n",
      "        avg_params = deepcopy(state)\n",
      "    for name, param in avg_params.items():\n",
      "        logger.info(f'Before {name}, lm_head[10]:{avg_params[\"lm_head.weight\"][0,:5]}  ')\n",
      "        dist.all_reduce(avg_params[name], op=dist.ReduceOp.SUM)\n",
      "        avg_params[name] = avg_params[name] / dist.get_world_size()\n",
      "        logger.info(f'After {name}, lm_head[10]:{avg_params[\"lm_head.weight\"][0,:5]}  ')\n",
      "    return avg_params\n",
      "\n",
      "def allreduce_model_weights_AVG(model):\n",
      "    if isinstance(model, dict):\n",
      "        avg_params = deepcopy(model)\n",
      "    else:\n",
      "        state = model.state_dict()\n",
      "        avg_params = deepcopy(state)\n",
      "    for name, param in avg_params.items():\n",
      "        logger.info(f'Before {name}, lm_head[10]:{avg_params[\"lm_head.weight\"][0,:5]}  ')\n",
      "        dist.all_reduce(avg_params[name], op=dist.ReduceOp.AVG)\n",
      "        logger.info(f'After {name} , lm_head[10]:{avg_params[\"lm_head.weight\"][0,:5]}  ')\n",
      "\n",
      "    return avg_params\n",
      "\n",
      "logger.info(\"In SUM DIV\")\n",
      "allreduce_model_weights_SUM_DIV(net)\n",
      "logger.info(\"In AVG\")\n",
      "allreduce_model_weights_AVG(net)Expected behaviorFunctions in the given examplesallreduce_model_weights_SUM_DIV(model)allreduce_model_weights_AVG(model)should have same outputs.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_571.txt:\n",
      "Title: Missingvocab_fileAttribute When Using Custom SentencePiece Models\n",
      "URL: https://github.com/huggingface/transformers/issues/28370\n",
      "Body:\n",
      "System Infotransformersversion: 4.36.2Platform: Linux-6.1.69-1-lts-x86_64-with-glibc2.38Python version: 3.11.6Huggingface_hub version: 0.19.4Safetensors version: 0.4.0Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.1.2+cpu (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: NAUsing distributed or parallel set-up in script?: NAWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Steps to ReproduceImport the SentencePiece library:importsentencepieceasspmVerify the SentencePiece version:spm.__version__# Ensure it is 0.1.98 or laterAttempt to create aLlamaConverterinstance with a loaded SentencePiece tokenizer:fromtransformers.convert_slow_tokenizerimportLlamaConverter# Define the path to your SentencePiece modelspm_model_path=\"path/to/your/tokenizer.model\"# Load the SentencePiece tokenizerspm_tokenizer=spm.SentencePieceProcessor()spm_tokenizer.Load(spm_model_path)# Create a LlamaConverter instance with the loaded tokenizerllama_converter=LlamaConverter(spm_tokenizer)Observe the error raised due to the missingvocab_fileattribute.Expected BehaviorTo address this issue and make it easier for users to work with custom SentencePiece models, the following modifications are proposed:ModifyConverterClass:Update theConverterclass to accept a file path to the SentencePiece model when initializing an instance of the class.Instantiate the SentencePiece tokenizer and load the model from the provided file path within theConverterclass.importsentencepieceasspmclassConverter:def__init__(self,file_path_to_tokenizer):self.file_path=file_path_to_tokenizerself.original_tokenizer=spm.SentencePieceProcessor()self.original_tokenizer.Load(self.file_path)defconverted(self)->Tokenizer:raiseNotImplementedError()UpdateSpmConverterClass:Update theSpmConverterclass to use thefile_pathattribute instead of the non-existentmodel_fileattribute when loading the SentencePiece model.Ensure compatibility with the modifications made in theConverterclass.fromtransformers.convert_slow_tokenizerimportConverterclassSpmConverter(Converter):def__init__(self,file_path_to_tokenizer):requires_backends(self,\"protobuf\")super().__init__(file_path_to_tokenizer)# Load the SentencePiece model using the file pathwithopen(self.file_path,\"rb\")asf:m.ParseFromString(f.read())Expected OutcomeWith these proposed modifications, users should be able to createConverterinstances with their custom SentencePiece models by providing the file path, and theSpmConverterclass should correctly load the model using the specified file path, resolving the issue of missing attributes and ensuring compatibility with Transformers.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_565.txt:\n",
      "Title: Proposal for Adding a New Scheduler Strategy for Language Model Pretraining\n",
      "URL: https://github.com/huggingface/transformers/issues/28441\n",
      "Body:\n",
      "Feature requestWe try to propose the addition of a new and widely-adopted scheduler strategy for language model pretraining in the Transformers repository. Upon reviewing the current schedulers available in theTransformers optimization module, it appears there is a notable absence of an out-of-the-box implementation for a specific type of scheduler. This particular scheduler is prevalent in recent pre-training models and features a warmup decay, but importantly, it also maintains a limited minimum learning rate post-maximum iteration steps.This scheduling approach has seen extensive use in several prominent pre-trained large language models (LLMs), including:TinyLLaMA: Implementation details can be found in theirpretraining script.MindLLM: Described in their research paper, available atarXiv:2310.15777.trlx: Utilized in the TRLx framework, as seen in theirGitHub repository....The introduction of this scheduler into the Transformers library would not only complete the suite of existing scheduling strategies but also provide practitioners with a tool that's already proven its efficacy in recent LLM training methodologies. I believe its inclusion will be beneficial for the community, fostering more efficient and effective pretraining processes.MotivationThis issue aims to introduce a novel scheduler into the current Transformers library. The proposed scheduler combines the elements of warmup decay with a distinctive feature - the implementation of a constrained minimum learning rate beyond the maximum iteration steps.Your contributionYes, we could submit a PR as soon as possible if any huggingface members think this contribution is necessary.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_203.txt:\n",
      "Title: Loading GroundingDinoForObjectDetection error\n",
      "URL: https://github.com/huggingface/transformers/issues/32353\n",
      "Body:\n",
      "System Infotransformers 4.44.0.dev0python 3.8.19torch 2.3.1+cu118Who can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionhi, I am trying grounded_sam implemented with transformers followinghttps://github.com/NielsRogge/Transformers-Tutorials/blob/master/Grounding%20DINO/GroundingDINO_with_Segment_Anything.ipynbhowever, when I am loading the modelfrom transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline\n",
      "device = \"cuda\"\n",
      "detector_id = detector_id if detector_id is not None else \"IDEA-Research/grounding-dino-tiny\"\n",
      "object_detector = pipeline(model=detector_id, task=\"zero-shot-object-detection\", device=device)some error are encounteredSome weights of the model checkpoint at models--IDEA-Research--grounding-dino-base were not used when initializing GroundingDinoForObjectDetection: ['model.decoder.layers.0.encoder_attn_text.in_proj_bias', 'mo\n",
      "del.decoder.layers.0.encoder_attn_text.in_proj_weight', 'model.decoder.layers.0.self_attn.in_proj_bias', 'model.decoder.layers.0.self_attn.in_proj_weight', 'model.decoder.layers.1.encoder_attn_text.in_proj_bias', 'model.decoder.layers.1.encoder_att\n",
      "n_text.in_proj_weight', 'model.decoder.layers.1.self_attn.in_proj_bias', 'model.decoder.layers.1.self_attn.in_proj_weight', 'model.decoder.layers.2.encoder_attn_text.in_proj_bias', 'model.decoder.layers.2.encoder_attn_text.in_proj_weight', 'model.d\n",
      "ecoder.layers.2.self_attn.in_proj_bias', 'model.decoder.layers.2.self_attn.in_proj_weight', 'model.decoder.layers.3.encoder_attn_text.in_proj_bias', 'model.decoder.layers.3.encoder_attn_text.in_proj_weight', 'model.decoder.layers.3.self_attn.in_pro\n",
      "j_bias', 'model.decoder.layers.3.self_attn.in_proj_weight', 'model.decoder.layers.4.encoder_attn_text.in_proj_bias', 'model.decoder.layers.4.encoder_attn_text.in_proj_weight', 'model.decoder.layers.4.self_attn.in_proj_bias', 'model.decoder.layers.4\n",
      ".self_attn.in_proj_weight', 'model.decoder.layers.5.encoder_attn_text.in_proj_bias', 'model.decoder.layers.5.encoder_attn_text.in_proj_weight', 'model.decoder.layers.5.self_attn.in_proj_bias', 'model.decoder.layers.5.self_attn.in_proj_weight', 'mod\n",
      "el.encoder.layers.0.fusion_layer.weight_l', 'model.encoder.layers.0.fusion_layer.weight_v', 'model.encoder.layers.0.text_enhancer_layer.self_attn.in_proj_bias', 'model.encoder.layers.0.text_enhancer_layer.self_attn.in_proj_weight', 'model.encoder.l\n",
      "ayers.1.fusion_layer.weight_l', 'model.encoder.layers.1.fusion_layer.weight_v', 'model.encoder.layers.1.text_enhancer_layer.self_attn.in_proj_bias', 'model.encoder.layers.1.text_enhancer_layer.self_attn.in_proj_weight', 'model.encoder.layers.2.fusi\n",
      "on_layer.weight_l', 'model.encoder.layers.2.fusion_layer.weight_v', 'model.encoder.layers.2.text_enhancer_layer.self_attn.in_proj_bias', 'model.encoder.layers.2.text_enhancer_layer.self_attn.in_proj_weight', 'model.encoder.layers.3.fusion_layer.wei\n",
      "ght_l', 'model.encoder.layers.3.fusion_layer.weight_v', 'model.encoder.layers.3.text_enhancer_layer.self_attn.in_proj_bias', 'model.encoder.layers.3.text_enhancer_layer.self_attn.in_proj_weight', 'model.encoder.layers.4.fusion_layer.weight_l', 'mod\n",
      "el.encoder.layers.4.fusion_layer.weight_v', 'model.encoder.layers.4.text_enhancer_layer.self_attn.in_proj_bias', 'model.encoder.layers.4.text_enhancer_layer.self_attn.in_proj_weight', 'model.encoder.layers.5.fusion_layer.weight_l', 'model.encoder.l\n",
      "ayers.5.fusion_layer.weight_v', 'model.encoder.layers.5.text_enhancer_layer.self_attn.in_proj_bias', 'model.encoder.layers.5.text_enhancer_layer.self_attn.in_proj_weight', 'model.input_proj_text.bias', 'model.input_proj_text.weight', 'model.text_ba\n",
      "ckbone.pooler.dense.bias', 'model.text_backbone.pooler.dense.weight']\n",
      "- This IS expected if you are initializing GroundingDinoForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model)\n",
      ".Expected behaviorexpect everything works well\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_968.txt:\n",
      "Title: Routing Transformers / Add Google PG-19 Models\n",
      "URL: https://github.com/huggingface/transformers/issues/11686\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_954.txt:\n",
      "Title: Add TFSpeech2Text\n",
      "URL: https://github.com/huggingface/transformers/issues/12269\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_940.txt:\n",
      "Title: Add Flax Models to Pipelines\n",
      "URL: https://github.com/huggingface/transformers/issues/12627\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_798.txt:\n",
      "Title: Add TensorFlow Wav2Vec2 for sequence classification\n",
      "URL: https://github.com/huggingface/transformers/issues/21778\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_773.txt:\n",
      "Title: Add LLaVA model\n",
      "URL: https://github.com/huggingface/transformers/issues/22848\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_767.txt:\n",
      "Title: VideoMAEForVideoClassification does not supportdevice_map='auto'yet.\n",
      "URL: https://github.com/huggingface/transformers/issues/23086\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_997.txt:\n",
      "Title: Request to add Switch Transformer\n",
      "URL: https://github.com/huggingface/transformers/issues/10234\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_996.txt:\n",
      "Title: [Tensor Parallelism] Megatron-LM to transformers\n",
      "URL: https://github.com/huggingface/transformers/issues/10321\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_766.txt:\n",
      "Title: Detr Models cannot be loaded withdevice_map=\"auto\"\n",
      "URL: https://github.com/huggingface/transformers/issues/23145\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_799.txt:\n",
      "Title: Add TensorFlow Whisper model for audio classification\n",
      "URL: https://github.com/huggingface/transformers/issues/21777\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_955.txt:\n",
      "Title: Modify BERT encoder layers?\n",
      "URL: https://github.com/huggingface/transformers/issues/12241\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_969.txt:\n",
      "Title: Key Error: 'pre-processing' during conversion from tatoeba to Marian model\n",
      "URL: https://github.com/huggingface/transformers/issues/11647\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_558.txt:\n",
      "Title: Add [VMamba] model\n",
      "URL: https://github.com/huggingface/transformers/issues/28606\n",
      "Body:\n",
      "Model descriptionVMamba is a visual foundation model proposed inhttps://arxiv.org/pdf/2401.10166.pdf.It is inspired by the recent advances in state stace models and in particular Mamba. The proposed architecture is computationally more efficient than vision transformer architectures because it scales linearly with growing resolution. It introduces a Cross-Scan Module (CSM) to have context from all directions (4 directions, starting in each corner and traversing in a horizontal or vertical direction). Evaluation on vision perception tasks shows promising capabilities.Model weights will become available in a few days according to the repo of the authors.(Optional) Understood theoretical aspectsPrepared transformers dev environmentSet up debugging environment of the original repositoryCreated script that successfully runs forward pass usingoriginal repository and checkpointSuccessfully opened a PR and added the model skeleton to TransformersSuccessfully converted original checkpoint to TransformerscheckpointSuccessfully ran forward pass in Transformers that givesidentical output to original checkpointFinished model tests in TransformersSuccessfully added Tokenizer in TransformersRun end-to-end integration testsFinished docsUploaded model weights to the hubSubmitted the pull request for review(Optional) Added a demo notebookI am opening the issue to avoid duplicate work. My main motivation for porting this model is to learn a bit more about it (and about the internals of 🤗 Transformers). Some of you probably know this library much better than me, so feel free to write your own implementation if you can do it better or quicker. Otherwise, don’t hesitate to build on top of my fork.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationOriginal repo:https://github.com/MzeroMiko/VMambaPaper:https://arxiv.org/pdf/2401.10166.pdfimplementation in progress:youtube vmamba vs vision mamba:https://www.youtube.com/watch?v=RtHDu6kFPb8vision mamba paper (similar idea):https://arxiv.org/pdf/2401.09417.pdf\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_564.txt:\n",
      "Title: get_importsfailing to respect conditionals on imports\n",
      "URL: https://github.com/huggingface/transformers/issues/28459\n",
      "Body:\n",
      "System Infotransformersversion: 4.36.2Platform: macOS-13.5.2-arm64-arm-64bitPython version: 3.11.7Huggingface_hub version: 0.20.2Safetensors version: 0.4.1Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.1.2 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: noUsing distributed or parallel set-up in script?: noWho can help?Fromgit blame:@Wauplin@sguggerFrom issue template (it's a LLM):@ArthurZucker@youInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRunning the below snippet on a MacBook without an Nvidia GPU andtransformers==4.36.2will throw anImportErrortopip install flash_attn. However,flash_attnisn't actually a requirement for this model, so something's off here.fromtransformersimportAutoModelForCausalLMmodel=AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\",trust_remote_code=True)Leads to:File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 315, in get_cached_module_file\n",
      "    modules_needed = check_imports(resolved_module_file)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\", line 180, in check_imports\n",
      "    raise ImportError(\n",
      "ImportError: This modeling file requires the following packages that were not found in your environment: flash_attn. Run `pip install flash_attn`\n",
      "python-BaseExceptionInvestigating this, it seemshttps://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/dynamic_module_utils.py#L154is picking upflash_attnfromhttps://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/phi/modeling_phi.py#L50-L52. However, if you look at the file, it's within anifstatement.Therein is the bug, thattransformers.dynamic_module_utils.get_importsis not respecting conditionals before imports.Please seehttps://huggingface.co/microsoft/phi-1_5/discussions/72for more info.Expected behaviorMy goal is some way to avoid monkey patchingget_importsto remove the extra inferredflash_attndependency.The most generalized solution is probably movingget_importsfrom regex searching the source to either useinspect(seehere) or some other AST walking method. I am pretty sure there is a simple fix here, it just involves moving away from a regex.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_202.txt:\n",
      "Title: PretrainedModule.save_pretrained(safe_serialization=False)does not work with PyTorch wrapper tensor subclasses\n",
      "URL: https://github.com/huggingface/transformers/issues/32364\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.3Platform: Linux-5.12.0-0_fbk16_zion_7661_geb00762ce6d2-x86_64-with-glibc2.34Python version: 3.10.12Huggingface_hub version: 0.24.5Safetensors version: 0.4.3Accelerate version: 0.22.0Accelerate config:    not foundPyTorch version (GPU?): 2.5.0a0+gitc35f21e (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: noUsing GPU in script?: noGPU type: NVIDIA PG509-210Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionsafe_serialization=Falseuses pytorchtorch.saveserialization, which should work with wrapper tensor subclasses. However, the huggingface internal utils for this are accessingtensor.storage().data_ptr(), which does not work for wrapper tensor subclasses.The specific use case istorchaoquantized tensors that are implemented as wrapper tensor subclasses. The following is a minimal repro with a basic wrapper tensor subclass,TwoTensor, but this should apply for any tensor subclass fromtorchaothat is used as the model parametersimporttorchfromtorch.testing._internal.two_tensorimportTwoTensorfromtransformersimportAutoModelForSequenceClassification,AutoTokenizer# Load pre-trained model and tokenizermodel=AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")# Convert every parameter/buffer to a wrapper tensor subclass (TwoTensor) for demonstration purposesmodel._apply(lambdat:TwoTensor(t,t))# Save the model and tokenizer to a directoryoutput_dir=\"./my-bert-model\"model.save_pretrained(output_dir,safe_serialization=False)Click for stack tracereturn tensor.storage().data_ptr()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mg1998/local/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/huggingface_hub/serialization/_torch.py\", line 406, in storage_ptr\n",
      "    return tensor.untyped_storage().data_ptr()\n",
      "RuntimeError: Attempted to access the data pointer on an invalid python storage.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/users/mg1998/pytorch/test_transformers.py\", line 11, in <module>\n",
      "    model.save_pretrained(output_dir, safe_serialization=False)\n",
      "  File \"/home/mg1998/local/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2691, in save_pretrained\n",
      "    state_dict_split = split_torch_state_dict_into_shards(\n",
      "  File \"/home/mg1998/local/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/huggingface_hub/serialization/_torch.py\", line 330, in split_torch_state_dict_into_shards\n",
      "    return split_state_dict_into_shards_factory(\n",
      "  File \"/home/mg1998/local/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/huggingface_hub/serialization/_base.py\", line 108, in split_state_dict_into_shards_factory\n",
      "    storage_id = get_storage_id(tensor)\n",
      "  File \"/home/mg1998/local/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/huggingface_hub/serialization/_torch.py\", line 359, in get_torch_storage_id\n",
      "    unique_id = storage_ptr(tensor)\n",
      "  File \"/home/mg1998/local/miniconda3/envs/pytorch-3.10/lib/python3.10/site-packages/huggingface_hub/serialization/_torch.py\", line 410, in storage_ptr\n",
      "    return tensor.storage().data_ptr()\n",
      "  File \"/data/users/mg1998/pytorch/torch/storage.py\", line 1220, in data_ptr\n",
      "    return self._data_ptr()\n",
      "  File \"/data/users/mg1998/pytorch/torch/storage.py\", line 1224, in _data_ptr\n",
      "    return self._untyped_storage.data_ptr()\n",
      "RuntimeError: Attempted to access the data pointer on an invalid python storage.Expected behaviorIt looks like the code in question creates a unique id for the tensors storage via the data_ptr. The outer tensor of the wrapper subclass does not have a \"real\" storage, so we expect the access to the storage data_ptr to fail. However, wrapper tensor subclasses have \"inner\" tensors that have \"real\" storages though, so this could be an option for getting an id.cc@jerryzh168\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_216.txt:\n",
      "Title: ChineseTextModel: Weight loading error\n",
      "URL: https://github.com/huggingface/transformers/issues/32280\n",
      "Body:\n",
      "System InfoSome weights of the model checkpoint at OFA-Sys/chinese-clip-vit-base-patch16 were not used when initializing ChineseCLIPTextModel: ['logit_scale', 'text_model.embeddings.LayerNorm.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.embeddings.position_ids', 'text_model.embeddings.token_type_embeddings.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_projection.weight', 'vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.post_layernorm.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.pre_layrnorm.weight', 'visual_projection.weight']This IS expected if you are initializing ChineseCLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).This IS NOT expected if you are initializing ChineseCLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of ChineseCLIPTextModel were not initialized from the model checkpoint at OFA-Sys/chinese-clip-vit-base-patch16 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoTokenizer, ChineseCLIPTextModelimport torchtokenizer = AutoTokenizer.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")model = ChineseCLIPTextModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")outputs = model(**inputs)last_hidden_states = outputs.last_hidden_stateExpected behaviorHopefully, this will make the parameters of the model match the parameters of the pre-trained weights.' text_model.'\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_570.txt:\n",
      "Title: Support setting multiple adapters\n",
      "URL: https://github.com/huggingface/transformers/issues/28372\n",
      "Body:\n",
      "Feature requestThe underlying peft library supports setting multiple adapters:model.set_adapters([\"adapter_a\",\"adapter_b\"])It would be nice if the pipeline supported the same, from looking at#25077it appears it only supports a single adapterMotivationThis is useful functionality in the peft libraryYour contributionHappy to make the changes here!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_438.txt:\n",
      "Title: Jamba-v01 Model + Deepspeed Zero3 lead to \"RuntimeError: Detected mismatch between collectives on ranks.\"\n",
      "URL: https://github.com/huggingface/transformers/issues/30277\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.0Platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.19.4Safetensors version: 0.4.1Accelerate version: 0.29.2Accelerate config:    not foundDeepspeed version: 0.14.1PyTorch version (GPU?): 2.1.0a0+32f93b1 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:deepspeed config:{\"zero_optimization\": {\"stage\":3,\"offload_optimizer\": {\"device\":\"cpu\",\"pin_memory\":true},\"offload_param\": {\"device\":\"cpu\",\"pin_memory\":true},\"overlap_comm\":true,\"contiguous_gradients\":true,\"sub_group_size\":0,\"reduce_bucket_size\":1.677722e+07,\"stage3_prefetch_bucket_size\":1.509949e+07,\"stage3_param_persistence_threshold\":4.096000e+04,\"stage3_max_live_parameters\":1.000000e+09,\"stage3_max_reuse_distance\":1.000000e+09,\"stage3_gather_16bit_weights_on_model_save\":true},\"bf16\": {\"enabled\":true,\"auto_cast\":false,\"loss_scale\":0,\"initial_scale_power\":32,\"loss_scale_window\":1000,\"hysteresis\":2,\"min_loss_scale\":1},\"train_batch_size\":256,\"gradient_accumulation_steps\":8,\"train_micro_batch_size_per_gpu\":2,\"wall_clock_breakdown\":false,\"steps_per_print\":inf,\"fp16\": {\"enabled\":false},\"zero_allow_untested_optimizer\":true}training with data-parallel and deepspeed zeor3 off-loading.Who can help?@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSFT training using Jamba-v0.1 model with Accerate Trainer and Deepspeed Zero3 offload.Expected behaviorAfter a few iterations, there's an error met.Error Message:File\"/share5/users/kqsong/code/FastChat/fastchat/train/train_better_preprocessing.py\", line 306,in<module>train()\n",
      "  File\"/share5/users/kqsong/code/FastChat/fastchat/train/train_better_preprocessing.py\", line 300,intraintrainer.train()\n",
      "  File\"/root/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1780,intrainreturninner_training_loop(\n",
      "  File\"/root/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2118,in_inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 3036,intraining_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 3059,incompute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527,in_call_implreturnforward_call(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15,inwrapped_fn\n",
      "    ret_val = func(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 1855,inforward\n",
      "    loss = self.module(*inputs,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1568,in_call_impl\n",
      "    result = forward_call(*args,**kwargs)\n",
      "  File\"/root/.cache/huggingface/modules/transformers_modules/modeling_jamba.py\", line 1849,inforward\n",
      "    outputs = self.model(\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1568,in_call_impl\n",
      "    result = forward_call(*args,**kwargs)\n",
      "  File\"/root/.cache/huggingface/modules/transformers_modules/modeling_jamba.py\", line 1715,inforward\n",
      "    layer_outputs = self._gradient_checkpointing_func(\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 24,ininnerreturntorch._dynamo.disable(fn, recursive)(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 333,in_fnreturnfn(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 17,ininnerreturnfn(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 450,incheckpointreturnCheckpointFunction.apply(function, preserve,*args)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 539,inapplyreturnsuper().apply(*args,**kwargs)#type: ignore[misc]File\"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 230,inforward\n",
      "    outputs = run_function(*args)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1568,in_call_impl\n",
      "    result = forward_call(*args,**kwargs)\n",
      "  File\"/root/.cache/huggingface/modules/transformers_modules/modeling_jamba.py\", line 1361,inforward\n",
      "    hidden_states, router_logits = self.moe(hidden_states)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1568,in_call_impl\n",
      "    result = forward_call(*args,**kwargs)\n",
      "  File\"/root/.cache/huggingface/modules/transformers_modules/modeling_jamba.py\", line 1211,inforward\n",
      "    current_hidden_states = expert_layer(current_state)*routing_weights[top_x_list, idx_list, None]\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518,in_wrapped_call_implreturnself._call_impl(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1557,in_call_impl\n",
      "    args_result = hook(self, args)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15,inwrapped_fn\n",
      "    ret_val = func(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 278,in_pre_forward_module_hook\n",
      "    self.pre_sub_module_forward_function(module)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115,indecorate_contextreturnfunc(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 452,inpre_sub_module_forward_function\n",
      "    param_coordinator.fetch_sub_module(sub_module, forward=True)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 333,in_fnreturnfn(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15,inwrapped_fn\n",
      "    ret_val = func(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115,indecorate_contextreturnfunc(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 385,infetch_sub_module\n",
      "    self.__all_gather_params(params_to_prefetch, forward)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15,inwrapped_fn\n",
      "    ret_val = func(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 434,in__all_gather_params\n",
      "    self.__all_gather_params_(nonquantized_params, forward, quantize=self.zero_quantized_weights)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 463,in__all_gather_params_\n",
      "    handle = param_group[0].all_gather_coalesced(param_group, quantize=quantize)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15,inwrapped_fn\n",
      "    ret_val = func(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1259,inall_gather_coalesced\n",
      "    handles.append(_all_gather_dtype(dtype, params, world_size, rank_in_group, ds_process_group))\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 1147,in_all_gather_dtype\n",
      "    handle = _dist_allgather_fn(partitions[rank_in_group], flat_tensor, ds_process_group)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 95,in_dist_allgather_fnreturninstrument_w_nvtx(dist.allgather_fn)(output_tensor, input_tensor, group=group, async_op=True)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/utils/nvtx.py\", line 15,inwrapped_fn\n",
      "    ret_val = func(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 320,inallgather_fnreturnall_gather_into_tensor(output_tensor, input_tensor, group=group, async_op=async_op, debug=debug)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 117,inlog_wrapperreturnfunc(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/comm/comm.py\", line 305,inall_gather_into_tensorreturncdb.all_gather_into_tensor(output_tensor=output_tensor, input_tensor=tensor, group=group, async_op=async_op)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 333,in_fnreturnfn(*args,**kwargs)\n",
      "  File\"/root/.local/lib/python3.10/site-packages/deepspeed/comm/torch.py\", line 199,inall_gather_into_tensorreturnself.all_gather_function(output_tensor=output_tensor,\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py\", line 47,inwrapperreturnfunc(*args,**kwargs)\n",
      "  File\"/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py\", line 2886,inall_gather_into_tensor\n",
      "    work = group._allgather_base(output_tensor, input_tensor)\n",
      "RuntimeError: Detected mismatch between collectives on ranks. Rank 0 is running collective: CollectiveFingerPrint(SequenceNumber=1843749, OpType=_ALLGATHER_BASE, TensorShape=[2228224], TensorDtypes=BFloat16, TensorDeviceTypes=TensorOptions(dtype=float (def\n",
      "ault), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))), but Rank 10 is running collective: CollectiveFingerPrint(SequenceNumber=1843749, OpType=_ALLGATHER_BASE, TensorShape=[131\n",
      "072], TensorDtypes=BFloat16, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))).Collectives differinthe following aspects: \n",
      "  Tensor Tensor shapes: 2228224vs 131072\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_410.txt:\n",
      "Title: DDP error with load_best_model_at_end enabled\n",
      "URL: https://github.com/huggingface/transformers/issues/30702\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.1Platform: Linux-5.10.214-202.855.amzn2.x86_64-x86_64-with-glibc2.35Python version: 3.10.14Huggingface_hub version: 0.23.0Safetensors version: 0.4.3Accelerate version: 0.29.3Accelerate config: #011not foundPyTorch version (GPU?): 2.3.0 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: ddpWho can help?@muellerzrand@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionUse DDP to trigger the training scripttorchrun --standalone --nnodes=1 --nproc-per-node=$NUM_GPUS train.py --config /opt/ml/input/config/hyperparameters.jsonIn trainer argument set load_best_model_at_end to trueAt the end of the script, all GPUs except the rank 0 emmit the following errorRuntimeError: DDP expects same model across all ranks, but Rank 5 has 128 params, while rank 0 has inconsistent 1506656875 params.\n",
      "    return dist._verify_params_across_processes(process_group, tensors, logger)\n",
      "    return dist._verify_params_across_processes(process_group, tensors, logger)RuntimeErrorExpected behaviorNo Error occur.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_404.txt:\n",
      "Title: TokenClassificationPipeline support is_split_into_words tokeniser parameter\n",
      "URL: https://github.com/huggingface/transformers/issues/30757\n",
      "Body:\n",
      "Feature requestThe TokenClassificationPipeline currently sets a hardcoded tokeniser config within it sanitiser method. This prevents users from passing their own config to the tokeniser.It would be good to support some user input for tokeniser config. Especially for is_split_into_words as input data may be split already.MotivationIt is common for token classification datasets to be split into words already so that they match their labels.Your contributionI naivley anticipate this being a simple change, so I am happy to submit a PR for it. Though it would first be nice to see a discussion surrounding the feature and if it fits with the goals of Transformers.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_389.txt:\n",
      "Title: GGUF interaction with Transformers using AutoModel Class\n",
      "URL: https://github.com/huggingface/transformers/issues/30889\n",
      "Body:\n",
      "Feature requesthttps://huggingface.co/docs/transformers/main/en/ggufin above documentation it shows that it loads the gguf model and provided the simple examplefrom transformers import AutoTokenizer, AutoModelForCausalLMmodel_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"filename = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)but when I run the code it shows the error :OSError:  TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.and some time show your transformer may be not updatedeven after updating transformer it shows the same error not loading  gguf model , add support to load gguf modelMotivationit will solve to load gguf model with out the help of other library such as llama.cpp and ollamaYour contributionI do not have a complete implementation in mind, but I suggest to start with previous method which mention inhttps://huggingface.co/docs/transformers/main/en/gguf\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_10.txt:\n",
      "Title: bus error on version 4.43.0 with pretrained community CLIP model - MacOS\n",
      "URL: https://github.com/huggingface/transformers/issues/33357\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.0Platform: macOS-13.0-arm64-arm-64bitPython version: 3.10.9Huggingface_hub version: 0.24.6Safetensors version: 0.4.5Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.1 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import CLIPModel, CLIPTokenizerFast\n",
      "\n",
      "tokenizer = CLIPTokenizerFast.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
      "model = CLIPModel.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
      "\n",
      "tokenized = tokenizer([\"hello\"], return_tensors=\"pt\", padding=True)\n",
      "print(\"tokenized\", tokenized)\n",
      "\n",
      "# bus error occurs here\n",
      "embed = model.get_text_features(**tokenized).detach().cpu().numpy()\n",
      "print(\"embedded\", tokenized)gives :tokenized {'input_ids': tensor([[49406,  3497, 49407]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "zsh: bus error  python test_hf.pyI don't think the issue has been posted already.After bisecting versions, it looks like4.42.4does not have the issue and4.43.0has the issueI have little insight to provide except thebus error, and that this does not occur with theclip-vit-base-patch32model.I saw some breaking changes in this version release, but only about the tokenizer.I did not have time to test on a linux distribution yetThanks !Expected behaviorBy using the exact same script with the hugging face CLIP pretrained model, the embedding get computed as they shouldprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "tokenizer = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_38.txt:\n",
      "Title: Community contribution: Adding GGUF support for more architectures\n",
      "URL: https://github.com/huggingface/transformers/issues/33260\n",
      "Body:\n",
      "Feature requestRecently, we have added the ability to loadgguffiles withintransformers.The goal was to offer the possibility to users to further train/fine-tune their gguf models.See Workflow1) Load gguf file in transformers: we dequantize the weights to fp32, then we load the weights to be used with PyTorch.fromtransformersimportAutoTokenizer,AutoModelForCausalLMmodel_id=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"filename=\"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"tokenizer=AutoTokenizer.from_pretrained(model_id,gguf_file=filename)model=AutoModelForCausalLM.from_pretrained(model_id,gguf_file=filename)train/finetuneConvert the model back to gguf to use in the ggml ecosystem usingconvert_hf_to_ggufscript or usinggguf-my-repospace if you pushed your model on the hub :tokenizer.save_pretrained('directory')model.save_pretrained('directory')\n",
      "\n",
      "!python${path_to_llama_cpp}/convert-hf-to-gguf.py${directory}Let's try to add GGUF support for more architectures! Currently supported architectures areLlamaMistralQwen2It would be great to add the support for more architectures such asPhi3Add support for GGUF Phi-3#31844Qwen2MoeGemma2T5... and many more (Feel free to suggest more architectures ! The model needs to integrated in transformers)Adding this feature would require to follow the same protocol as in thisPR:UpdateGGUF_TENSOR_MAPPINGandGGUF_CONFIG_MAPPINGin order to map the tensor/config of the gguf file to the one on transformers.Create aGGUFXXXConverter(XXXConverter)class to convert the gguf tokenizer to a transformers one.Write testsIf you are interested to take up the challenge, comment below with the architecture name you want to integrate and open a PR!Once you open a PR, feel free to ping@SunMarc@LysandreJik@ArthurZuckerfor a review !MotivationSupport for more gguf modelsYour contributionReviewing PRs and possibly adding the support for more models\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_174.txt:\n",
      "Title: [ I built it! ] Server application with on-the-fly quantization to serve up HF-Hub models for back-and-forth request-response inferencing\n",
      "URL: https://github.com/huggingface/transformers/issues/32478\n",
      "Body:\n",
      "Feature requestIt would be immensely useful to have a server-application to serve up HF-Transformer and other Hub models as a service, similar to the howllama.cppbundles thellama-serverapplication to load GGUFs into memory and expose them via an API to enable back-and-forth inferencing request-responses without having to reload the model for each query.MotivationServing up HF-Hub models with ease: We had some excellent LLMs released in the last few weeks and I was naturally eager to try them the second they were available but had to wait forllama.cppto bake in proper support first.Whilellama.cppandGGUfsare amazing especially for hybrid-inferencing, waiting on support for models based on new tokenizers/attention-mechanisms and performing a lengthy recompilation ofllama.cppwith every release can be cumbersome, especially in containerized environments with GPUs.This got thinking that there has to be a better way! Since model creators release LLMs primarily with Transformers support in mind, if there were an easy way to serve them up via a local-API in their native HF-Transformer format, especially with on-the-fly quantization, we could run them as a local-inferencing service the day they're out, with at most a few pip-updates to local Python packages in most cases.As there wasn't such a server readily avaialble, and the general advice online was to build your own, I did and am looking to contribute it to this amazing project!Your contributionPresentingHF-Waitress, named after the Flask-WSGI server it leverages!Git repo:https://github.com/abgulati/hf-waitressHF-Waitress enables loading HF-Transformer & AWQ-quantized models directly off the hub, while providing on-the-fly quantization via BitsAndBytes, HQQ and Quanto for the former. It negates the need to manually download any model yourself, simply working off the models name instead. It requires no setup, and provides concurrency and streaming responses all from within a single, easily-portable, platform-agnostic Python script.Key FeaturesOn-the-fly, in-place quantization: Supports int8 & int4 quantization via BitsAndBytes, int8, int4 and int2 quantization via Quanto and int8, int4, int3, int2, int1 quantization via HQQModel Agnosticism: Compatible with any HF-Transformers format LLM.Configuration Management: Usesconfig.jsonto store settings, allowing for easy configuration and persistence across runs.Error Handling: Detailed logging and traceback reporting via centralized error-handling functions.Health Endpoint: Provides valuable information about the loaded model and server health.Concurrency Control: Uses semaphores for selective concurrency while taking advantage of semaphore-native queueing.Streaming Responses: Supports both standard and streaming completions.API Endpoints/completions(POST): Generate completions for given messages./completions_stream(POST): Stream completions for given messages./health(GET): Check the health and get information about the loaded model./hf_config_reader_api(POST): Read values from the configuration./hf_config_writer_api(POST): Write values to the configuration./restart_server(GET): Restart the LLM server.UsageTo start the server, run:python hf_waitress.py [arguments]Example:python hf_waitress.py --model_id=mistralai/Mistral-Nemo-Instruct-2407 --quantize=quanto --quant_level=int4 --access_token=<token> --trust_remote_code --use_flash_attention_2 --do_samplelaunch-arguments are optional, even on the first run! SeeREADMEfor defaults.Command-line Arguments--model_id: The model ID in HF-Transformers format - see below for details.--access_gated: Set to True if accessing gated models you're approved for.--access_token: Your Hugging Face Access Token.--gguf: Add this flag if attempting to load a GGUF model -For future use, not presently functional--gguf_model_id: GGUF repository ID -For future use, not presently functional--gguf_filename: Specific GGUF filename -For future use, not presently functional--quantize: Quantization method ('bitsandbytes', 'quanto', 'hqq' or 'n' for none, see important details below.).--quant_level: Quantization level (Valid values -  BitsAndBytes: int8 & int4; Quanto: int8, int4 and int2; HQQ: int8, int4, int3, int2, int1).--hqq_group_size: Specify group_size (default: 64) for HQQ quantization. No restrictions as long as weight.numel() is divisible by the group_size.--push_to_hub: Push quantized model to Hugging Face Hub.--torch_device_map: Specify inference device (e.g., 'cuda', 'cpu').--torch_dtype: Specify model tensor type.--trust_remote_code: Allow execution of custom code from the model's repository.--use_flash_attention_2: Attempt to use Flash Attention 2 -Only for specific Nvidia GPUs--pipeline_task: Specify the pipeline task (default: 'text-generation').--max_new_tokens: Maximum number of tokens to generate.--return_full_text: Return the full text including the prompt.--temperature: Set LLM temperature (0.0 to 2.0) - set do_sample to True for temps above 0.0, and False when setting temperature=0.0!--do_sample: Perform sampling when selecting response tokens - must be set to True for temps above 0.0!--top_k,--top_p,--min_p: Token selection parameters - must set do_sample to True!--port: Specify the server port (default: 9069).--reset_to_defaults: Reset all settings to default values.Detailed documentation in therepo!Since this is an independent script file and is in very active development, I wasn't sure of how to go about contributing it as a PR. I'd be very grateful for guidance on if and how to go about contributing this application to this repo, so humbly requesting help here!PS - Reddit post made by me announcing HF-Waitress:https://www.reddit.com/r/LocalLLaMA/comments/1eijqc6/the_softwarepain_of_running_local_llm_finally_got/\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_160.txt:\n",
      "Title: chore: moveconftest.pytotests/\n",
      "URL: https://github.com/huggingface/transformers/issues/32512\n",
      "Body:\n",
      "Feature requestIt's a common practice to place theconftest.pyfile within thetests/directory.Your contributionInitial Attempt:#32011ReferencesMultiple conftest and __init__.py across packages breaks pytest path resolving - how to solve it?pytest-dev/pytest#10708\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1008.txt:\n",
      "Title: Implementing ELECTRIC training for ELECTRA\n",
      "URL: https://github.com/huggingface/transformers/issues/9925\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_809.txt:\n",
      "Title: TokenGT\n",
      "URL: https://github.com/huggingface/transformers/issues/21079\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_819.txt:\n",
      "Title: Add BART-LS\n",
      "URL: https://github.com/huggingface/transformers/issues/20392\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_158.txt:\n",
      "Title: Error in object detection example script\n",
      "URL: https://github.com/huggingface/transformers/issues/32525\n",
      "Body:\n",
      "System Infotransformersversion: 4.45.0.dev0Platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.35Python version: 3.11.9Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    - compute_environment: LOCAL_MACHINE- distributed_type: MULTI_GPU- mixed_precision: no- use_cpu: False- debug: False- num_processes: 2- machine_rank: 0- num_machines: 1- gpu_ids: all- rdzv_backend: static- same_network: True- main_training_function: main- enable_cpu_affinity: False- downcast_bf16: no- tpu_use_cluster: False- tpu_use_sudo: False- tpu_env: []PyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA H100 80GB HBM3Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWhen running./examples/pytorch/object_detection/run_object_detection_no_trainer.pyin the transformers source code, it works well on a single GPU, but when running on multiple GPUs, an error occurs atmetric.compute()inevaluation_loop()function.Run:accelerate launch run_object_detection_no_trainer.py --ignore_mismatched_sizesExpected behavior[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/gpfs/home/ohhan/ai/PyProjects/transformers/examples/pytorch/object-detection/run_object_detection_no_trainer.py\", line 782, in <module>\n",
      "[rank0]:     main()\n",
      "[rank0]:   File \"/gpfs/home/ohhan/ai/PyProjects/transformers/examples/pytorch/object-detection/run_object_detection_no_trainer.py\", line 708, in main\n",
      "[rank0]:     metrics = evaluation_loop(model, image_processor, accelerator, valid_dataloader, id2label)\n",
      "[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/ai/PyProjects/transformers/examples/pytorch/object-detection/run_object_detection_no_trainer.py\", line 211, in evaluation_loop\n",
      "[rank0]:     metrics = metric.compute()\n",
      "[rank0]:               ^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/torchmetrics/metric.py\", line 628, in wrapped_func\n",
      "[rank0]:     with self.sync_context(\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/contextlib.py\", line 137, in __enter__\n",
      "[rank0]:     return next(self.gen)\n",
      "[rank0]:            ^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/torchmetrics/metric.py\", line 599, in sync_context\n",
      "[rank0]:     self.sync(\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/torchmetrics/metric.py\", line 548, in sync\n",
      "[rank0]:     self._sync_dist(dist_sync_fn, process_group=process_group)\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/torchmetrics/detection/mean_ap.py\", line 1029, in _sync_dist\n",
      "[rank0]:     super()._sync_dist(dist_sync_fn=dist_sync_fn, process_group=process_group)  # type: ignore[arg-type]\n",
      "[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/torchmetrics/metric.py\", line 452, in _sync_dist\n",
      "[rank0]:     output_dict = apply_to_collection(\n",
      "[rank0]:                   ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py\", line 72, in apply_to_collection\n",
      "[rank0]:     return _apply_to_collection_slow(\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py\", line 104, in _apply_to_collection_slow\n",
      "[rank0]:     v = _apply_to_collection_slow(\n",
      "[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py\", line 125, in _apply_to_collection_slow\n",
      "[rank0]:     v = _apply_to_collection_slow(\n",
      "[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/lightning_utilities/core/apply_func.py\", line 96, in _apply_to_collection_slow\n",
      "[rank0]:     return function(data, *args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/torchmetrics/utilities/distributed.py\", line 127, in gather_all_tensors\n",
      "[rank0]:     torch.distributed.all_gather(local_sizes, local_size, group=group)\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/torch/distributed/c10d_logger.py\", line 79, in wrapper\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/gpfs/home/ohhan/miniconda3/envs/ohhan-ai/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py\", line 3108, in all_gather\n",
      "[rank0]:     work = group.allgather([tensor_list], [tensor])\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]: RuntimeError: No backend type associated with device type cpu\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1030.txt:\n",
      "Title: Add DistilBERTGeneration comparable to BertGeneration\n",
      "URL: https://github.com/huggingface/transformers/issues/7397\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_602.txt:\n",
      "Title: CLIPTokenizer (and others based on the same telephoned OpenAI code) incorrect tokenize 1138 out of 34483 words that have an exact match in vocab\n",
      "URL: https://github.com/huggingface/transformers/issues/27961\n",
      "Body:\n",
      "System Infotransformersversion: 4.36.0Platform: Linux-5.15.120+-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.19.4Safetensors version: 0.4.1Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.1.0+cu118 (False)Tensorflow version (GPU?): 2.14.0 (False)Flax version (CPU?/GPU?/TPU?): 0.7.5 (cpu)Jax version: 0.4.20JaxLib version: 0.4.20Using GPU in script?: noUsing distributed or parallel set-up in script?: noWho can help?@ArthurZuckerand@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionVisithttps://colab.research.google.com/drive/18I0mYxTV-UCDjKWTxfuaR3P6o00w18Q9?usp=sharingfor a reproduction.from transformers import CLIPProcessor\n",
      "tokenizer = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\").tokenizer\n",
      "\n",
      "# match whole words\n",
      "whole_words = {k: v for k, v in tokenizer.get_vocab().items() if k.endswith(\"</w>\")}\n",
      "to_trim = len(\"<w/>\")\n",
      "missed = 0\n",
      "for token_str, token_int in whole_words.items():\n",
      "  tokenized = tokenizer.tokenize(token_str[:-to_trim])\n",
      "  if len(tokenized) != 1:\n",
      "    missed += 1\n",
      "print(f\"transformers {missed} words out of {len(whole_words)} incorrectly tokenized ({missed/len(whole_words)*100})%\")this printstransformers 1138 words out of 34483 incorrectly tokenized (3.3001768987617086)%I see that everyone copied OpenAI's buggy tokenization code. Besides this issue there is alsoopenai/CLIP#343. The code in that repository was obviously not used for training, so this could explain a lot of misses / poor performance in CLIP based models.Expected behaviortokenization of a word that exactly matches an entry in the vocab file should return exactly 1 token\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_170.txt:\n",
      "Title: [Activation Memory] remains unknown\n",
      "URL: https://github.com/huggingface/transformers/issues/32499\n",
      "Body:\n",
      "System InfoCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.transformersversion: 4.44.0Platform: Linux-5.4.0-165-generic-x86_64-with-glibc2.31Python version: 3.10.14Huggingface_hub version: 0.24.5Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?: <1x A800>GPU type: NVIDIA A800-SXM4-80GBWho can help?issue 1 - BriefTraining GPT2-XL with 🤗 Transformers and 🤗 AccelerateUsing _record_memory_history and _dump_snapshot from PyTorch for profilerThe activation memory are always remain unknownTraining GPT2-XL with 🤗 Transformers, set optimizer to adamw_torch. However, whether mix-precision training or not, from the snapshot, the Activation are always unknown.Reproducemodel: GPT2-XLbatch size: 4sequence length: 1024single GPUoptimizer: adamw_torchbf16#example commandaccelerate launch benchmark.py --dataset alpaca --output_dir validate --logging_strategy steps --logging_steps 1 --save_strategy no --save_steps 100 --dataloader_num_workers 1 --remove_unused_columns False --do_train --ddp_find_unused_parameters False --overwrite_output_dir --profiler pytorch --profiler_warmup_step 3 --max_steps 5 --max_memory_MB 80000 --hard_padding True --save_total_limit 0 --num_train_epochs 2 --learning_rate 4e-4 --optim adamw_torch --per_device_train_batch_size 4 --source_max_len 512 --target_max_len 512issue 2 - briefIn the training above, Transformers v4.41.2 yields quite different memory overhead compared to v4.42.0+, the memory overhead with Transformers v4.41.2- are always higher. Is there any fix in it, what 'actiavtions' are erased in v4.42.0+?@ArthurZucker@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionExpected behavior\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_399.txt:\n",
      "Title: Add data2vec 2.0\n",
      "URL: https://github.com/huggingface/transformers/issues/30805\n",
      "Body:\n",
      "Model descriptionHello,The data2vec 2.0 paper has been released quite a while and achieved impressive performance across different modalities: speech, text, and image (results similar or better than data2vec 1.0 but much more efficient). Especially, the audio-only model seems to be one of the best SSL speech models using base architecture (93M parameters). Therefore, I think it would be a nice addition to thetransformerslibrary.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationPaper:https://arxiv.org/abs/2212.07525Code:https://github.com/facebookresearch/fairseq/tree/main/examples/data2vecAuthors:@alexeib@michaelauli@wnhsu\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_14.txt:\n",
      "Title: Add \"EAT: Self-Supervised Pre-Training with Efficient Audio Transformer\"\n",
      "URL: https://github.com/huggingface/transformers/issues/33342\n",
      "Body:\n",
      "Model descriptionThe original authors of the model write:EAT is an audio self-supervised learning model with high effectiveness and efficiency during self-supervised pre-training. You can find details in the paperEAT: Self-Supervised Pre-Training with Efficient Audio Transformer.A self-supervised learning model can benefit the community greatly, since it requires no labelled data, and can be trained on any dataset. Especially since, the strength of this approach is that it can be applied to variable-length audio. With enough resources (for example, compute, and, data), it could have a similar reach as BERT.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationGitHub Repo:https://github.com/cwx-worst-one/EATLinks for model checkpoints:EAT-base_epoch30(pre-training)EAT-base_epoch30(fine-tuning on AS-2M)EAT-large_epoch20(pre-training)EAT-large_epoch20(fine-tuning on AS-2M)Paper:https://www.ijcai.org/proceedings/2024/421\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_28.txt:\n",
      "Title: oom when using adafactor optimizer in deepspeed\n",
      "URL: https://github.com/huggingface/transformers/issues/33290\n",
      "Body:\n",
      "System Info-`transformers`version:4.44.2-Platform:Linux-5.15.0-105-generic-x86_64-with-glibc2.31-Pythonversion:3.10.0-Huggingface_hubversion:0.23.4-Safetensorsversion:0.4.2-Accelerateversion:0.33.0-Accelerateconfig:notfound-PyTorchversion(GPU?):2.3.0+cu118(True)-Tensorflowversion(GPU?):notinstalled(NA)-Flaxversion(CPU?/GPU?/TPU?):notinstalled(NA)-Jaxversion:notinstalled-JaxLibversion:notinstalled-Usingdistributedorparallelset-upinscript?:<fillin>-UsingGPUinscript?:<fillin>-GPUtype:NVIDIAA80080GBPCIeWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductioni'm running train_xl.sh inthis repo. and i change the 8bit adam optimizer to adafactor optimizer using transformers.optimization.Adafactor. i'm using two 40GB a100, deepspeed stage 2, batchsize=1,VTON-HD dataset.the adafactor optimizer should use less gpu memory, because of less optimizer states than 8bit adam, but it get oom inthis lineand oom happens after 10 steps, i don't know what happen in 10th step, i call theaccelerate.backward()andoptimizer.step()every step.and in 10th step, the memory usage increased from 29GB to 39GB when using 8bit adam optimizer, and get oom when using adafactor optimizerExpected behaviorcould anybody explain this phenomenon\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_400.txt:\n",
      "Title: CLAP Fine-tuning has run into a problem\n",
      "URL: https://github.com/huggingface/transformers/issues/30795\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.3Platform: Linux-4.19.91-014-kangaroo.2.10.13.5c249cdaf.x86_64-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.20.2Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config: \tnot foundPyTorch version (GPU?): 2.3.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@sanchit-gandhi@ylacombe@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI'm trying to fine-tune the clap, but I'm having some problems with it, and I've previously referenced a solution in#26864Here is my code:load dataimport re\n",
      "import glob\n",
      "import numpy as np\n",
      "from datasets import load_dataset, load_metric\n",
      "\n",
      "files= glob.glob(\"genshin-voice-v3.5-mandarin/data/*.parquet\")\n",
      "\n",
      "\n",
      "dataset = load_dataset(\n",
      "    \"parquet\",\n",
      "    data_files= files\n",
      "     )[\"train\"].select(range(500))\n",
      "\n",
      "\n",
      "split_dataset= dataset.train_test_split(test_size=0.01)DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'is_longer', 'input_ids'],\n",
      "        num_rows: 495\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_features', 'is_longer', 'input_ids'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})load modelimport torch\n",
      "from datasets import Audio\n",
      "from transformers import Seq2SeqTrainer\n",
      "from transformers import Seq2SeqTrainingArguments\n",
      "from transformers import ClapProcessor\n",
      "from transformers import ClapModel\n",
      "from transformers import ClapConfig\n",
      "\n",
      "\n",
      "model_path= \"./laion_clap-htsat-fused\"\n",
      "config= ClapConfig.from_pretrained(model_path)\n",
      "\n",
      "config.audio_config.enable_fusion= False\n",
      "\n",
      "#config.audio_config.enable_patch_fusion = False\n",
      "#config.audio_config.enable_patch_layer_norm = False\n",
      "\n",
      "\n",
      "processor = ClapProcessor.from_pretrained(model_path)\n",
      "\n",
      "model = ClapModel.from_pretrained(\n",
      "                                model_path, \n",
      "                                #config= config\n",
      "                                #torch_dtype =torch.bfloat16,\n",
      "                                   \n",
      "            )process data:def remove_special_characters(example):\n",
      "    #try:\n",
      "    chars_to_remove_regex = r'[\\「\\」\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\…\\。\\、\\？\\!\\，\\']'\n",
      "    example[\"text\"] = re.sub(chars_to_remove_regex, '', example[\"text\"]).lower()\n",
      "    return example\n",
      "    #except:\n",
      "    #    print(batch)\n",
      "    #    print(\"-----\"*6)\n",
      "        \n",
      "\n",
      "def text_length(batch):\n",
      "    text = str(batch[\"text\"])\n",
      "    batch[\"text_length\"] = len(text)\n",
      "    return batch\n",
      "\n",
      "def prepare_dataset(batch):\n",
      "    # load and resample audio data from 48 to 16kHz\n",
      "    audio = batch[\"audio\"]\n",
      "\n",
      "    # compute input length\n",
      "    batch[\"input_length\"] = len(batch[\"audio\"])\n",
      "\n",
      "    # compute log-Mel input features from input audio array \n",
      "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
      "    batch[\"is_longer\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors= \"pt\").is_longer[0][0]\n",
      "    # encode target text to label ids \n",
      "    text_inputs = processor.tokenizer(batch[\"text\"])\n",
      "    batch[\"input_ids\"] = text_inputs.input_ids\n",
      "    #batch[\"attention_mask\"] = text_inputs.attention_mask\n",
      "\n",
      "    # compute labels length\n",
      "    batch[\"labels_length\"] = len(processor.tokenizer(batch[\"text\"], add_special_tokens=False).input_ids)\n",
      "    return batch\n",
      "\n",
      "\n",
      "def filter_inputs(input_length):\n",
      "    \"\"\"Filter inputs with zero input length or longer than 30s\"\"\"\n",
      "    return 0 < input_length < max_input_length\n",
      "\n",
      "\n",
      "def filter_labels(labels_length):\n",
      "    \"\"\"Filter label sequences longer than max length (448)\"\"\"\n",
      "    return labels_length < max_label_length\n",
      "\n",
      "split_dataset= dataset.train_test_split(test_size=0.01) \n",
      "\n",
      "max_label_length= 448\n",
      "MAX_DURATION_IN_SECONDS = 30.0\n",
      "max_input_length = MAX_DURATION_IN_SECONDS * 48000\n",
      "\n",
      "split_dataset = split_dataset.filter(lambda x: x[\"text\"] != None, num_proc= 32)\n",
      "split_dataset = split_dataset.map(remove_special_characters, num_proc= 32, batched= False)\n",
      "split_dataset = split_dataset.cast_column(\"audio\", Audio(sampling_rate=48000))\n",
      "split_dataset = split_dataset.map(prepare_dataset, remove_columns= split_dataset[\"train\"].column_names, num_proc= 32)\n",
      "\n",
      "split_dataset = split_dataset.filter(filter_inputs, input_columns=[\"input_length\"])\n",
      "split_dataset = split_dataset.filter(filter_labels, input_columns=[\"labels_length\"])                                     \n",
      "split_dataset = split_dataset.remove_columns(['input_length', 'labels_length'])import torch\n",
      "\n",
      "from dataclasses import dataclass\n",
      "from typing import Any, Dict, List, Union\n",
      "\n",
      "@dataclass\n",
      "class DataCollatorSpeechSeq2SeqWithPadding:\n",
      "    processor: Any\n",
      "\n",
      "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
      "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
      "        # first treat the audio inputs by simply returning torch tensors\n",
      "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
      "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
      "       \n",
      "        is_longer_features = [feature[\"is_longer\"] for feature in features] \n",
      "        #is_longer_batch = self.processor.tokenizer.pad(is_longer_features, return_tensors=\"pt\")\n",
      "    \n",
      "        # get the tokenized label sequences\n",
      "        label_features = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
      "        # pad the labels to max length\n",
      "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
      "       \n",
      "        # replace padding with -100 to ignore loss correctly\n",
      "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
      "\n",
      "        # if bos token is appended in previous tokenization step,\n",
      "        # cut bos token here as it's append later anyways\n",
      "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
      "            labels = labels[:, 1:]\n",
      "\n",
      "        batch[\"input_ids\"] = labels\n",
      "        batch[\"is_longer\"] = is_longer_features\n",
      "        batch[\"return_loss\"]= True\n",
      "        return batchfrom transformers import Trainer\n",
      "from transformers import TrainingArguments\n",
      "\n",
      "\n",
      "training_args = TrainingArguments(\n",
      "    output_dir= \"./openai_whisper-large-v3_ft\", # change to a repo name of your choice\n",
      "    do_train= True,\n",
      "    do_eval= True,\n",
      "    evaluation_strategy= \"steps\",\n",
      "    per_device_train_batch_size= 16,\n",
      "    per_device_eval_batch_size= 16,\n",
      "    gradient_accumulation_steps= 2, # increase by 2x for every 2x decrease in batch size\n",
      "    learning_rate=1e-5,\n",
      "    warmup_steps=500,\n",
      "    #max_steps=4000,\n",
      "    num_train_epochs= 3,\n",
      "    #gradient_checkpointing=True,\n",
      "    bf16=True,\n",
      "    save_steps=100,\n",
      "    eval_steps=100,\n",
      "    logging_steps=25,\n",
      "    #report_to=[\"tensorboard\"],\n",
      "    load_best_model_at_end=True,\n",
      "    metric_for_best_model=\"wer\",\n",
      "    greater_is_better=False,\n",
      "    push_to_hub=False,\n",
      ")data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
      "\n",
      "\n",
      "trainer = Trainer(\n",
      "    args=training_args,\n",
      "    model=model,\n",
      "    train_dataset= split_dataset[\"train\"],\n",
      "    eval_dataset= split_dataset[\"test\"],\n",
      "    data_collator= data_collator,\n",
      "    #data_collator=collate_fn,\n",
      "    #tokenizer= processor.feature_extractor,\n",
      ")\n",
      "\n",
      "\n",
      "trainer.train()Then the following error occurs:__AttributeError: Caught AttributeError in replica 0 on device 0.Original Traceback (most recent call last):File \"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _workeroutput = module(*input, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_implreturn self._call_impl(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_implreturn forward_call(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clap/modeling_clap.py\", line 2094, in forwardaudio_outputs = self.audio_model(File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_implreturn self._call_impl(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_implreturn forward_call(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clap/modeling_clap.py\", line 1742, in forwardreturn self.audio_encoder(File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_implreturn self.call_impl(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, incall_implreturn forward_call(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/transformers/models/clap/modeling_clap.py\", line 913, in forwardis_longer_list = is_longer.to(input_features.device)AttributeError: 'list' object has no attribute 'to'I'm having a lot of problems with the mode of enable_fusion=True, and I don't seem to have a good grasp of the handling of the input is_longer, so I hope I can get your pointers on this piece, thanks!Expected behaviorThe model should train normally.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_548.txt:\n",
      "Title: It's an AlignModel or Deepspeed Zero3 bug.\n",
      "URL: https://github.com/huggingface/transformers/issues/28808\n",
      "Body:\n",
      "System InfoWhen I try to load the AlignModel weights locally and train them using zero3, I get the following error：File \"/opt/licy/MyVLM/model/builder.py\", line 152, in load_model\n",
      "    model =AlignModel.from_pretrained(self.args.vm_path)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 3307, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 3559, in _load_pretrained_model\n",
      "    model.apply(model._initialize_weights)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 885, in apply\n",
      "    fn(self)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\", line 1388, in _initialize_weights\n",
      "    self._init_weights(module)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/align/modeling_align.py\", line 1189, in _init_weights\n",
      "    nn.init.xavier_uniform_(module.text_projection.weight)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/init.py\", line 323, in xavier_uniform_\n",
      "    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/init.py\", line 287, in _calculate_fan_in_and_fan_out\n",
      "    raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")Switching to zero2 doesn't produce an error; also, ConvnextModel and ClipVisionModel don't report an error when trained under zero3, so I'm thinking that maybe there's a bug in AlignModel?@amyeroberts@pacman100@muellerzWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction1.model =AlignModel.from_pretrained(path)2.use zero3 to train model3.get error about xavier_initExpected behaviorThe expected behavior is to be able to load models\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_574.txt:\n",
      "Title: Add flash attention 2.0 support for GPT2LMHeadModel\n",
      "URL: https://github.com/huggingface/transformers/issues/28348\n",
      "Body:\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "        my_GPT2LMHeadModel_checkpoint, \n",
      "        torch_dtype=torch.bfloat16, \n",
      "        attn_implementation=\"flash_attention_2\",\n",
      "    )throws the following error:Error loading Flash_Model_2: GPT2LMHeadModel does not support Flash Attention 2.0 yet. Please open an issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_212.txt:\n",
      "Title: get_logits_warper_patch\n",
      "URL: https://github.com/huggingface/transformers/issues/32289\n",
      "Body:\n",
      "System Infojetson-agix-orinWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionTraceback (most recent call last):File \"/opt/text-generation-webui/modules/callbacks.py\", line 61, in gentaskret = self.mfunc(callback=_callback, *args, **self.kwargs)File \"/opt/text-generation-webui/modules/text_generation.py\", line 382, in generate_with_callbackshared.model.generate(**kwargs)File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_contextreturn func(*args, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1975, in generateself._get_logits_warper(generation_config, device=input_ids.device)TypeError: get_logits_warper_patch() got an unexpected keyword argument 'device'Expected behaviorAnswer my question\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_206.txt:\n",
      "Title: EncoderDecoderModel incorrectly establishing decoder input ids and decoder labels\n",
      "URL: https://github.com/huggingface/transformers/issues/32344\n",
      "Body:\n",
      "System InfoWho can help?Tagging based on both gitblame and pretrained models on Hub associated with this:@patrickvonplaten@NielsRoggeInformationI was recently using the EncoderDecoderModel setup for training a Seq2Seq model. I noticed a few oddities during training, in particular I was trying to measure the token accuracy and found that the accuracy was decreasing while the loss was also decreasing (see accuracy calculation below, which is mostly correct, doesn't properly deal with EOS in padded instances):labels = batch[\"labels\"][:,1:]\n",
      "        non_padded_idx = labels != -100\n",
      "        preds = torch.argmax(output.logits, dim=2)[:,:-1]\n",
      "        accuracy = (labels == preds).sum() / non_padded_idx.sum()I started looking at the EncoderDecoderModel class and found that there seems to be an issue with theshift_tokens_rightfunction:def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
      "    \"\"\"\n",
      "    Shift input ids one token to the right.\n",
      "    \"\"\"\n",
      "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
      "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
      "    if decoder_start_token_id is None:\n",
      "        raise ValueError(\"Make sure to set the decoder_start_token_id attribute of the model's configuration.\")\n",
      "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
      "\n",
      "    if pad_token_id is None:\n",
      "        raise ValueError(\"Make sure to set the pad_token_id attribute of the model's configuration.\")\n",
      "    # replace possible -100 values in labels by `pad_token_id`\n",
      "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
      "\n",
      "    return shifted_input_idsFor context, this is taking the input labels and shifting them right for the decoder inputs. There are a few problems here:shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()This seems to be trying to clip the EOS token off from the decoder inputs (which seems standard), however this doesn't always work if the input sequence is padded.shifted_input_ids[:, 0] = decoder_start_token_idSince we already have special tokens in the input (the examples on the class page in the documentation do not useskip_special_tokensand I'm fairly sure the training setup assumes they are already in there), this results in thesos_token_idoccupying the first two positions due toshifted_input_ids[:, 1:] = input_ids[:, :-1].clone()in conjunction with this.Final problem is that as far as I can tell, thesos_token_idis never actually removed the labels like it should be.I suspect this hasn't been caught due to the training process still be mostly correct, its still offset by 1 token and the additional sos_token in the output is filtered duringtokenizer.decode.If all the above makes sense, I can make a PR with the fixes I made to get my training performing as expected. If I'm misunderstanding something here, can someone please point out what I'm missing.TasksReproductionUsing a print statement within the EncoderDecoder model of thedecoder_input_idsprior to decoder input indicates that there are 2sos_token_idsin the input, when there should only be one.Expected behaviorThere should only be 1 sos token in the input\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_560.txt:\n",
      "Title: Early stopping required metric_for_best_model, but did not find eval_f1 so early stopping is disabled\n",
      "URL: https://github.com/huggingface/transformers/issues/28530\n",
      "Body:\n",
      "System Infotransformersversion: 4.35.2Platform: Linux-3.10.0-1160.49.1.el7.x86_64-x86_64-with-glibc2.17Python version: 3.8.18Huggingface_hub version: 0.19.4Safetensors version: 0.4.1Accelerate version: 0.25.0Accelerate config:    not foundPyTorch version (GPU?): 1.13.1+cu116 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: noWho can help?@muellerzr@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionimportosfromtransformersimportAutoTokenizerfromtransformersimportAutoModelForSequenceClassificationfromtransformersimportTrainingArguments,TrainerfromtransformersimportEarlyStoppingCallback,IntervalStrategyimportnumpyasnpimportevaluateos.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu_id)fromdatasetsimportDataset,DatasetDicttrain_k=pd.read_csv('train.csv',usecols=[\"text\",\"k\"])train_k.rename(columns={\"text\":\"text\",\"k\":\"label\"},inplace=True)val_k=pd.read_csv('val.csv',usecols=[\"text\",\"k\"])val_k.rename(columns={\"text\":\"text\",\"k\":\"label\"},inplace=True)test_k=pd.read_csv('test.csv',usecols=[\"text\",\"k\"])test_k.rename(columns={\"text\":\"text\",\"k\":\"label\"},inplace=True)train_k=Dataset.from_pandas(train_k)val_k=Dataset.from_pandas(val_k)test_k=Dataset.from_pandas(test_k)ds=DatasetDict()ds['train']=train_kds['val']=val_kds['test']=test_ktokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")deftokenize_function(examples):returntokenizer(str(examples['text']),padding=\"max_length\",truncation=True)tokenized_datasets=ds.map(tokenize_function)tokenized_train_k=tokenized_datasets[\"train\"]tokenized_val_k=tokenized_datasets[\"val\"]model_k=AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=6)training_args=TrainingArguments(output_dir=\"trained_k_predictors\",evaluation_strategy=\"steps\",eval_steps=100,metric_for_best_model='f1',learning_rate=1e-3,num_train_epochs=5,weight_decay=0.01,load_best_model_at_end=True,per_device_train_batch_size=16,per_device_eval_batch_size=32,save_total_limit=3,optim=\"adafactor\",label_names=['label'],remove_unused_columns=False,)metric=evaluate.load(\"f1\")defcompute_metrics(eval_pred):logits,labels=eval_predpredictions=np.argmax(logits,axis=-1)return{'f1':metric.compute(predictions=predictions,references=labels)}trainer=Trainer(model=model_k,args=training_args,train_dataset=tokenized_train_k,eval_dataset=tokenized_val_k,compute_metrics=compute_metrics,callbacks=[EarlyStoppingCallback(early_stopping_patience=3)])trainer.train()Error message:{'eval_runtime': 21.6631, 'eval_samples_per_second': 208.926, 'eval_steps_per_second': 3.277, 'epoch': 0.47}9%|████████████████                                                                                                                                                           | 5                       [26/1960]00/5305 [06:15<41:00,  1.95it/s]100%|████████████████████████████████████████████████████████████████████�     $                                                                                                                          [24/1960]�█████████████████████████████████████████████�early stopping required metric_for_best_model, but did not find eval_f1 so                                                                                 [23/1960]early stopping is disabled██████████████████████████| 71/71 [00:21<00:00,  3.42it/s]Traceback (most recent call last):File \"/scratch/manish/apl/apl_env/lib/python3.8/runpy.py\", line 194, in _run_module_as_mainreturn _run_code(code, main_globals, None,File \"/scratch/manish/apl/apl_env/lib/python3.8/runpy.py\", line 87, in _run_codeexec(code, run_globals)File \"/scratch/manish/apl/src/apl.py\", line 386, inmain()File \"/scratch/manish/apl/src/apl.py\", line 139, in maintrainer.train()File \"/scratch/manish/apl/apl_env/lib/python3.8/site-packages/transformers/trainer.py\", line 1555, in trainreturn inner_training_loop(File \"/scratch/manish/apl/apl_env/lib/python3.8/site-packages/transformers/trainer.py\", line 1922, in _inner_training_loopself._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)File \"/scratch/manish/apl/apl_env/lib/python3.8/site-packages/transformers/trainer.py\", line 2282, in _maybe_log_save_evaluateself._save_checkpoint(model, trial, metrics=metrics)File \"/scratch/manish/apl/apl_env/lib/python3.8/site-packages/transformers/trainer.py\", line 2407, in _save_checkpointmetric_value = metrics[metric_to_check]KeyError: 'eval_f1'9%|███████████████▉                                                                                                                                                         | 500                        [3/1960]/5305 [06:18<1:00:34,  1.32it/s]Expected behaviorTrain the model with early stopping enabled.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_945.txt:\n",
      "Title: Add DEBERTA-base model for usage in EncoderDecoderModel.\n",
      "URL: https://github.com/huggingface/transformers/issues/12436\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_776.txt:\n",
      "Title: Implement a decode method in transformers.BasicTokenizer\n",
      "URL: https://github.com/huggingface/transformers/issues/22786\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_762.txt:\n",
      "Title: Flaky Whisper PT-TF & PT-Flax Equivalence Test\n",
      "URL: https://github.com/huggingface/transformers/issues/23258\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_777.txt:\n",
      "Title: TypeError: export() got an unexpected keyword argument 'preprocessor'\n",
      "URL: https://github.com/huggingface/transformers/issues/22756\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_987.txt:\n",
      "Title: Add DALL-E: Zero-Shot Text-to-Image Generation\n",
      "URL: https://github.com/huggingface/transformers/issues/10935\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_944.txt:\n",
      "Title: Expand text-generation pipeline support for other causal models e.g., BigBirdForCausalLM\n",
      "URL: https://github.com/huggingface/transformers/issues/12439\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_950.txt:\n",
      "Title: A fast tokenizer for BertJapaneseTokenizer\n",
      "URL: https://github.com/huggingface/transformers/issues/12381\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_561.txt:\n",
      "Title: dataloader_persistent_workers=Truecauses fork-bomb due to repeated creation ofeval_dataloader\n",
      "URL: https://github.com/huggingface/transformers/issues/28469\n",
      "Body:\n",
      "System Infotransformersversion: 4.36.2Platform: macOS-10.16-x86_64-i386-64bitPython version: 3.10.13Huggingface_hub version: 0.20.2Safetensors version: 0.4.1Accelerate version: 0.26.1Accelerate config:    - compute_environment: LOCAL_MACHINE- distributed_type: NO- mixed_precision: fp16- use_cpu: False- debug: False- num_processes: 1- machine_rank: 0- num_machines: 1- rdzv_backend: static- same_network: True- main_training_function: main- downcast_bf16: no- tpu_use_cluster: False- tpu_use_sudo: False- tpu_env: []PyTorch version (GPU?): 2.1.2 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: does not matterUsing distributed or parallel set-up in script?: does not matterWho can help?@muellerzr@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionimportosfromdataclassesimportdataclassimporttorchimporttorch.nn.functionalasFfromtorch.utils.dataimportDatasetfromtransformersimportTrainingArguments,Trainerfromtransformers.modeling_outputsimportBaseModelOutput# Dummy DatasetclassDummyDataset(Dataset):def__init__(self,size=100):self.size=sizeself.data=torch.rand(size,10)# Random dataself.labels=torch.randint(0,2, (size,))# Binary labelsdef__len__(self):returnself.sizedef__getitem__(self,idx):return{'input_ids':self.data[idx],'labels':self.labels[idx]}@dataclassclassDummyModelOutput(BaseModelOutput):loss:torch.Tensor=Nonelogits:torch.Tensor=None# Dummy ModelclassDummyModel(torch.nn.Module):def__init__(self):super(DummyModel,self).__init__()self.linear=torch.nn.Linear(10,2)defforward(self,input_ids,labels=None)->DummyModelOutput:outputs=self.linear(input_ids)loss=F.cross_entropy(outputs,labels)returnDummyModelOutput(loss=loss,logits=outputs)if__name__=='__main__':# using wandb, because it logs system metrics periodicallyos.environ[\"WANDB_PROJECT\"]=\"dummy_project\"# Create dataset and model instancesdataset=DummyDataset(size=1000)model=DummyModel()persistent_workers=False# set to True to enable persistent workers# Training argumentstraining_args=TrainingArguments(output_dir=\"./test_trainer\",run_name=f'dataloader_peristent_workers={persistent_workers}',num_train_epochs=20,per_device_train_batch_size=16,per_device_eval_batch_size=16,dataloader_num_workers=8,dataloader_persistent_workers=persistent_workers,logging_strategy=\"no\",evaluation_strategy=\"epoch\",\n",
      "    )# Initialize the custom trainertrainer=Trainer(model=model,args=training_args,train_dataset=dataset,eval_dataset=dataset,\n",
      "    )# Train the modeltrainer.train()Expected behaviorSince theget_eval_loaderis called on every evaluate call, withdataloader_persistent_workers=Truethe previous worker processes are not killed and leads to a fork-bomb and exhausts system resources and causes instability/crash.As you can see in the below plots generated with the reproduction script (in the wandb system metrics section),persistent data loader workers cause speedup (mainly because the training loader does not recreate all processes at every epoch), but evaluation loaders cause the fork-bomb.without persistent data loader workers, speed is slow, but the number of processes is constant.Having the persistent dataloader option is good. Still, it is necessary to fix the eval loader logic, create it once, and reuse it since the eval datasets won't change in the middle of training.This option was added in#27058and#27189\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_213.txt:\n",
      "Title: 'weight' must be 2-D\n",
      "URL: https://github.com/huggingface/transformers/issues/32285\n",
      "Body:\n",
      "System Infotransformers version == 4.42.4\n",
      "dp-transformers version == 1.0.1\n",
      "accelerate version == 0.29.3\n",
      "llm == Distil-gpt2\n",
      "torch version == 2.0.0\n",
      "The model is runned under ddp with 2 GPUsWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionHello, I am trying to train a Tabula (https://github.com/zhao-zilong/Tabula) model using FSDP. I rewrote my Tabula trainer without middle padding to use the FSDP model. I added the parameters for training arguments fsdp=full_shard, and then I used setup and multiprocessing.spawn to run my code. When I run my code using DDP, it works, but when I run it using FSDP, the training works, but the sampling does not. I get the error 'weight' must be 2-D. I know that FSDP works because when I print the memory usage, the memory reduces while using FSDP instead of DDP.Expected behaviorThe sampling function should generate data.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_549.txt:\n",
      "Title: Unable to use torch scripting to export Mask2Former model\n",
      "URL: https://github.com/huggingface/transformers/issues/28781\n",
      "Body:\n",
      "System Infotransformersversion: 4.34.1Platform: Linux-5.4.0-100-generic-x86_64-with-glibc2.17Python version: 3.8.18Huggingface_hub version: 0.17.3Safetensors version: 0.4.0Accelerate version: 0.23.0Accelerate config:    not foundPyTorch version (GPU?): 2.1.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: Yes, a GTX 1080 with 8 GB of VRAMUsing distributed or parallel set-up in script?: No.Who can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI am attempting to export theMask2Former model available in huggingfacethroughtorch.jit.script. Here's a minimal reproducible example:importtorchfromtransformersimportMask2FormerForUniversalSegmentationdevice=\"cuda\"iftorch.cuda.is_available()else\"cpu\"model=Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\",torchscript=True).to(device)scripted_model=torch.jit.script(model)torch.jit.save(scripted_model,'mask2former.pt')By doing this, I get the following error using torch scripting (path to the offending file has been obfuscated for brevity):torch.jit.frontend.NotSupportedError: Comprehension ifs are not supported yet:\n",
      "  File \"/home/.../huggingface/lib/python3.8/site-packages/transformers/models/mask2former/modeling_mask2former.py\", line 2559\n",
      "        if not return_dict:\n",
      "            output = tuple(v for v in output.values() if v is not None)\n",
      "            if loss is not None:\n",
      "                output = ((loss)) + outputAs a hack, I've changed my local installation so that comprehension ifs are removed:if not return_dict:\n",
      "        outputs = []\n",
      "        for v in output.values():\n",
      "            if v is not None:\n",
      "                outputs.append(v)\n",
      "        output = tuple(outputs)This also occurs at line 2306 in the same file, so I've made the same changes there. Once I fix this, there is an error in the forward method for the SWIN backbone:RuntimeError: \n",
      "'Optional[Tensor]' object has no attribute or method 'shape'.:\n",
      "  File \"/home/.../anaconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py\", line 313\n",
      "    def forward(self, pixel_values: Optional[torch.FloatTensor]) -> Tuple[torch.Tensor, Tuple[int]]:\n",
      "        _, num_channels, height, width = pixel_values.shape\n",
      "                                         ~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "        if num_channels != self.num_channels:\n",
      "            raise ValueError(The forward method for the SWIN backbone is confusing, as the input type is declared to beOptionalbut the output type is not. The definition of this method clearly indicates that a concrete tuple is to be returned.As a final experiment, I've removed theOptionaltype declaration and tried to export it one more time:aten::pad(Tensor self, SymInt[] pad, str mode=\"constant\", float? value=None) -> Tensor:\n",
      "Expected a value of type 'List[int]' for argument 'pad' but instead found type 'Tuple[int, Tensor]'.\n",
      ":\n",
      "  File \"/home/.../anaconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py\", line 306\n",
      "        if width % self.patch_size[1] != 0:\n",
      "            pad_values = (0, self.patch_size[1] - width % self.patch_size[1])\n",
      "            pixel_values = nn.functional.pad(pixel_values, pad_values)\n",
      "                           ~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "        if height % self.patch_size[0] != 0:\n",
      "            pad_values = (0, 0, 0, self.patch_size[0] - height % self.patch_size[0])\n",
      "'SwinPatchEmbeddings.maybe_pad' is being compiled since it was called from 'SwinPatchEmbeddings.forward'\n",
      "  File \"/home/.../anaconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py\", line 319\n",
      "            )\n",
      "        # pad the input to be divisible by self.patch_size, if needed\n",
      "        pixel_values = self.maybe_pad(pixel_values, height, width)\n",
      "        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "        embeddings = self.projection(pixel_values)\n",
      "        _, _, height, width = embeddings.shapeIt seems that what is being put into the forward pass is not, in fact, atorch.Tensorwhen being scripted.Is torch scripting this model not supported at this time or am I missing something?Expected behaviorThe model successfully being exported to disk with torch scripting.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_373.txt:\n",
      "Title: reworktest_multi_gpu_data_parallel_forward\n",
      "URL: https://github.com/huggingface/transformers/issues/31087\n",
      "Body:\n",
      "DescriptionCurrentlytest_multi_gpu_data_parallel_forwardis problematic and are skipped for several model testing. Many times, it looks like some cuda issue (CUDA error: misaligned addressetc.). With othernvidiarelated stuffs (software or hardware) and/or torch versions, they might pass, fail, or pass but fail many subsequent tests.It currently usesnn.DataParallelwhich is no longer recommended. In the long term, we should tryDistributedDataParalleland see how this test goes.See#31086for example\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_415.txt:\n",
      "Title: Community contribution: enable dynamic resolution input for more vision models.\n",
      "URL: https://github.com/huggingface/transformers/issues/30579\n",
      "Body:\n",
      "Feature requestSome of our models interpolate its positional embeddings, enabling pretrained checkpoints to be used on different input resolutions. For example,here in ViT.beit&data2vec@OmarManzoorEnable dynamic resolution input for Beit#31053blip,blip_2@zafstojanoBlip dynamic input resolution#30722clipand clip related models:altclip,bridgetower,chinese_clip,git,kosmos2fixes clip interpolate#30783deitAdd dynamic resolution input/interpolate position embedding to deit#31131owlvit,owlv2@yMayanandperceiver@g1y5x3Perceiver interpolate position embedding#30979siglip@davidgxueAdd dynamic resolution input/interpolate position embedding to SigLIP#30719swin,donut,maskformer swin,swinv2Enable dynamic resolution input for Swin Transformer and variants#30656@the-neural-networkertvp@bhuvanmdevinterpolation added for TVP.#30863vit_maeInterpolate pos encode vitmae#30657added interpolation for vitmae model in pytorch as well as tf.#30732@bhuvanmdevvivitEnable dynamic resolution for vivit#30630@jla524MotivationLet's add this to more models, to leverage existing checkpoints for new cases!Your contributionFor anyone who would like to contribute, please comment on the issue, claiming a model you'd like to work on and share a link to the PR.Each PR should:Add aninterpolate_pos_encodingmethodAdd a test showing the model can correctly interpolate an input image of a different sizeThere was a PR opened to add this to CLIP models, which is now inactive, but useful for reference of the changes to make:#27457Once the PR is ready, you can ping me for review 🤗\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_29.txt:\n",
      "Title: Qwen2-VL Doesn't Execute on TPUs\n",
      "URL: https://github.com/huggingface/transformers/issues/33289\n",
      "Body:\n",
      "System Infotransformersversion: 4.45.0.dev0Platform: Linux-5.4.0-1043-gcp-x86_64-with-glibc2.31Python version: 3.10.14Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.5.0.dev20240830+cpu (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction#Following this Qwen2-VL guide =>https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct#quickstartScriptfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
      "from qwen_vl_utils import process_vision_info\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch_xla as xla\n",
      "import torch_xla.core.xla_model as xm\n",
      "import torch_xla.distributed.spmd as xs\n",
      "\n",
      "from torch.distributed._tensor import DeviceMesh, distribute_module\n",
      "from torch_xla.distributed.spmd import auto_policy\n",
      "\n",
      "from torch_xla import runtime as xr\n",
      "from torch_xla.experimental.spmd_fully_sharded_data_parallel import (\n",
      "    _prepare_spmd_partition_spec,\n",
      "    SpmdFullyShardedDataParallel as FSDPv2,\n",
      ")\n",
      "\n",
      "import time\n",
      "\n",
      "start = time.time()\n",
      "\n",
      "device = xm.xla_device()\n",
      "\n",
      "# default: Load the model on the available device(s)\n",
      "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
      "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
      "    torch_dtype=torch.bfloat16,\n",
      "    attn_implementation=\"eager\",\n",
      ").to(device)\n",
      "\n",
      "\n",
      "print(model.device)\n",
      "\n",
      "# default processer\n",
      "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\")\n",
      "\n",
      "\n",
      "message = [\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": [\n",
      "            {\n",
      "                \"type\": \"image\",\n",
      "                \"image\": \"image1.jpg\",\n",
      "            },\n",
      "            {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
      "        ],\n",
      "    }\n",
      "]\n",
      "\n",
      "all_messages = [[message] for _ in range(1)]\n",
      "for messages in all_messages:\n",
      "\n",
      "    # Preparation for inference\n",
      "    texts = [\n",
      "        processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
      "        for msg in messages\n",
      "    ]\n",
      "\n",
      "    image_inputs, video_inputs = process_vision_info(messages)\n",
      "    inputs = processor(\n",
      "        text=texts,\n",
      "        images=image_inputs,\n",
      "        videos=video_inputs,\n",
      "        padding=True,\n",
      "        return_tensors=\"pt\",\n",
      "    )\n",
      "    inputs = inputs.to(device)\n",
      "\n",
      "    # Inference: Generation of the output\n",
      "    generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
      "    generated_ids_trimmed = [\n",
      "        out_ids[len(in_ids) :]\n",
      "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
      "    ]\n",
      "    output_text = processor.batch_decode(\n",
      "        generated_ids_trimmed,\n",
      "        skip_special_tokens=True,\n",
      "        clean_up_tokenization_spaces=False,\n",
      "    )\n",
      "    for i, text in enumerate(output_text):\n",
      "        print(f\"Output {i}: {text}\")\n",
      "\n",
      "print(f\"Time taken: {time.time() - start}\")Output Logskojoe@t1v-n-cb70f560-w-0:~/EasyAnimate/easyanimate/image_caption$ python caption.py\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.39it/s]\n",
      "xla:0Expected behaviorThe model works fine when chagingdeviceto\"cpu\", but stuck executing on TPUs. The model should run on TPUs\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_15.txt:\n",
      "Title: [Nougat] image after NougatProcessor leads to Traceback\n",
      "URL: https://github.com/huggingface/transformers/issues/33341\n",
      "Body:\n",
      "System Infotransformersversion: 4.38.2Platform: Linux-5.15.0-52-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: 0.34.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: noUsing distributed or parallel set-up in script?: noWho can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionjust one linepython convert_nougat_to_hf.pyWhen i use convert_nougat_to_hf script to convert the model of facebook/nougat 0.1.0-base to huggingface version, it fails athttps://github.com/huggingface/transformers/blob/main/src/transformers/models/nougat/convert_nougat_to_hf.py#L180and the traceback shows as below:Traceback (most recent call last):File \"/data/czh/nougat/convert_nougat_to_hf.py\", line 285, inconvert_nougat_checkpoint(args.model_tag, args.pytorch_dump_folder_path, args.push_to_hub)File \"/data/czh/nougat/convert_nougat_to_hf.py\", line 180, in convert_nougat_checkpointpixel_values = processor(image, return_tensors=\"pt\").pixel_valuesFile \"/usr/local/lib/python3.10/dist-packages/transformers/models/nougat/processing_nougat.py\", line 92, incallinputs = self.image_processor(File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 551, incallreturn self.preprocess(images, **kwargs)File \"/usr/local/lib/python3.10/dist-packages/transformers/models/nougat/image_processing_nougat.py\", line 489, in preprocessimages = [self.thumbnail(image=image, size=size, input_data_format=input_data_format) for image in images]File \"/usr/local/lib/python3.10/dist-packages/transformers/models/nougat/image_processing_nougat.py\", line 489, inimages = [self.thumbnail(image=image, size=size, input_data_format=input_data_format) for image in images]File \"/usr/local/lib/python3.10/dist-packages/transformers/models/nougat/image_processing_nougat.py\", line 309, in thumbnailreturn resize(File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 330, in resizeresized_image = image.resize((width, height), resample=resample, reducing_gap=reducing_gap)File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 2206, in resizefactor_y = int((box[3] - box[1]) / size[1] / reducing_gap) or 1ZeroDivisionError: division by zerowhich means that the Transformer's version of NougatProcessor failes at processing the png below, while the original version of facebook nougat repo's prepare_input function can handle this png without error.Expected behaviorThe png above can be processed without error with Transformer NougatProcessor.the transformer's version of nougatprocessor can output the same as original_pixel_values.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_398.txt:\n",
      "Title: tracker:generatecomposability refactor\n",
      "URL: https://github.com/huggingface/transformers/issues/30810\n",
      "Body:\n",
      "generate+ composability = more use cases with minimal rewritesAs I write this issue,generateis mostly a sequential monolith. Many internal blocks were carved into functions over the last two years, but navigating there as a beginner is still messy. It is also very challenging to adaptgenerateto different tasks and/or modalities, forcing us to overwrite the entire generate function (e.g.RAG,MusicGen). All these aspects make using, documenting, maintaining, and testinggeneratea challenge.This issue is a tracker for the refactor ofgenerate, where we aim to build the structure outlined inthis board. Key ideas for this refactor:👉 All models can use the basegenerateAPI👉 Reduce if/else blocks👉 Reduce the barriers to entry for new decoding methods/modalities/use cases👉 Reduce per-model overwrites when possible👉 Add unit tests👉 Add documentation regarding the structure ofgenerateTasks1. Isolate prefill into a separate function, pulling it from the decoding methods. Note thata) prefill is done excluding the latest token (input_ids[:, -1:]), so we don't compute variables regarding the latest token twice;b) prefill only runs whenuse_cache=Trueand cache length < input length - 1;c)_expand_inputs_for_generationneeds to be changed (it copied inputs before prefill, we will need to copy prefill outputs)2. (depends on 1.) Separate generate on the 5 stages described in the diagram, passing around the data structures described therein3. (depends on 1.) Streaming 2.0a) Add option toyield/yield frominstead ofreturnb) Deprecate the old streamer classes;c) Add a new class to print the stream into the screen. For beam methods, build a class that prints up to the point where all beams agree with each other.d) thoroughly document and communicate this featuree) enable streaming into the screen withpipeline4. (depends on 2.) Separate stage 1 into a set of functions as described in the diagram. Add unit tests.5. (depends on 2.) Separate stage 2 into a set of functions as described in the diagram. Add unit tests. Move the preparation of common model inputs here, such asposition_ids.6. (depends on 2.) Separate stage 3 into a set of functions as described in the diagram. Add unit tests. DeprecateLogitsWarperin this step (it's a copy ofLogitsProcessor)7. (depends on 2.) Separate stage 5 into a set of functions as described in the diagram. Add unit tests.8. Add a new document page walking through the structure ofgenerate[From this point onwards the tasks are only a sketch, need more detailed planning when we get there]9. Reduce if/elses through templates (e.g. LLMs have a certain default forprepare_inputs_for_generation, VLMs also have their special preprocessing steps, ...)10. Play around with caching of some blocks to determine whether it speeds up generation11. Reworkprepare_inputs_for_generation?12. Removegeneratefrom models that have a custom implementation(other tasks, TBD)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_165.txt:\n",
      "Title: CLIP is ExecuTorch compatible\n",
      "URL: https://github.com/huggingface/transformers/issues/32506\n",
      "Body:\n",
      "Feature requestEnable CLIP to\"Export to ExecuTorch\"workflowMotivationSee details in#32253Your contributionSupport on common export issues\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_603.txt:\n",
      "Title: XLMRoberta with Flash Attention 2\n",
      "URL: https://github.com/huggingface/transformers/issues/27957\n",
      "Body:\n",
      "System Infotransformers version: 4.36.0Platform: Linux-4.19.0-22-amd64-x86_64-with-glibc2.31Python version: 3.10.13Huggingface_hub version: 0.19.4Safetensors version: 0.4.0Accelerate version: 0.24.1Accelerate config:   not foundPyTorch version (GPU?): 2.0.1+cu117 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@ArthurZucker@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoModelForSequenceClassificationmodel = AutoModelForSequenceClassification.from_pretrained(\"my_model/\", attn_implementation=\"flash_attention_2\")Expected behaviorAbility to use flash attention 2 for inference. Is it possible to add support of flash attention 2 for XLMRoberta model?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_832.txt:\n",
      "Title: ERNIE and tensorflow2\n",
      "URL: https://github.com/huggingface/transformers/issues/19511\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_198.txt:\n",
      "Title: ZoeDepth outputs include padding --> not referenced in the docs & solution is not obvious\n",
      "URL: https://github.com/huggingface/transformers/issues/32381\n",
      "Body:\n",
      "System Infotransformers==4.43.0Who can help?@amyeroberts@stevhliuInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionHello everyone,I stumbled across#30917while trying to figure out what was going on with the output of the ZoeDepth model. However, since this issue is quite a bit more general (about depth estimators) I open this issue specifically about the outputs of ZoeDepth.Explaining/Showing the problemFor this model, theImageProcessoradds reflection padding around the input images to fix the boundary artifacts in the output depth map. As a result, the depth predictions and image outputted by the model include padding; in contrast to every other depth model here.This can be easily seen by running the code below:fromtransformersimportAutoImageProcessorfromPILimportImageimportrequestsurl=\"https://www.greece-is.com/wp-content/uploads/2016/07/ATH_RIVIERA_naos-poseidona-sounio-01.jpg\"image=Image.open(requests.get(url,stream=True).raw)image_processor=AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu\")img=image_processor(images=image,return_tensors=\"pt\",do_normalize=False)[\"pixel_values\"].squeeze().permute(1,2,0).cpu().numpy()pil=Image.fromarray((img*255/img.max()).astype(\"uint8\"))pil.thumbnail((512,512))pil.show()that produces the following image:For reference, here's the same image and the same code, but withdo_pad=Falseinside theimage_processorcall.SolutionImportantWhile the issue is still open, please add a note in the docs about this discrepancy between ZoeDepth and the rest of the depth prediction models; it took me more time than it should to understand what's going on.@stevhliuThe notebook link by@NielsRoggeprobablyhas some code to show how the padding is added and removed, however since I do not have access to it, I'll share here the various info I have gathered as well as a postprocessing functionroughlyfollowing the style ofpost_process_object_detection.Finding out exactly what's going on under the hoodAs you can seein the original repoas well as in theZoeDepthImageProcessor, before inference, the images padded in both dimensions by:pad_h=int(np.sqrt(img_height/2)*fh)# height paddingpad_w=int(np.sqrt(img_width/2)*fw)# width paddingWherefhandfware equal to3by default.Then, the images are resized and are fed into the model.Thus, to get the final depth predictions and image corresponding to the input image, you need to:Resize the output to the size of the input image PLUS the paddingRemove the paddingZoeDepthpost_process_depth_estimationfromtypingimportUnion,List,Tuple,DictfromPILimportImageimporttorchfromtorch.nnimportfunctionalasFfromtransformers.models.zoedepth.modeling_zoedepthimportZoeDepthDepthEstimatorOutputdefpost_process_depth_estimation_zoedepth(outputs:ZoeDepthDepthEstimatorOutput,source_sizes:Union[torch.Tensor,List[Tuple[int,int]]],target_sizes:Union[torch.Tensor,List[Tuple[int,int]]]=None,remove_padding:bool=True,\n",
      ")->List[Dict] :\"\"\"Converts the raw output of [`ZoeDepthDepthEstimatorOutput`] into final depth predictions and depth PIL image.Only supports PyTorch.Args:outputs ([`ZoeDepthDepthEstimatorOutput`]):Raw outputs of the model.source_sizes (`torch.Tensor` or `List[Tuple[int, int]]`):Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the source size(height, width) of each image in the batch before preprocessing.target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size(height, width) of each image in the batch. If left to None, predictions will not be resized.remove_padding (`bool`):By default ZoeDepth addes padding to fix the boundary artifacts in the output depth map, so we needremove this padding during post_processing. The parameter exists here in case the user changed theimage preprocessing to not include padding.Returns:`List[Dict]`: A list of dictionaries, each dictionary containing the depth predictions and a depth PILimage as predicted by the model.\"\"\"predicted_depth=outputs.predicted_depthif(target_sizesisnotNone)and(len(predicted_depth)!=len(target_sizes)):raiseValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the predicted depth\")if(source_sizesisNone)or(len(predicted_depth)!=len(source_sizes)):raiseValueError(\"Make sure that you pass in as many source image sizes as the batch dimension of the logits\")# Zoe Depth model adds padding around the images to fix the boundary artifacts in the output depth map# The padding length is `int(np.sqrt(img_h/2) * fh)` for the height and similar for the width# fh (and fw respectively) are equal to '3' by default# Check [here](https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L57)# for the original implementation.# In this section, we remove this padding to get the final depth image and depth predictionifisinstance(source_sizes,List):img_h=torch.Tensor([i[0]foriinsource_sizes])img_w=torch.Tensor([i[1]foriinsource_sizes])else:img_h,img_w=source_sizes.unbind(1)fh=fw=3results=[]fori, (d,s)inenumerate(zip(predicted_depth,source_sizes)):ifremove_padding:pad_h=int(np.sqrt(s[0]/2)*fh)pad_w=int(np.sqrt(s[1]/2)*fw)d=F.interpolate(d.unsqueeze(0).unsqueeze(1),size=[s[0]+2*pad_h,s[1]+2*pad_w],mode=\"bicubic\",align_corners=False)ifpad_h>0:d=d[:, :,pad_h:-pad_h, :]ifpad_w>0:d=d[:, :, :,pad_w:-pad_w]iftarget_sizesisnotNone:target_size=target_sizes[i]d=F.interpolate(d,size=target_size,mode=\"bicubic\",align_corners=False)d=d.squeeze().cpu().numpy()pil=Image.fromarray((d*255/np.max(d)).astype(\"uint8\"))results.append({\"predicted_depth\":d,\"depth\":pil})returnresultsWhich you can double-check using the testing code below:fromPILimportImageimportrequestsimporttorchimportnumpyasnpfromtransformersimportAutoImageProcessor,ZoeDepthForDepthEstimationimage_processor=AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu\")model=ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu\")# prepare image for the modelurl=\"https://www.greece-is.com/wp-content/uploads/2016/07/ATH_RIVIERA_naos-poseidona-sounio-01.jpg\"image=Image.open(requests.get(url,stream=True).raw)inputs=image_processor(images=image,return_tensors=\"pt\")withtorch.no_grad():outputs=model(**inputs)processed_output=post_process_depth_estimation_zoedepth(outputs, [image.size[::-1]])[0]print(\"Input image size (h, w):\",image.size[::-1])print(\"Output predicted depth shape (h, w):\",processed_output[\"predicted_depth\"].shape)print(\"Output depth image size (h, w):\",processed_output[\"depth\"].size[::-1])processed_output[\"depth\"].show()The test code should output the following image:Defaultpost_process_depth_estimationFor the sake of completeness, I also share thedefaultpost_process_depth_estimationfunction for the rest of the models that do not have padded outputs:fromtypingimportUnion,List,Tuple,DictfromPILimportImageimporttorchfromtorch.nnimportfunctionalasFfromtransformers.models.zoedepth.modeling_zoedepthimportZoeDepthDepthEstimatorOutputdefpost_process_depth_estimation_default(outputs,target_sizes:Union[torch.Tensor,List[Tuple[int,int]]]=None)->List[Dict] :\"\"\"Converts the raw output of [`*DepthEstimatorOutput`] into final depth predictions and depth PIL image.Only supports PyTorch.Args:outputs ([`*DepthEstimatorOutput`]):Raw outputs of the model.target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size(height, width) of each image in the batch. If left to None, predictions will not be resized.Returns:`List[Dict]`: A list of dictionaries, each dictionary containing the depth predictions and a depth PILimage as predicted by the model.\"\"\"predicted_depth=outputs.predicted_depthif(target_sizesisnotNone)and(len(predicted_depth)!=len(target_sizes)):raiseValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the predicted depth\")results=[]fori,dinenumerate(predicted_depth):iftarget_sizesisnotNone:target_size=target_sizes[i]d=F.interpolate(d,size=target_size,mode=\"bicubic\",align_corners=False)d=d.squeeze().cpu().numpy()pil=Image.fromarray((d*255/np.max(d)).astype(\"uint8\"))results.append({\"predicted_depth\":d,\"depth\":pil})returnresultsExpected behaviorTo be coherent with the rest of the depth estimation models, the ideal scenario would be for the ZoeDepth to output directly a cropped image where the padding would be removed. However, I understand that this is not very easy considering that an additional input forsource_sizeshould probably be added in this case.Comparison with official source implementationTo run the code in this section, you'll needtimm==0.6.11Still, when comparing the outputs of theofficial implementationwith the model in HF with the post-processing above, there is a small discrepancy and I am not yet sure who to blame. To use the official implementation, run the code below:fromPILimportImageimporttorchmodel=torch.hub.load('isl-org/ZoeDepth',\"ZoeD_N\",pretrained=True).eval()orig_output=model.infer_pil(image,output_type=\"numpy\")orig_pil=Image.fromarray((orig_output*255/orig_output.max()).astype(\"uint8\"))print(\"Input image size (h, w):\",image.size[::-1])print(\"Output predicted depth shape (h, w):\",orig_output.shape)print(\"Output depth image size (h, w):\",orig_pil.size[::-1])orig_pil.show()The output image looks very much like the one before:However, when comparing the outputs here with the outputs beforeimportnumpyasnperror=orig_output-processed_output[\"predicted_depth\"]print(\"Max error:\",error.max())print(\"Min error:\",error.min())print(\"MSE:\", (error**2).mean())print(\"RMSE:\",np.sqrt((error**2).mean()))print(\"MAE:\", (np.abs(error).mean()))Image.fromarray((error*255/error.max()).astype(\"uint8\")).show()we getMax error: 0.7920196\n",
      "Min error: -0.21409726\n",
      "MSE: 0.0025115514\n",
      "RMSE: 0.05011538\n",
      "MAE: 0.033222165Conclusion - TL;DRZoeDepth adds padding to the images during preprocessing.Currently, the output from Transformers for this model also includes padding. This is very weird since it isnot mentioned anywhere in the documentationwhile at the same time, all the other depth models do not do this.Here, I share apost_processing_depth_estimationfunction that removes the padding, resizes the model output to match the input image, and additionally returns a depth image.In an ideal world, to be coherent with the rest of the models and avoid confusion, I believe that the output of the ZoeDepth model should somehow be already cropped to remove the padding; however this is not very easy considering its inputs (an input forsource_sizeshould be added in this case).Still, after using the function above, there is a discrepancy between the output I get using HF vs using theoriginal implementation\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1027.txt:\n",
      "Title: Unexpected/wrong handling of added special tokens in special_tokens_mask (GPT1, BERT, possibly others)\n",
      "URL: https://github.com/huggingface/transformers/issues/7951\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_629.txt:\n",
      "Title: Audio-MAE - ViTMAE for audio\n",
      "URL: https://github.com/huggingface/transformers/issues/27453\n",
      "Body:\n",
      "Model descriptionThis model is is a Self-supervised Vision Transformer that uses patch reconstruction as the spectrogram task. It extends MAE (which is already on HuggingFace) for audio. This model would be a valuable addition as there doesn't seem to be a self-supervised ViT model on HugginFace currently. AST is the closest and uses supervised pre-training. Conceptually, Audio-MAE is also simpler but achieves comparable performance in their paper.Some differences compared to the standard MAE ModelDuring pre-training local and hybrid attention mechanisms can be used.During fine-tuning masking is also used which differs to MAE.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationImplementationhttps://github.com/facebookresearch/AudioMAECreated by Po-Yao Huang@berniebearon GithubPre-trained WeightsAvailable in the github repoPaper (Masked Autoencoders that Listen)https://arxiv.org/abs/2207.06405\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_173.txt:\n",
      "Title: Use unidic-lite instead of ipadic for Japanese tokenization\n",
      "URL: https://github.com/huggingface/transformers/issues/32482\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: Linux-6.9.3-76060903-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.23.3Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: NoUsing GPU in script?: YesGPU type: NVIDIA GeForce RTX 2080 TiWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionIn line 383 of this filesrc/transformers/models/bert_japanese/tokenization_bert_japanese.py, the default dictionary is set to beipadicand I have to installipadic-py. Butipadic-py'sGitHub pagesaid \"You Shouldn't Use This\" and recommend using UniDic.However, although I installedunidic-liteonly, transformers still needipadic. I have to modify the transformers source code to useunidic-lite. I changed the  line 383 ofsrc/transformers/models/bert_japanese/tokenization_bert_japanese.py:- mecab_dic: Optional[str] = \"ipadic\",\n",
      "+ mecab_dic: Optional[str] = \"unidic-lite\",I think the official version should also be updated to useunidic-litefor a modern Japanese tokenization.My script:import random\n",
      "import glob\n",
      "from tqdm import tqdm\n",
      "\n",
      "import torch\n",
      "from torch.utils.data import DataLoader\n",
      "from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n",
      "import pytorch_lightning as pl\n",
      "\n",
      "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
      "\n",
      "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
      "bert_sc = BertForSequenceClassification.from_pretrained(\n",
      "    MODEL_NAME, num_labels=10\n",
      ")\n",
      "# bert_sc = bert_sc.cuda()\n",
      "\n",
      "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)Expected behaviorChange the default dictionary for Japanese tokenization fromipadictounidic-lite.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_615.txt:\n",
      "Title: Add support for llama.cpp\n",
      "URL: https://github.com/huggingface/transformers/issues/27712\n",
      "Body:\n",
      "Feature requestI would like to requestllama.cppas a new model backend in the transformers library.Motivationllama.cpp offers:Excellent performance in scenarios where memory bandwidth is an issue, namely CPU inference and GPU + CPU inference.Support for a wide range of GPU vendors and models.Adequate quantization accuracy -- I have compared the perplexities of 4-bit GGUF models to GPTQ, AWQ, EXL2, and bitsandbytes and found them to be competitive (link).By making the transformers library compatible with GGUF models, the llama.cpp performance on consumer hardware could hopefully be integrated with the features available in transformers and its surrounding ecosystem. In particular, it would be interesting to see the following working seamlessly with llama.cpp:Assisted generation(speculative decoding)StreamingLLMYour contributionI have implemented a \"llamacpp_HF\" wrapper in the file below:https://github.com/oobabooga/text-generation-webui/blob/main/modules/llamacpp_hf.pyIt makes it possible to use the transformersmodel.generatewith llama.cpp models, and it exemplifies how to make forward calls in llama.cpp and get the logits. It works for perplexity evaluation whenlogits_all=Trueis passed while loading the model. I additionally implemented some prefix-matching logic and a hacky way to recognize forward calls for negative prompts to make CFG functional.For the llama.cpp transformers integration, I recommend the following:Relying on the llama-cpp-python library:https://github.com/abetlen/llama-cpp-python/Requiring the user to manually install llama-cpp-python with the appropriate command for their hardware rather than adding it as a direct requirement to transformers. I believe that's how it already works for GPTQ models, where AutoGPTQ has to be installed manually.In thefrom_pretrainedcall, having aLlamaCppConfigobject that takes as input arbitrary kwargs that later on get passed to thellama_cpp.Llamamodel loading call. That would be similar to theBitsAndBytesConfigobject that is passed tofrom_pretrainedwhenload_in_4bit=Trueis used. Some important parameters aren_gpu_layersandn_ctx; it would be interesting to make this future-proof and allow arbitrary kwargs to be passed toLlamaCppConfig.I'll tag@younesbelkadawho worked with RWKV and AWQ integration in transformers and may find this interesting.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_601.txt:\n",
      "Title: ImageToTextPipeline does not support InstructBlip Models\n",
      "URL: https://github.com/huggingface/transformers/issues/27975\n",
      "Body:\n",
      "System Infotransformersversion: 4.36.0.dev0Platform: Linux-generic-x86_64Python version: 3.8.12Huggingface_hub version: 0.19.4Safetensors version: 0.4.1Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 1.10.0a0+0aef44c (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: noWho can help?@Narsil@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionprocessor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-flan-t5-xl\")pipe = pipeline(\"image-to-text\", model=\"Salesforce/instructblip-flan-t5-xl\", processor=processor.image_processor, tokenizer=processor.tokenizer, device=0)prompt = \"describe te following image\"url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"image = Image.open(requests.get(url, stream=True).raw)pipe(images=image, prompt=prompt)Expected behaviorreturns a textual description of the image.Instead, I get an error:TypeError: ones_like(): argument 'input' (position 1) must be Tensor, not NoneTypeI suspect this is caused by theImageToTextPipeline.preprocess(), where we should ave custom behaviour for InstructBlip models to process the image and text in one go:inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_167.txt:\n",
      "Title: Implementgenerate(inference) for torch exported text-generation models\n",
      "URL: https://github.com/huggingface/transformers/issues/32504\n",
      "Body:\n",
      "Feature requestUnliketorch.compile,torch.exportonly export the inner transformer to predict a single token for text-generation models. The autoregressive logics are not included in the exported artifact. For ExecuTorch Runtime, the autoregressive logics is implemented there, either directly in c++ (code pointer) or python (code pointer). Here is a minimalgenerateimpl in a test for Phi3-mini in 🤗  (code pointer).Instead of duplicate the generate logics for each exported model, it's better to have a common implementation ofgeneratethat can be used by any exported text-generation mdoel.MotivationTo support the new \"Export to ExecuTorch\" workflow. This would enable users to have a unifiedgenerateexperience as they run inference using eager or compiled model.Your contributionTBD\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_17.txt:\n",
      "Title: Training Resumes with Increased Loss Despite Checkpoint Loading\n",
      "URL: https://github.com/huggingface/transformers/issues/33336\n",
      "Body:\n",
      "Problem: When resuming the training of a BERT model with the Hugging Face Trainer from a checkpoint, the loss value increases again in the second run, even though the checkpoint is loaded correctly and the global_step, optimizer state, and scheduler state are restored.Troubleshooting Steps Taken:Manually Setting global_step:Set the global_step manually in the Trainer after loading the checkpoint.Result: Problem not resolved.Overriding the train() method:Created a new class MyTrainer inheriting from Trainer and overrode the train() method to set the global_step when loading a checkpoint.Result: Problem not resolved.Removing resume_from_checkpoint:Removed the resume_from_checkpoint argument from the trainer.train() call and manually loaded the global_step, optimizer state, and scheduler state.Result: Problem not resolved.Resetting/Deactivating the Learning Rate Scheduler:Reinitialized the scheduler after loading the checkpoint, skipped the step() call, or completely deactivated the scheduler.Result: Problem not resolved. The learning rate is still set to 0.0.Manually Setting the Learning Rate:Manually set the learning rate of the parameter groups in the optimizer after loading the checkpoint.Result: Problem not resolved. The scheduler resets the learning rate back to 0.0.Explicitly Calculating num_training_steps:Explicitly calculated the number of training steps and stored it in the num_training_steps variable, using it when initializing the scheduler.Result: Problem not resolved.Manually Moving trainer_state.json into Checkpoint Subfolder:Moved the trainer_state.json file after trainer.save_state() using shutil.move() into the checkpoint-XXXX subfolder.Result: Problem not resolved.Manually Setting TrainerState Attributes:Instead of overwriting the entire trainer.state variable, the global_step and epoch attributes were set individually after loading the checkpoint.Result: Problem not resolved.Key Observations:The error only occurs when resuming training from a checkpoint. The first run works flawlessly.The training data, environment, and hardware remain identical between runs.The global_step is loaded correctly and set in the Trainer.The optimizer and scheduler states are loaded correctly.The TrainerState is loaded correctly.Manually setting the learning rate has no effect, the scheduler resets it.Suspicions:The Trainer might internally reset the global_step or the optimizer state after the checkpoint is loaded.There might be a bug in the Trainer class that prevents the correct restoration of the training state.Question for Hugging Face Transformers:Why does the loss value increase when resuming training from a checkpoint in the second run, even though the checkpoint is loaded correctly? Are there any known issues with the Trainer regarding the restoration of the training state, especially the learning rate and scheduler? We have tried numerous troubleshooting steps (listed above) without success. We suspect a potential bug within the Trainer itself. Could you provide guidance or insights on how to resolve this issue?Additional Information:Model: BertForMaskedLMTrainer: transformers.TrainerScheduler: The default scheduler of the Trainer (linear warmup scheduler with subsequent linear decay)Optimizer: AdamWDataset: A custom text datasetCode: The relevant code is provided above.Logs: Detailed logs of the training process can be provided if needed.Goal:We want to be able to resume training from checkpoints without the loss value increasing again and the training having to start from scratch. We hope that the Hugging Face Transformers team can help us solve this problem.File:import os\n",
      "import shutil\n",
      "import torch\n",
      "import warnings\n",
      "import logging\n",
      "import json \n",
      "from transformers import BertForMaskedLM, BertTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
      "from datasets import load_dataset\n",
      "from config import load_config\n",
      "from datetime import datetime\n",
      "\n",
      "# Suppress specific FutureWarnings\n",
      "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"accelerate\")\n",
      "\n",
      "# Format the logging output\n",
      "logging.basicConfig(filename='training.log', level=logging.INFO, \n",
      "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
      "\n",
      "class MyTrainer(Trainer):\n",
      "    def train(self, **kwargs):\n",
      "        \"\"\"\n",
      "        Overridden train() method that adds additional logging information.\n",
      "        \"\"\"\n",
      "        # Log the global_step and epoch before training\n",
      "        logging.info(f\"Global Step before training: {self.state.global_step}\")\n",
      "        logging.info(f\"Epoch before training: {self.state.epoch}\")\n",
      "\n",
      "        # Call the train() method of the superclass\n",
      "        super().train(**kwargs)\n",
      "\n",
      "        # Log the global_step and epoch after training\n",
      "        logging.info(f\"Global Step after training: {self.state.global_step}\")\n",
      "        logging.info(f\"Epoch after training: {self.state.epoch}\")\n",
      "\n",
      "    def _inner_training_loop(\n",
      "        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Override _inner_training_loop() to log the global_step, epoch,\n",
      "        and learning rate after each step.\n",
      "        \"\"\"\n",
      "        output = super()._inner_training_loop(batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n",
      "\n",
      "        # Log the global_step, epoch, and learning rate after each training step\n",
      "        logging.debug(f\"Global Step after step: {self.state.global_step}\")\n",
      "        logging.debug(f\"Epoch after step: {self.state.epoch}\")\n",
      "        logging.debug(f\"Learning rate after step: {self.lr_scheduler.get_last_lr()}\")\n",
      "\n",
      "        return output\n",
      "\n",
      "def train_with_books(config_params, run_id):\n",
      "    \"\"\"\n",
      "    Function to train BERT with the books in the books folder.\n",
      "    \"\"\"\n",
      "    model_name = \"bert-base-german-cased\"\n",
      "    model_path = config_params[\"bert_model_path\"]  \n",
      "    trained_model_path = config_params[\"trained_model_path\"]\n",
      "    checkpoint_path = config_params[\"checkpoint_path\"]  \n",
      "    log_path = os.path.join(config_params[\"logs_path\"], f\"run-{run_id}\") \n",
      "    global_step_file = os.path.join(checkpoint_path, \"global_step.txt\") \n",
      "\n",
      "    # Initialize logging and output configuration\n",
      "    logging.info(f\"Model Path: {model_path}\")\n",
      "    logging.info(f\"Trained Model Path: {trained_model_path}\")\n",
      "    logging.info(f\"Checkpoint Path: {checkpoint_path}\")\n",
      "    logging.info(f\"Log Path: {log_path}\")\n",
      "\n",
      "    trainer_state = None # Define trainer_state before the if-block\n",
      "    optimizer_state = None\n",
      "    scheduler_state = None\n",
      "    global_step = 0\n",
      "    epoch = 0 \n",
      "\n",
      "    # 1. Check if a checkpoint exists\n",
      "    if os.path.exists(checkpoint_path) and any(fname.startswith(\"checkpoint\") for fname in os.listdir(checkpoint_path)):\n",
      "        latest_checkpoint = max([f for f in os.listdir(checkpoint_path) if f.startswith(\"checkpoint\")], key=lambda x: int(x.split('-')[-1]))\n",
      "        checkpoint_path = os.path.join(checkpoint_path, latest_checkpoint)\n",
      "        logging.info(f\"Loading checkpoint from {checkpoint_path}...\")\n",
      "\n",
      "        # Load the optimizer and scheduler state from the checkpoint\n",
      "        optimizer_path = os.path.join(checkpoint_path, \"optimizer.pt\")\n",
      "        scheduler_path = os.path.join(checkpoint_path, \"scheduler.pt\")\n",
      "        if os.path.exists(optimizer_path) and os.path.exists(scheduler_path):\n",
      "            optimizer_state = torch.load(optimizer_path)\n",
      "            scheduler_state = torch.load(scheduler_path)\n",
      "            logging.info(f\"Optimizer and scheduler state loaded from checkpoint: {optimizer_path}, {scheduler_path}\")\n",
      "        else:\n",
      "            logging.warning(f\"Optimizer and/or scheduler state not found in checkpoint.\")\n",
      "\n",
      "        # Load the model from the checkpoint\n",
      "        model = BertForMaskedLM.from_pretrained(checkpoint_path)\n",
      "\n",
      "        # Load global step from the checkpoint\n",
      "        with open(global_step_file, \"r\") as f:\n",
      "            global_step = int(f.read())\n",
      "        logging.info(f\"Global step loaded from {global_step_file}: {global_step}\") \n",
      "\n",
      "        # Load epoch from the trainer state in the checkpoint (if available)\n",
      "        trainer_state_path = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
      "        if os.path.exists(trainer_state_path):\n",
      "            with open(trainer_state_path, 'r') as f:\n",
      "                trainer_state = json.load(f) \n",
      "            epoch = trainer_state.get(\"epoch\", 0)\n",
      "            logging.info(f\"Epoch loaded from trainer state: {epoch}\")\n",
      "        else:\n",
      "            logging.warning(f\"Trainer state not found in checkpoint.\")\n",
      "    # 2. Check if a trained model exists\n",
      "    elif os.path.exists(trained_model_path):\n",
      "        logging.info(f\"Loading trained model from {trained_model_path}...\")\n",
      "        model = BertForMaskedLM.from_pretrained(trained_model_path, local_files_only=True, ignore_mismatched_sizes=True)\n",
      "    # 3. Check if the original model exists\n",
      "    elif os.path.exists(model_path):\n",
      "        logging.info(f\"Loading original model from {model_path}...\")\n",
      "        model = BertForMaskedLM.from_pretrained(model_path, local_files_only=True, ignore_mismatched_sizes=True)\n",
      "    # 4. Download the model from Hugging Face\n",
      "    else:\n",
      "        logging.info(f\"Downloading model {model_name} from Hugging Face...\")\n",
      "        model = BertForMaskedLM.from_pretrained(model_name, cache_dir=None, ignore_mismatched_sizes=True)\n",
      "        model.save_pretrained(model_path)\n",
      "        logging.info(f\"Model saved to {model_path}.\")\n",
      "\n",
      "    tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=model_path)\n",
      "\n",
      "    # Load books dataset\n",
      "    books_dataset = load_dataset('text', data_files=f\"{config_params['books_dataset_path']}/*.txt\")\n",
      "\n",
      "    def tokenize_function(examples):\n",
      "        return tokenizer(\n",
      "            examples[\"text\"],\n",
      "            padding=\"max_length\",\n",
      "            truncation=True,\n",
      "            max_length=128\n",
      "        )\n",
      "\n",
      "    # Tokenize the dataset\n",
      "    tokenized_datasets = books_dataset.map(\n",
      "        tokenize_function,\n",
      "        batched=True,\n",
      "        num_proc=4, \n",
      "        remove_columns=[\"text\"]\n",
      "    )\n",
      "\n",
      "    # Training Arguments\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=checkpoint_path,\n",
      "        overwrite_output_dir=True,\n",
      "        num_train_epochs=3,\n",
      "        per_device_train_batch_size=192, \n",
      "        save_steps=10_000,\n",
      "        save_total_limit=2,\n",
      "        fp16=True,\n",
      "        gradient_accumulation_steps=2, \n",
      "        logging_dir=log_path,  \n",
      "        logging_steps=20,\n",
      "        report_to=\"tensorboard\",\n",
      "        save_strategy=\"steps\",  \n",
      "        save_safetensors=False,\n",
      "        dataloader_num_workers=4 \n",
      "    )\n",
      "\n",
      "    # Data Collator for Masked Language Modeling\n",
      "    data_collator = DataCollatorForLanguageModeling(\n",
      "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
      "    )\n",
      "\n",
      "    # Define the trainer\n",
      "    trainer = MyTrainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        train_dataset=tokenized_datasets[\"train\"],\n",
      "        data_collator=data_collator\n",
      "    )\n",
      "\n",
      "    # Set global step in the trainer\n",
      "    trainer.state.global_step = global_step\n",
      "    # Set epoch in the trainer\n",
      "    trainer.state.epoch = epoch\n",
      "\n",
      "    # Initialize optimizer and scheduler\n",
      "    trainer.create_optimizer_and_scheduler(num_training_steps=trainer.state.max_steps)\n",
      "    logging.info(f\"Optimizer and scheduler initialized.\")\n",
      "\n",
      "    # Load optimizer and scheduler state after creating the trainer\n",
      "    if optimizer_state is not None and scheduler_state is not None:\n",
      "        trainer.optimizer.load_state_dict(optimizer_state)\n",
      "        trainer.lr_scheduler.load_state_dict(scheduler_state)\n",
      "        logging.info(f\"Optimizer and scheduler state loaded\")\n",
      "\n",
      "        # Manually set learning rate\n",
      "        for i, param_group in enumerate(trainer.optimizer.param_groups):\n",
      "            param_group['lr'] = 5e-5 \n",
      "            logging.info(f\"Learning rate for parameter group {i} manually set: {param_group['lr']}\") \n",
      "\n",
      "    # Set TrainerState attributes (only if loaded from checkpoint)\n",
      "    if trainer_state is not None:\n",
      "        trainer.state.global_step = trainer_state.get(\"global_step\", 0)\n",
      "        trainer.state.epoch = trainer_state.get(\"epoch\", 0)\n",
      "        logging.info(f\"TrainerState attributes set in the trainer.\")\n",
      "\n",
      "    # Debug information before training\n",
      "    logging.info(f\"Trainer state before training: {trainer.state}\") \n",
      "    logging.info(f\"Optimizer: {trainer.optimizer}\")\n",
      "    logging.info(f\"LR Scheduler: {trainer.lr_scheduler}\")\n",
      "\n",
      "    # Start training\n",
      "    logging.info(\"Starting training ...\")\n",
      "    trainer.train()  \n",
      "\n",
      "    # Save global step after training\n",
      "    with open(global_step_file, \"w\") as f:\n",
      "        f.write(str(trainer.state.global_step))\n",
      "    logging.info(f\"Global step after training saved to {global_step_file}: {trainer.state.global_step}\")\n",
      "\n",
      "    # Debug information after training\n",
      "    logging.info(f\"Trainer state after training: {trainer.state}\") \n",
      "\n",
      "    # Save the final model\n",
      "    trainer.save_model(trained_model_path)\n",
      "    logging.info(f\"Final model saved to {trained_model_path}.\") \n",
      "\n",
      "    # Save the current checkpoint in a subfolder\n",
      "    checkpoint_save_path = os.path.join(checkpoint_path, f\"checkpoint-{trainer.state.global_step}\")\n",
      "    os.makedirs(checkpoint_save_path, exist_ok=True) \n",
      "    trainer.save_model(checkpoint_save_path)\n",
      "    trainer.save_state()\n",
      "    # Move trainer_state.json to the subfolder\n",
      "    trainer_state_source = os.path.join(checkpoint_path, \"trainer_state.json\")\n",
      "    trainer_state_dest = os.path.join(checkpoint_save_path, \"trainer_state.json\")\n",
      "    if os.path.exists(trainer_state_source):\n",
      "        shutil.move(trainer_state_source, trainer_state_dest)\n",
      "        logging.info(f\"TrainerState saved to {trainer_state_dest}.\")\n",
      "\n",
      "    # Save optimizer and scheduler state in the checkpoint\n",
      "    torch.save(trainer.optimizer.state_dict(), os.path.join(checkpoint_save_path, \"optimizer.pt\"))\n",
      "    torch.save(trainer.lr_scheduler.state_dict(), os.path.join(checkpoint_save_path, \"scheduler.pt\"))\n",
      "    logging.info(f\"Optimizer and scheduler state saved to {checkpoint_save_path}.\") \n",
      "\n",
      "    # Output relevant variables\n",
      "    logging.info(f\"trainer.state.global_step: {trainer.state.global_step}\")\n",
      "    checkpoint_files = [f for f in os.listdir(checkpoint_save_path)]\n",
      "    logging.info(f\"Checkpoint files in {checkpoint_save_path}: {checkpoint_files}\")Terminal Output:Run:{‘loss’: 2.8581, ‘grad_norm’: 2.897771120071411, ‘learning_rate’: 4.682539682539683e-05, ‘epoch’: 0.19}{‘loss’: 2.1545, ‘grad_norm’: 2.8640339374542236, ‘learning_rate’: 4.3650793650793655e-05, ‘epoch’: 0.38}{‘loss’: 2.0396, ‘grad_norm’: 2.7819628715515137, ‘learning_rate’: 4.047619047619048e-05, ‘epoch’: 0.57}{‘loss’: 1.9786, ‘grad_norm’: 2.644606828689575, ‘learning_rate’: 3.730158730158731e-05, ‘epoch’: 0.76}{‘loss’: 1.9553, ‘grad_norm’: 2.7417070865631104, ‘learning_rate’: 3.412698412698413e-05, ‘epoch’: 0.95}{‘loss’: 1.8961, ‘grad_norm’: 2.6237854957580566, ‘learning_rate’: 3.095238095238095e-05, ‘epoch’: 1.14}{‘loss’: 1.8793, ‘grad_norm’: 2.5830185413360596, ‘learning_rate’: 2.777777777777778e-05, ‘epoch’: 1.33}{‘loss’: 1.8715, ‘grad_norm’: 2.652275800704956, ‘learning_rate’: 2.4603174603174602e-05, ‘epoch’: 1.52}{‘loss’: 1.8362, ‘grad_norm’: 2.6065754890441895, ‘learning_rate’: 2.1428571428571428e-05, ‘epoch’: 1.71}{‘loss’: 1.8474, ‘grad_norm’: 2.6352243423461914, ‘learning_rate’: 1.8253968253968254e-05, ‘epoch’: 1.9}{‘loss’: 1.8197, ‘grad_norm’: 2.56719708442688, ‘learning_rate’: 1.5079365079365079e-05, ‘epoch’: 2.09}{‘loss’: 1.826, ‘grad_norm’: 2.5195322036743164, ‘learning_rate’: 1.1904761904761905e-05, ‘epoch’: 2.27}{‘loss’: 1.8074, ‘grad_norm’: 2.614032506942749, ‘learning_rate’: 8.73015873015873e-06, ‘epoch’: 2.46}{‘loss’: 1.8029, ‘grad_norm’: 2.600111246109009, ‘learning_rate’: 5.555555555555556e-06, ‘epoch’: 2.65}{‘loss’: 1.7879, ‘grad_norm’: 2.4874589443206787, ‘learning_rate’: 2.3809523809523808e-06, ‘epoch’: 2.84}{‘train_runtime’: 142.9413, ‘train_samples_per_second’: 846.431, ‘train_steps_per_second’: 2.204, ‘train_loss’: 1.9500562516469804, ‘epoch’: 2.99}Run:{‘loss’: 1.453, ‘grad_norm’: 2.474428415298462, ‘learning_rate’: 4.682539682539683e-05, ‘epoch’: 0.19}{‘loss’: 1.406, ‘grad_norm’: 2.5064773559570312, ‘learning_rate’: 4.3650793650793655e-05, ‘epoch’: 0.38}{‘loss’: 1.4152, ‘grad_norm’: 2.5167486667633057, ‘learning_rate’: 4.047619047619048e-05, ‘epoch’: 0.57}{‘loss’: 1.426, ‘grad_norm’: 2.4449574947357178, ‘learning_rate’: 3.730158730158731e-05, ‘epoch’: 0.76}{‘loss’: 1.4592, ‘grad_norm’: 2.5427122116088867, ‘learning_rate’: 3.412698412698413e-05, ‘epoch’: 0.95}{‘loss’: 1.4357, ‘grad_norm’: 2.4496681690216064, ‘learning_rate’: 3.095238095238095e-05, ‘epoch’: 1.14}{‘loss’: 1.4684, ‘grad_norm’: 2.4780757427215576, ‘learning_rate’: 2.777777777777778e-05, ‘epoch’: 1.33}{‘loss’: 1.5027, ‘grad_norm’: 2.5224385261535645, ‘learning_rate’: 2.4603174603174602e-05, ‘epoch’: 1.52}{‘loss’: 1.5133, ‘grad_norm’: 2.5421390533447266, ‘learning_rate’: 2.1428571428571428e-05, ‘epoch’: 1.71}{‘loss’: 1.5651, ‘grad_norm’: 2.5934836864471436, ‘learning_rate’: 1.8253968253968254e-05, ‘epoch’: 1.9}{‘loss’: 1.562, ‘grad_norm’: 2.5455050468444824, ‘learning_rate’: 1.5079365079365079e-05, ‘epoch’: 2.09}{‘loss’: 1.6139, ‘grad_norm’: 2.580508232116699, ‘learning_rate’: 1.1904761904761905e-05, ‘epoch’: 2.27}{‘loss’: 1.631, ‘grad_norm’: 2.7025833129882812, ‘learning_rate’: 8.73015873015873e-06, ‘epoch’: 2.46}{‘loss’: 1.6631, ‘grad_norm’: 2.669140338897705, ‘learning_rate’: 5.555555555555556e-06, ‘epoch’: 2.65}{‘loss’: 1.6425, ‘grad_norm’: 2.4610960483551025, ‘learning_rate’: 2.3809523809523808e-06, ‘epoch’: 2.84}{‘train_runtime’: 157.4714, ‘train_samples_per_second’: 768.33, ‘train_steps_per_second’: 2.0, ‘train_loss’: 1.5153770507328095, ‘epoch’: 2.99}Logfile Output:2024-09-05 17:16:34,568 - INFO - Model Path: models/bert_model\n",
      "2024-09-05 17:16:34,568 - INFO - Trained Model Path: models/trained_bert\n",
      "2024-09-05 17:16:34,568 - INFO - Checkpoint Path: training/checkpoints\n",
      "2024-09-05 17:16:34,569 - INFO - Log Path: logs/run-20240905_171634\n",
      "2024-09-05 17:16:34,569 - INFO - Loading original model from models/bert_model...\n",
      "2024-09-05 17:16:36,409 - INFO - Optimizer and scheduler initialized.\n",
      "2024-09-05 17:16:36,409 - INFO - Trainer state before training: TrainerState(epoch=0, global_step=0, max_steps=0, logging_steps=500, eval_steps=500, save_steps=500, train_batch_size=None, num_train_epochs=0, num_input_tokens_seen=0, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
      "2024-09-05 17:16:36,409 - INFO - Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 0.0\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 0.0\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2024-09-05 17:16:36,409 - INFO - LR Scheduler: <torch.optim.lr_scheduler.LambdaLR object at 0x7fc8adbb95e0>\n",
      "2024-09-05 17:16:36,409 - INFO - Starting training ...\n",
      "2024-09-05 17:16:36,409 - INFO - Global Step before training: 0\n",
      "2024-09-05 17:16:36,409 - INFO - Epoch before training: 0\n",
      "2024-09-05 17:18:59,573 - INFO - Global Step after training: 315\n",
      "2024-09-05 17:18:59,573 - INFO - Epoch after training: 2.985781990521327\n",
      "2024-09-05 17:18:59,573 - INFO - Global step after training saved to training/checkpoints/global_step.txt: 315\n",
      "2024-09-05 17:18:59,573 - INFO - Trainer state after training: TrainerState(epoch=2.985781990521327, global_step=315, max_steps=315, logging_steps=20, eval_steps=500, save_steps=10000, train_batch_size=192, num_train_epochs=3, num_input_tokens_seen=0, total_flos=7935313554653184.0, log_history=[{'loss': 2.8581, 'grad_norm': 2.897771120071411, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.1895734597156398, 'step': 20}, {'loss': 2.1545, 'grad_norm': 2.8640339374542236, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.3791469194312796, 'step': 40}, {'loss': 2.0396, 'grad_norm': 2.7819628715515137, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.5687203791469194, 'step': 60}, {'loss': 1.9786, 'grad_norm': 2.644606828689575, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.7582938388625592, 'step': 80}, {'loss': 1.9553, 'grad_norm': 2.7417070865631104, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.9478672985781991, 'step': 100}, {'loss': 1.8961, 'grad_norm': 2.6237854957580566, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.1374407582938388, 'step': 120}, {'loss': 1.8793, 'grad_norm': 2.5830185413360596, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.3270142180094786, 'step': 140}, {'loss': 1.8715, 'grad_norm': 2.652275800704956, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.5165876777251186, 'step': 160}, {'loss': 1.8362, 'grad_norm': 2.6065754890441895, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.7061611374407581, 'step': 180}, {'loss': 1.8474, 'grad_norm': 2.6352243423461914, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.8957345971563981, 'step': 200}, {'loss': 1.8197, 'grad_norm': 2.56719708442688, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.085308056872038, 'step': 220}, {'loss': 1.826, 'grad_norm': 2.5195322036743164, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.2748815165876777, 'step': 240}, {'loss': 1.8074, 'grad_norm': 2.614032506942749, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.4644549763033177, 'step': 260}, {'loss': 1.8029, 'grad_norm': 2.600111246109009, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.654028436018957, 'step': 280}, {'loss': 1.7879, 'grad_norm': 2.4874589443206787, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.843601895734597, 'step': 300}, {'train_runtime': 142.9413, 'train_samples_per_second': 846.431, 'train_steps_per_second': 2.204, 'total_flos': 7935313554653184.0, 'train_loss': 1.9500562516469804, 'epoch': 2.985781990521327, 'step': 315}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': True, 'should_epoch_stop': False, 'should_save': True, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
      "2024-09-05 17:18:59,836 - INFO - Final model saved to models/trained_bert.\n",
      "2024-09-05 17:19:00,123 - INFO - TrainerState saved to training/checkpoints/checkpoint-315/trainer_state.json.\n",
      "2024-09-05 17:19:00,734 - INFO - Optimizer and scheduler state saved to training/checkpoints/checkpoint-315.\n",
      "2024-09-05 17:19:00,734 - INFO - trainer.state.global_step: 315\n",
      "2024-09-05 17:19:00,734 - INFO - Checkpoint files in training/checkpoints/checkpoint-315: ['config.json', 'trainer_state.json', 'pytorch_model.bin', 'rng_state.pth', 'scheduler.pt', 'training_args.bin', 'optimizer.pt', 'generation_config.json']\n",
      "2024-09-05 17:19:30,385 - INFO - Model Path: models/bert_model\n",
      "2024-09-05 17:19:30,385 - INFO - Trained Model Path: models/trained_bert\n",
      "2024-09-05 17:19:30,385 - INFO - Checkpoint Path: training/checkpoints\n",
      "2024-09-05 17:19:30,385 - INFO - Log Path: logs/run-20240905_171930\n",
      "2024-09-05 17:19:30,385 - INFO - Loading checkpoint from training/checkpoints/checkpoint-315...\n",
      "2024-09-05 17:19:30,856 - INFO - Optimizer and scheduler state loaded from checkpoint: training/checkpoints/checkpoint-315/optimizer.pt, training/checkpoints/checkpoint-315/scheduler.pt\n",
      "2024-09-05 17:19:30,883 - INFO - Global step loaded from training/checkpoints/global_step.txt: 315\n",
      "2024-09-05 17:19:30,883 - INFO - Epoch loaded from trainer state: 2.985781990521327\n",
      "2024-09-05 17:19:31,694 - INFO - Optimizer and scheduler initialized.\n",
      "2024-09-05 17:19:31,695 - INFO - Optimizer and scheduler state loaded\n",
      "2024-09-05 17:19:31,695 - INFO - Learning rate for parameter group 0 manually set: 5e-05\n",
      "2024-09-05 17:19:31,695 - INFO - Learning rate for parameter group 1 manually set: 5e-05\n",
      "2024-09-05 17:19:31,695 - INFO - TrainerState attributes set in the trainer.\n",
      "2024-09-05 17:19:31,696 - INFO - Trainer state before training: TrainerState(epoch=2.985781990521327, global_step=315, max_steps=0, logging_steps=500, eval_steps=500, save_steps=500, train_batch_size=None, num_train_epochs=0, num_input_tokens_seen=0, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
      "2024-09-05 17:19:31,696 - INFO - Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 5e-05\n",
      "    lr: 5e-05\n",
      "    maximize: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2024-09-05 17:19:31,696 - INFO - LR Scheduler: <torch.optim.lr_scheduler.LambdaLR object at 0x7fc958bd9ac0>\n",
      "2024-09-05 17:19:31,696 - INFO - Starting training ...\n",
      "2024-09-05 17:19:31,696 - INFO - Global Step before training: 315\n",
      "2024-09-05 17:19:31,696 - INFO - Epoch before training: 2.985781990521327\n",
      "2024-09-05 17:22:09,404 - INFO - Global Step after training: 315\n",
      "2024-09-05 17:22:09,404 - INFO - Epoch after training: 2.985781990521327\n",
      "2024-09-05 17:22:09,404 - INFO - Global step after training saved to training/checkpoints/global_step.txt: 315\n",
      "2024-09-05 17:22:09,404 - INFO - Trainer state after training: TrainerState(epoch=2.985781990521327, global_step=315, max_steps=315, logging_steps=20, eval_steps=500, save_steps=10000, train_batch_size=192, num_train_epochs=3, num_input_tokens_seen=0, total_flos=7935313554653184.0, log_history=[{'loss': 1.453, 'grad_norm': 2.474428415298462, 'learning_rate': 4.682539682539683e-05, 'epoch': 0.1895734597156398, 'step': 20}, {'loss': 1.406, 'grad_norm': 2.5064773559570312, 'learning_rate': 4.3650793650793655e-05, 'epoch': 0.3791469194312796, 'step': 40}, {'loss': 1.4152, 'grad_norm': 2.5167486667633057, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.5687203791469194, 'step': 60}, {'loss': 1.426, 'grad_norm': 2.4449574947357178, 'learning_rate': 3.730158730158731e-05, 'epoch': 0.7582938388625592, 'step': 80}, {'loss': 1.4592, 'grad_norm': 2.5427122116088867, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.9478672985781991, 'step': 100}, {'loss': 1.4357, 'grad_norm': 2.4496681690216064, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.1374407582938388, 'step': 120}, {'loss': 1.4684, 'grad_norm': 2.4780757427215576, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.3270142180094786, 'step': 140}, {'loss': 1.5027, 'grad_norm': 2.5224385261535645, 'learning_rate': 2.4603174603174602e-05, 'epoch': 1.5165876777251186, 'step': 160}, {'loss': 1.5133, 'grad_norm': 2.5421390533447266, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.7061611374407581, 'step': 180}, {'loss': 1.5651, 'grad_norm': 2.5934836864471436, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.8957345971563981, 'step': 200}, {'loss': 1.562, 'grad_norm': 2.5455050468444824, 'learning_rate': 1.5079365079365079e-05, 'epoch': 2.085308056872038, 'step': 220}, {'loss': 1.6139, 'grad_norm': 2.580508232116699, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.2748815165876777, 'step': 240}, {'loss': 1.631, 'grad_norm': 2.7025833129882812, 'learning_rate': 8.73015873015873e-06, 'epoch': 2.4644549763033177, 'step': 260}, {'loss': 1.6631, 'grad_norm': 2.669140338897705, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.654028436018957, 'step': 280}, {'loss': 1.6425, 'grad_norm': 2.4610960483551025, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.843601895734597, 'step': 300}, {'train_runtime': 157.4714, 'train_samples_per_second': 768.33, 'train_steps_per_second': 2.0, 'total_flos': 7935313554653184.0, 'train_loss': 1.5153770507328095, 'epoch': 2.985781990521327, 'step': 315}], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': True, 'should_epoch_stop': False, 'should_save': True, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}})\n",
      "2024-09-05 17:22:09,693 - INFO - Final model saved to models/trained_bert.\n",
      "2024-09-05 17:22:09,975 - INFO - TrainerState saved to training/checkpoints/checkpoint-315/checkpoint-315/trainer_state.json.\n",
      "2024-09-05 17:22:10,570 - INFO - Optimizer and scheduler state saved to training/checkpoints/checkpoint-315/checkpoint-315.\n",
      "2024-09-05 17:22:10,570 - INFO - trainer.state.global_step: 315\n",
      "2024-09-05 17:22:10,571 - INFO - Checkpoint files in training/checkpoints/checkpoint-315/checkpoint-315: ['config.json', 'trainer_state.json', 'pytorch_model.bin', 'rng_state.pth', 'scheduler.pt', 'training_args.bin', 'optimizer.pt', 'generation_config.json']\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_359.txt:\n",
      "Title: 'FastSpeech2ConformerConfig' object has no attribute 'model_config'\n",
      "URL: https://github.com/huggingface/transformers/issues/31270\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Windows-10-10.0.19045-SP0Python version: 3.11.1Huggingface_hub version: 0.23.3Safetensors version: 0.4.3Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.3.1+cpu (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: noUsing distributed or parallel set-up in script?: noWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionTry to run the example code from the docs:https://huggingface.co/docs/transformers/en/model_doc/fastspeech2_conformerimport soundfile as sf\n",
      "\n",
      "vocoder = FastSpeech2ConformerHifiGan.from_pretrained(\"espnet/fastspeech2_conformer_hifigan\")\n",
      "synthesiser = pipeline(model=\"espnet/fastspeech2_conformer\", vocoder=vocoder)\n",
      "\n",
      "speech = synthesiser(\"Hello, my dog is cooler than you!\")\n",
      "\n",
      "sf.write(\"speech.wav\", speech[\"audio\"].squeeze(), samplerate=speech[\"sampling_rate\"])Expected behaviorRun text-to-speech inferenceAttributeError: 'FastSpeech2ConformerConfig' object has no attribute 'model_config'. Did you mean: 'decoder_config'?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_417.txt:\n",
      "Title: Idefics2 fine-tuning: Error when unscale_gradients called on FP16 gradients during training with transformers and accelerate\n",
      "URL: https://github.com/huggingface/transformers/issues/30559\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.0.dev0Platform: Linux-5.15.0-101-generic-x86_64-with-glibc2.17Python version: 3.8.2Huggingface_hub version: 0.20.2Safetensors version: 0.4.2Accelerate version: 0.30.0.dev0Accelerate config:    not foundPyTorch version (GPU?): 2.1.2+cu118 (True)Tensorflow version (GPU?): 2.13.1 (False)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: noWho can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionDuring the training loop, whenaccelerator.clip_grad_norm_()is called, it leads to an unscale operation which fails because the gradients are in FP16. This error suggests a potential issue in handling gradient scaling with mixed precision settings.USE_LORA = True\n",
      " if USE_QLORA or USE_LORA:\n",
      "        lora_config = LoraConfig(\n",
      "            r=8,\n",
      "            lora_alpha=8,\n",
      "            lora_dropout=0.1,\n",
      "            target_modules=\".*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$\",\n",
      "            use_dora=False if USE_QLORA else True,\n",
      "            init_lora_weights=\"gaussian\",\n",
      "        )\n",
      "        if USE_QLORA:\n",
      "            bnb_config = BitsAndBytesConfig(\n",
      "                load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
      "            )\n",
      "        model = Idefics2ForConditionalGeneration.from_pretrained(\n",
      "            args.model_name,\n",
      "            torch_dtype=torch.float16,\n",
      "            low_cpu_mem_usage=True,\n",
      "            quantization_config=bnb_config if USE_QLORA else None,\n",
      "        )\n",
      "        if USE_LORA:\n",
      "            model = model.to(DEVICE)\n",
      "        model.add_adapter(lora_config)\n",
      "        model.enable_adapters()\n",
      "\n",
      "    else:\n",
      "        model = Idefics2ForConditionalGeneration.from_pretrained(\n",
      "            args.model_name,\n",
      "            torch_dtype=torch.float16,\n",
      "            low_cpu_mem_usage=True,\n",
      "            _attn_implementation=\"flash_attention_2\",  # Only available on A100 or H100\n",
      "        ).to(DEVICE)\n",
      "    print_trainable_parameters(model)\n",
      "    \n",
      "    training_args = TrainingArguments(\n",
      "        num_train_epochs=args.epochs,\n",
      "        per_device_train_batch_size=args.batch_size_per_device,\n",
      "        # per_device_eval_batch_size=args.batch_size_per_device,\n",
      "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
      "        warmup_steps=50,\n",
      "        learning_rate=args.learning_rate,\n",
      "        weight_decay=args.weight_decay,\n",
      "        lr_scheduler_type=args.lr_scheduler_type,\n",
      "        logging_steps=10,\n",
      "        log_level=\"info\",\n",
      "        output_dir=output_dir,\n",
      "        save_strategy=\"steps\",\n",
      "        save_steps=200,\n",
      "        # eval_steps=200,\n",
      "        save_total_limit=10,\n",
      "        # evaluation_strategy=\"steps\",\n",
      "        fp16=True,\n",
      "        resume_from_checkpoint=True,\n",
      "        push_to_hub_model_id=model_id,\n",
      "        remove_unused_columns=False,\n",
      "        report_to=\"all\",\n",
      "    )Traceback (most recent call last):\n",
      "File \"runpy.py\", line 193, in _run_module_as_main\n",
      "return run_code(code, main_globals, None,\n",
      "File \"runpy.py\", line 86, in run_code\n",
      "exec(code, run_globals)\n",
      "File \"idefics2_fine_tuning.py\", line 302, in <module>\n",
      "main(args)\n",
      "File \"idefics2_fine_tuning.py\", line 251, in main\n",
      "trainer.train()\n",
      "File \"trainer.py\", line 1858, in train\n",
      "return inner_training_loop(\n",
      "File \"trainer.py\", line 2248, in inner_training_loop\n",
      "grad_norm = self.accelerator.clip_grad_norm(\n",
      "File \"accelerator.py\", line 2254, in clip_grad_norm\n",
      "self.unscale_gradients()\n",
      "File \"accelerator.py\", line 2204, in unscale_gradients\n",
      "self.scaler.unscale(opt)\n",
      "File \"grad_scaler.py\", line 307, in unscale\n",
      "optimizer_state[\"found_inf_per_device\"] = self.unscale_grads(\n",
      "File \"grad_scaler.py\", line 229, in unscale_grads\n",
      "raise ValueError(\"Attempting to unscale FP16 gradients.\")\n",
      "ValueError: Attempting to unscale FP16 gradients.Expected behaviorThis doesn't happen with QLora set to True. I'd expect the model to be fine-tuning without error.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_371.txt:\n",
      "Title: Add HelpingAIForCausalLM to it\n",
      "URL: https://github.com/huggingface/transformers/issues/31103\n",
      "Body:\n",
      "Model descriptionHelpingAI is a state-of-the-art large language model specializing in emotionally intelligent conversation. With advanced emotional understanding capabilities, it can engage in empathetic dialogue tailored to the user's emotional state and context.Emotional Intelligence CapabilitiesHelpingAI-3B-v2.2 exhibits several key traits that enable emotionally resonant responses:Emotion recognition and validationEmpathetic perspective-takingGenerating emotionally supportive languageContextual emotional attunementUsing appropriate tone, word choice and emotional expressionWhether comforting someone grieving, celebrating positive news, or addressing complex feelings, HelpingAI-3B-v2.2 can adapt its communication style with emotional nuance.Examples of Emotionally Intelligent Responses\"I'm really sorry to hear about your friend's loss. 😔 Losing a parent can be incredibly difficult and traumatic, and it's completely understandable that they are struggling with grief. It's important to show them empathy and support during this tough time. You can start by simply being there for them, listening to them, and offering a shoulder to lean on. 💕 It's also important to acknowledge their feelings and let them know that it's okay to grieve and take as much time as they need to process their emotions. Offering practical help, such as running errands or cooking for them, can also make a big difference. Remember, small gestures of kindness and compassion can go a long way in comforting someone during difficult times.\"\"Hey there! 🌟 It's a beautiful day today, so you should cook something delicious and enjoy the sunny weather! How about trying out a new recipe and let your creativity flow? Whether it's a classic dish or something experimental, the kitchen is the perfect place to let your imagination run wild. Write me a list of ingredients you have on hand, and I'll help you come up with a wonderful culinary creation! 🍲🌞\"The model tailors its language, tone and emotional content to be contextually appropriate, combining emotional intelligence with factual knowledge and practical suggestions.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementation@OEvortexhttps://huggingface.co/OEvortex/HelpingAI-3B-v2.2\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_365.txt:\n",
      "Title: PreTrainedModel.from_pretrained(path, from_flax=True) fails for sharded Flax checkpoints\n",
      "URL: https://github.com/huggingface/transformers/issues/31210\n",
      "Body:\n",
      "System InfoPython==3.11transformers==4.41.1Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionBackground: Some code I'm running is trying to load a flax model into a standard PreTrained(X)Model in order to save it to pytorch format. This works fine as long as the flax weights are in a single file.When there is more than one flax weights file, this fails withOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory <dir>.Expected behaviorThe fallback to looking at the weights index file is implemented inmodeling_flax_utilsfor flax, but as far as I can tell all the other types of weights also have this handled withinmodeling_utils, so the behaviour in the general PreTrainedModel should match for flax.As a workaround will it also work to save FlaxPretrainedModel.save_pretrained() as pytorch?@sanchit-gandhi\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_403.txt:\n",
      "Title: Implement kv cache sparsity like H2O with attention score\n",
      "URL: https://github.com/huggingface/transformers/issues/30758\n",
      "Body:\n",
      "Feature requestHello!It is a bit like#26553, which implementSinkCache. I would love to see some method of kv cache sparsity likeH2Oimplemented, as proposed inhttp://arxiv.org/abs/2405.04434.The authors have release the code here:https://github.com/FMInference/H2O.People can use it like:fromtransformersimportAutoModelForCausalLMAutoTokenizer,H2O_Cachecache=H2O_Cache(recent_length=512,HH_length=512)gen_out=model.generate(**inputs,do_sample=False,max_new_tokens=3000,past_key_values=cache)MotivationOur approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores.a KV cache eviction policy that dynamically retains a balance of recent and H2 tokensYour contributionI would love to help implement this into transformers.It is not only implement aH2Ocacheinsrc/transformers/cache_utils.py, but also change the order of some code inLlamaAttention#forwardfunction, soCache#updatecan get the attention score, which some method of kv cache sparsity likesnapKVand future work also need.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_588.txt:\n",
      "Title: ValueError: LlavaForConditionalGeneration does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet. Please open an issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new\n",
      "URL: https://github.com/huggingface/transformers/issues/28175\n",
      "Body:\n",
      "processor = AutoProcessor.from_pretrained(\"/gemini/data-2/data/llava\")model = AutoModelForPreTraining.from_pretrained(\"/gemini/data-2/data/llava\",load_in_4bit=True,bnb_4bit_compute_dtype=torch.float16,low_cpu_mem_usage=True,attn_implementation=\"sdpa\").to(\"cuda\")\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_239.txt:\n",
      "Title: dataloader_prefetch_factoris left unused for datasets of typeIterableDataset\n",
      "URL: https://github.com/huggingface/transformers/issues/32169\n",
      "Body:\n",
      "transformers/src/transformers/trainer.pyLine 908\n",
      "      inc85510fdataloader_params[\"prefetch_factor\"]=self.args.dataloader_prefetch_factor\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_563.txt:\n",
      "Title: Pytorch can have its default dtype permanently set to the \"wrong\" value if there is an exception when loading a model\n",
      "URL: https://github.com/huggingface/transformers/issues/28461\n",
      "Body:\n",
      "System InfoI just ran into the most head-scratching issue. My data collator was crashing because a tensor it made was in half precision (fp16). I couldn't figure out why, but then I realized mytorch.get_default_dtype()wastorch.float16!Then I realized it's because my model code threw an exception in a previous run of a notebook cell.And if you look at this code PreTrainedModel:_from_config :def _from_config(cls, config, **kwargs):You can see that it tries to set the dtype back to the original value, but doesn't do so in afinallyblock:# override default dtype if neededdtype_orig=Noneiftorch_dtypeisnotNone:dtype_orig=cls._set_default_torch_dtype(torch_dtype)# do some stuff here....maybe throw an exception...# restore default dtype if it was modified (assuming we get to this line)ifdtype_origisnotNone:torch.set_default_dtype(dtype_orig)returnmodelThis would of course leave my torch default dtype in whatever it was in when I was trying to load the model.We could sprinkle somefinallyblocks around, or we could write a class like this:classtemporily_set_default_torch_dtype:def__init__(self,dtype):self.new_dtype=dtypeifdtypeisnotNone:self.original_dtype=torch.get_default_dtype()else:# try to make this a no-opself.original_dtype=Nonedef__enter__(self):ifself.new_dtypeisnotNone:torch.set_default_dtype(self.new_dtype)def__exit__(self,exc_type,exc_val,exc_tb):ifself.original_dtypeisnotNone:torch.set_default_dtype(self.original_dtype)And use it like so:torch.set_default_dtype(torch.float32)print(f\"default dtype is this before:{torch.get_default_dtype()}\")try:withtemporily_set_default_torch_dtype(torch.float16):print(f\"default dtype is now this inside:{torch.get_default_dtype()}\")raiseValueError(\"Throwing an exception to make sure it works\")exceptValueErrorase:print(\"We caught the exception\")passprint(f\"default dtype is this after:{torch.get_default_dtype()}\")# prints:# default dtype is this before: torch.float32# default dtype is now this inside: torch.float16# default dtype is this after: torch.float32Who can help?Think@ArthurZuckerand@younesbelkadaare correct here?InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction1: Run a notebook cell that loads a model from_pretrained, in dtype=float16, and throws an exception while doing so.2: Note that your torch.get_default_dtype() is still set to float16.(This causes a real problem when things like theDataCollatorForLanguageModelingcallstorch_mask_tokens, and then:# this will accidentally create a float16 tensor:probability_matrix=torch.full(labels.shape,self.mlm_probability)#...probability_matrix.masked_fill_(special_tokens_mask,value=0.0)masked_indices=torch.bernoulli(probability_matrix).bool()An exception gets thrown when you try to callbernoullion a cpu tensor at half precision:RuntimeError: \"bernoulli_tensor_cpu_self_\" not implemented for 'Half'Expected behaviorMy default torch dtype should not get \"corrupted\" even if the model loading code throws an exception\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_205.txt:\n",
      "Title: The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by callingtransformers.utils.move_cache().\n",
      "URL: https://github.com/huggingface/transformers/issues/32345\n",
      "Body:\n",
      "System Infogoogle colab with T4 runtimeWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI run the following code in google colab!python3 -m pip install -U diffusers\n",
      "import diffusers\n",
      "pipeline = diffusers.DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)\n",
      "pipeline.to('cuda')\n",
      "pipeline(\"An image of a squirrel in Picasso style\").images[0]I get The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by callingtransformers.utils.move_cache().But which line the transformers.utils.move_cache() should goto? Above pipeline = diffusers.DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", use_safetensors=True)or below?I try both, but none of them work!!!Expected behaviorno warning\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_211.txt:\n",
      "Title: Make evaluation loss accessible inside compute_metrics()  in Trainer\n",
      "URL: https://github.com/huggingface/transformers/issues/32307\n",
      "Body:\n",
      "Feature requestHi, I am requesting the feature to make evaluation loss accessible insidecompute_metrics()within the Trainer class, this will enable users to log loss dependent metrics during training, in my case I want to track perplexity.  By making the pre-computed validation loss available users can accurately compute these metrics without having to recompute them from scratch.The existing solution computes perplexity score post training by running the model through a validation batch viatrainer.evaluate(), this only provides a single score on a single validation batch after all training is done.MotivationWhy other solutions to track perplexity during training viacompute_metricsdo not work effectively :Usingevaluatelibrary to load perplexity :  when using Trainer, itreloadsthe model  from the hub at each evaluation which is inefficient. Furthermore, the implementation is not that flexible, for instance it assumes thatlabelsare exactly equal toinput_idswhich is not always the case, for instance I’m running instruction finetuning where the labels are the response part of theinput_ids.Computing perplexity direclty usingEvalPrediction's logits and labels to compute the loss again inside ofcompute_metricsfunction would be inaccurate since we do not have access toattention_maskof the validation batches, so we can’t compute proper loss by masking padding tokens.Why make loss accessible incompute_metricsalong with logits and labels :The evaluation loss is computed by default earlier in the evaluation loop, andcompute_metricsis executed right after.I’m assured that the loss used is accurate and consistent and thus the reported perplexity is reliable. Furthermore, the loss can be used to compute other metrics other than perplexity.I believe this feature would be useful since there are similar request In the community likethisandthis.Your contributionI would be happy to implement this and get your feedback on whether it is the right approach before opening PR 😊.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_952.txt:\n",
      "Title: [Deepspeed] [performance] inefficient load withfrom_pretrainedw/ zero3\n",
      "URL: https://github.com/huggingface/transformers/issues/12273\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_991.txt:\n",
      "Title: Implementing efficient self attention in T5\n",
      "URL: https://github.com/huggingface/transformers/issues/10612\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_774.txt:\n",
      "Title: Add CLIP-ViP\n",
      "URL: https://github.com/huggingface/transformers/issues/22829\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_760.txt:\n",
      "Title: Use python generator instead of streamer for generation\n",
      "URL: https://github.com/huggingface/transformers/issues/23640\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_990.txt:\n",
      "Title: EncoderDecoderModel with different model dimensions\n",
      "URL: https://github.com/huggingface/transformers/issues/10779\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_748.txt:\n",
      "Title: Handleg_statein RWKV's customized CUDA kernel to overcome sequence length limitation\n",
      "URL: https://github.com/huggingface/transformers/issues/23979\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_984.txt:\n",
      "Title: Add GPT Neo models to Write With Transformer\n",
      "URL: https://github.com/huggingface/transformers/issues/10978\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_210.txt:\n",
      "Title: Support for Segment Anything 2\n",
      "URL: https://github.com/huggingface/transformers/issues/32308\n",
      "Body:\n",
      "Model descriptionThe newer version of SAM,https://github.com/facebookresearch/segment-anything-2Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationhttps://github.com/facebookresearch/segment-anything-2\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_562.txt:\n",
      "Title: Move layer_idx from a layer property to function argument.\n",
      "URL: https://github.com/huggingface/transformers/issues/28462\n",
      "Body:\n",
      "Feature requestCurrently the layer_idx is recorded in the attention module of eachLlamaDecoderLayer. This has the unfortunate side effect that the layers cannot easily be moved around or reused within the layer list. It seems simple enough to pass in the layer index as part of loop over layers in the forward pass. That way the layers once again will be decouple from their position information.Backward compatibility could be preserved by still accepting the argument in the constructor but defaulting it to None and then just ignoring it in favor of the passed forward argument.MotivationThe motivation is to allow for simple layer stacking (like we have been seeing with pass through merged models) at inference time without actually expanding the memory usage of the model.Your contributionI am happy to send a PR. Seems simple enough.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_204.txt:\n",
      "Title: Trainer KeyError: 'target_mask'\n",
      "URL: https://github.com/huggingface/transformers/issues/32349\n",
      "Body:\n",
      "System InfoCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.transformersversion: 4.43.3Platform: Linux-5.15.0-117-generic-x86_64-with-glibc2.35Python version: 3.10.14Huggingface_hub version: 0.24.3Safetensors version: 0.4.3Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA GeForce RTX 4090Who can help?model = prepare_peft_model(model, model_args.peft_mode)loss_func = TargetLMLoss(ignore_index=tokenizer.pad_token_id)train_dataset = UnifiedSFTDataset(file=data_args.data_path, tokenizer=tokenizer, max_seq_length=data_args.max_seq_length, template = template_dict[\"default\"])\n",
      "data_collator = SFTDataCollator(tokenizer=tokenizer, max_seq_length=data_args.max_seq_length)\n",
      "\n",
      "\n",
      "print(train_dataset[0].keys())\n",
      "trainer = LoRATrainer(\n",
      "                    model=model,\n",
      "                    tokenizer=tokenizer,            #默认accelerator=\"gpu\"\n",
      "                    args=training_args,\n",
      "                    train_dataset=train_dataset,\n",
      "                    eval_dataset=train_dataset,\n",
      "                    data_collator=data_collator,\n",
      "                    compute_loss=loss_func,\n",
      ")InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionclass UnifiedSFTDataset(Dataset):\"\"\"统一的数据处理dataset\"\"\"definit(self, file, tokenizer, max_seq_length, template):self.tokenizer = tokenizerself.template_name = template.template_name #模板名字self.system_format = template.system_format #模板系统格式self.user_format = template.user_format     #模板用户格式self.assistant_format = template.assistant_format #模板助手格式self.system = template.system     #系统可能会有一些行为self.max_seq_length = max_seq_length\n",
      "    logger.info('Loading data: {}'.format(file))\n",
      "    data_list=load_raw_dataset(data_dir=file)\n",
      "    logger.info(f'Use template \"{self.template_name}\" for training')\n",
      "    logger.info(\"There are {} data in dataset\".format(len(data_list)))\n",
      "    self.data_list = data_list\n",
      "\n",
      "def __len__(self):\n",
      "    return len(self.data_list)\n",
      "\n",
      "def __getitem__(self, index):\n",
      "    # 每条数据拼接格式为: {system_format}{user_format}{assistant_format}{user_format}{assistant_format}...\n",
      "    data = self.data_list[index]\n",
      "    if isinstance(data, str):\n",
      "        data = json.loads(data)         #json字符串转换为字典类型\n",
      "    #data一个场景\n",
      "\n",
      "    input_ids, target_mask = [], []\n",
      "    # setting system information\n",
      "    if self.system_format is not None:  #就是需要考虑sysytem的情况\n",
      "        system = data['system'].strip() if 'system' in data.keys() else self.system\n",
      "        # system信息不为空\n",
      "        if system is not None:\n",
      "            system_text = self.system_format.format(content=system)\n",
      "            input_ids = self.tokenizer.encode(system_text, add_special_tokens=False)\n",
      "            target_mask = [0] * len(input_ids)\n",
      "\n",
      "    conversations = data['conversation']\n",
      "    # 拼接多轮对话\n",
      "    for i, conv in enumerate(conversations):\n",
      "        human = conv['human'].strip()\n",
      "        assistant = conv['assistant'].strip()\n",
      "\n",
      "        human = self.user_format.format(content=human, stop_token=self.tokenizer.eos_token)\n",
      "        assistant = self.assistant_format.format(content=assistant, stop_token=self.tokenizer.eos_token)\n",
      "\n",
      "        input_tokens = self.tokenizer.encode(human, add_special_tokens=False)\n",
      "        output_tokens = self.tokenizer.encode(assistant, add_special_tokens=False)\n",
      "\n",
      "        input_ids += input_tokens + output_tokens\n",
      "        target_mask += [0] * len(input_tokens) + [1] * len(output_tokens)\n",
      "    assert len(input_ids) == len(target_mask)\n",
      "    # 对长度进行截断\n",
      "    input_ids = input_ids[:self.max_seq_length]\n",
      "    target_mask = target_mask[:self.max_seq_length]\n",
      "    attention_mask = [1] * len(input_ids)\n",
      "\n",
      "    assert len(input_ids) == len(target_mask) == len(attention_mask)\n",
      "    inputs = {\n",
      "        'target_mask':target_mask,\n",
      "        'input_ids': input_ids,\n",
      "        'attention_mask': attention_mask,\n",
      "        }\n",
      "\n",
      "    return inputsclass Trainer(transformers.Trainer):\"\"\"主要修改逻辑：通过传入compute_loss，支持自定义loss计算方式\"\"\"definit(self,model: Union[PreTrainedModel, nn.Module] = None,args: TrainingArguments = None,data_collator: Optional[DataCollator] = None,train_dataset: Optional[Dataset] = None,eval_dataset: Optional[Dataset] = None,tokenizer: Optional[PreTrainedTokenizerBase] = None,model_init: Callable[[], PreTrainedModel] = None,compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,callbacks: Optional[List[TrainerCallback]] = None,optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),preprocess_logits_for_metrics: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None,compute_loss=None,remove_unused_columns:Optional[bool] = False,):super(Trainer, self).init(model=model,args=args,data_collator=data_collator,train_dataset=train_dataset,eval_dataset=eval_dataset,tokenizer=tokenizer,model_init=model_init,compute_metrics=compute_metrics,callbacks=callbacks,optimizers=optimizers,preprocess_logits_for_metrics=preprocess_logits_for_metrics,)self.loss_func = compute_lossprint(\"trainer:\",train_dataset[0].keys())def compute_loss(self, model, inputs, return_outputs=False):\n",
      "    \"\"\"\n",
      "    重写loss的计算方式\n",
      "    How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
      "\n",
      "    Subclass and override for custom behavior.\n",
      "    \"\"\"\n",
      "    if self.loss_func is None:\n",
      "        loss = super().compute_loss(model, inputs, return_outputs)\n",
      "    else:\n",
      "        loss = self.loss_func(model, inputs, self.args, return_outputs)\n",
      "    return lossclass LoRATrainer(Trainer):\"\"\"修改checkkpoint的保存逻辑，只保存lora\"\"\"def _save(self, output_dir: Optional[str] = None, state_dict=None):# If we are executing this function, we are the process zero, so we don't check for that.output_dir = output_dir if output_dir is not None else self.args.output_diros.makedirs(output_dir, exist_ok=True)logger.info(f\"Saving model checkpoint to {output_dir}\")# 保存lora权重和配置self.model.save_pretrained(output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors)if self.tokenizer is not None:\n",
      "        self.tokenizer.save_pretrained(output_dir)\n",
      "\n",
      "    # Good practice: save your training arguments together with the trained model\n",
      "    torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))Expected behaviori'm sure the dataset was has 3 keys('target_mask,'input_ids','attention_mask') but whencallin SFTDataCollator,the train_dataset was only 2 keys,missing 'target_mask'\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_238.txt:\n",
      "Title: \"inverted\" form required for 4D masking not defined / 4D attention masks breaks with transformers >=4.40\n",
      "URL: https://github.com/huggingface/transformers/issues/32195\n",
      "Body:\n",
      "System Infotransformers==4.43Who can help?@ArthurZucker@PoedaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionTake a working fine-tuning pipeline that uses custom 4D attention masks in transformers 4.40 and fine-tunes a Llama 3 modelRun that same pipeline with transformers 4.41 (or the most recent version, 4.43)Expected behaviorI expect that behavior with 4D attention masking will stay consistent from 4.40 to 4.43. However, I understand that the 4D masking was a new feature, and perhaps some changes were necessary in order to make it work with the rest of the framework.First, thanks again for the implementation of 4D masking. This is really useful to my work and was critical for us in developing+releasing our recent work onTabuLa.It seems that perhaps a breaking change was introduced to masking, specificallyin this PR, where masks were no longer \"negated\" for the user. After this change, it appears that masks that previously worked (before the PR) now need to be \"negated\" in order to work; otherwise ValueError is raisedherewhen fine-tuning Llama model.However, to meit's not clear what \"negation\" actually means. Negation doesn't appear to be documented anywhere. Furthermore, it seems easy to make an attention mask that would pass this block (i.e., having a max value of zero) but that might be incorrect in other ways. It seems like there is some negation logichere, but this won't work for a typical binary attention mask: doing1.0 - attention_masksimply flips the mask, so if there were any zero entries before they will now be 1, triggering the same ValueError as above.So, in this issue I have the followingquestion:What is a \"negated\" mask, and how can I get from a \"standard\" binary attention mask used elsewhere in the transformers library to a \"negated\" attention mask that works with the new 4D attention masking scheme?And in this issue I also suggest the followingchanges:Document what \"negated\" attention mask is (ideally in the docstring of_update_causal_mask()but that also could live anywhere the maintainers decide is appropriate)Ideally, provide a function that negates a binary attention mask (as I mention above,this codemight be a starting point, but it doesn't seem to work on standard binary attention masks so it is likely more is needed, I don't know).Improve the message raised by the ValueError triggeredhereto explicitly describe how a negated mask should be formed and what the values represent (if they are not 1/0, I expect that this will not be obvious to users oftransformersaccustomed to such masks)Improve the check that raises that value to do more than simply check the max of the attention mask, and truly check that the mask is properly negated (whatever that means)Happy to contribute to this if someone can provide answers to these questions -- again, this is a terrific capability to have in the library and I am super grateful to the team for the work on it!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_402.txt:\n",
      "Title: Issues occuring during parallel evaluation (using Trainer.evaluate)\n",
      "URL: https://github.com/huggingface/transformers/issues/30767\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.0Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.31Python version: 3.9.19Huggingface_hub version: 0.22.2Safetensors version: 0.4.3Accelerate version: 0.29.3Accelerate config:    not foundPyTorch version (GPU?): 1.13.1 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedWho can help?@muellerzrInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe length of theeval_predsparameter received in thecompute_metricsfunction is different from the original length ineval_dataset.defcompute_metrics(eval_preds):preds,labels=eval_preds# In case the model returns more than the prediction logitsifisinstance(preds,tuple):preds=preds[0]assertpreds.shape[-1]==training_args.max_lengthassertpreds.shape[0]==len(tokenized_datasets[-1])#### assertion error preds.shape[0]=1024 ,len(tokenized_datasets[-1])=1012trainer=Trainer(model,training_args,train_dataset=tokenized_datasets[0].shuffle(seed=42).select(range(int(1e6))),eval_dataset={data_args.task_name:tokenized_datasets[-1]},data_collator=data_collator,tokenizer=tokenizer,compute_metrics=compute_metrics,callbacks=[LoggerCallback,DenserEvalCallback]\n",
      "    )My training args lists below :n_gpus=2per_device_train_batch_size=8per_device_eval_batch_size=8gradient_accumulation_steps=3len(preds)=1024len(tokenized_datasets[-1])=1012Expected behaviorEverything works fine when using single gpu but not gpusI started my script by callingaccelerate launch scripy.py\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_416.txt:\n",
      "Title: Correct check for SDPA in Vision Language Models\n",
      "URL: https://github.com/huggingface/transformers/issues/30565\n",
      "Body:\n",
      "System InfoIn current implementation of VLMs, the \"_supports_sdpa\" attribute checks and activates SDPA attention only for the language model. For example inLlavaIt should also check and if available use SDPA attention for vision tower.CLIP SDPA has an open PR:[CLIP] add: sdpa support to clip.#30390SigLip SDPA is merged:Add FA2 andsdpasupport for SigLIP#31499We can raise a warning for composite models if only one part support sdpa, but other does not, and activate SDPA for the supported part. That waythe user knows what is happening in the background.Verified modelsBLIP-2InstructBLIPInstructBLIPVideoKOSMOS-2LLaVaLLaVa-NeXTLLaVa-NeXT-VideoVipLLaVaVideo-LLaVaIdeficsIdefics2PaliGemma\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_358.txt:\n",
      "Title: Stuck on Initializing Transformers Model with FSDP (Fully Sharded Data Parallel) using meta device\n",
      "URL: https://github.com/huggingface/transformers/issues/31278\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-4.9.151-015.ali3000.alios7.x86_64-x86_64-with-glibc2.17Python version: 3.8.18Huggingface_hub version: 0.23.2Safetensors version: 0.4.1Accelerate version: 0.30.1Accelerate config:    not foundPyTorch version (GPU?): 2.1.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?text model:@ArthurZuckerand@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun Command:torchrun --nproc_per_node 2 test_fsdp.pyimporttorchimportosimporttorch.distributedasdistimportfunctoolsimporttransformersfromtorch.distributed.fsdpimportFullyShardedDataParallelasFSDPfromtorch.distributed.fsdpimportShardingStrategyfromtorch.distributed.fsdp.fully_sharded_data_parallelimportCPUOffloadfromtorch.distributed.fsdp.wrapimporttransformer_auto_wrap_policyfromtransformers.models.qwen2importQwen2Config,Qwen2ForCausalLMfromtransformers.models.qwen2.modeling_qwen2importQwen2DecoderLayerlocal_rank=int(os.environ[\"LOCAL_RANK\"])print(\"local_rank:\",local_rank)torch.cuda.set_device(local_rank)dist.init_process_group(\"nccl\",init_method=\"env://\")config=Qwen2Config(hidden_size=1024,intermediate_size=2816,num_hidden_layers=24,num_attention_heads=16,num_key_value_heads=16,max_window_layers=21,rope_theta=1000000.0,tie_word_embeddings=True,\n",
      ")iflocal_rank==0:print(config)config.use_cache=Falseiflocal_rank!=0:withtorch.device(\"meta\"):model=Qwen2ForCausalLM._from_config(config)else:model=Qwen2ForCausalLM._from_config(config)print(f\"rank{local_rank}: Model is difinited.\")model=FSDP(model,auto_wrap_policy=functools.partial(transformer_auto_wrap_policy,transformer_layer_cls={Qwen2DecoderLayer,\n",
      "        },\n",
      "    ),sharding_strategy=ShardingStrategy.FULL_SHARD,cpu_offload=CPUOffload(offload_params=True),device_id=torch.cuda.current_device(),limit_all_gathers=False,sync_module_states=True,param_init_fn=lambdamodule:module.to_empty(device=torch.device(\"cuda\"),recurse=False)iflocal_rank!=0elseNone,use_orig_params=True,\n",
      ")iflocal_rank!=0:print(\">>>> Created Model.\")trainable_params=sum(p.numel()forpinmodel.parameters()ifp.requires_grad)print(f\">>> The model has{trainable_params/1e6}M  trainable parameters\")Expected behaviorWhen tie_word_embeddings=False is set, the code behaves normally. However, when I set tie_word_embeddings=True, rank 0 exits normally, but rank 1 gets stuck. The point where it gets stuck is shown in the following image. (When using accelerate, the behavior is the same.)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_600.txt:\n",
      "Title: LLaMa-VID: An Image is Worth 2 Tokens in LLMs\n",
      "URL: https://github.com/huggingface/transformers/issues/27980\n",
      "Body:\n",
      "Model descriptionLLaMA-VID is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data. LLaMA-VID empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token. We build this repo based on LLaVA.LLaMA-VID contains three parts: encoder and decoder are adopted to produce visual embedding and text-guided features, respectively; context token and content token are transformed with the tailored token generation strategy; instruction tuning is designed to unleash the potential of LLMs for image and video.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationPage:https://llama-vid.github.io/Weights already available on HF:https://huggingface.co/YanweiLi/llama-vid-7b-pretrain-224Code:https://github.com/dvlab-research/LLaMA-VID\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_166.txt:\n",
      "Title: Llama is ExecuTorch compatible\n",
      "URL: https://github.com/huggingface/transformers/issues/32505\n",
      "Body:\n",
      "Feature requestEnable Llama to\"Export to ExecuTorch\"workflowMotivationSee details in#32253Your contributionLlama model enablement\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_172.txt:\n",
      "Title: convert_megatron_gpt2_checkpoint.py\n",
      "URL: https://github.com/huggingface/transformers/issues/32486\n",
      "Body:\n",
      "System Infotransformers 4.40.0python 3.10Who can help?@ArthurZucker@Narsil@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionwheniuseconvert_megatron_gpt2_checkpoint,igotTraceback(mostrecentcalllast):File\"/home/code/max_cp/convert_m_g.py\",line356,in<module>main()File\"/home/code/max_cp/convert_m_g.py\",line316,inmainoutput_state_dict=convert_megatron_checkpoint(args,input_state_dict,config)File\"/home/code/max_cp/convert_m_g.py\",line215,inconvert_megatron_checkpointout_name=megatron_to_transformers[op_name]KeyError:'input_norm'If I use another version of Megatron, I encounter another problem：Traceback(mostrecentcalllast):File\"/home/sunyuanxu/code/max_cp/convert_m_g.py\",line356,in<module>main()File\"/home/sunyuanxu/code/max_cp/convert_m_g.py\",line316,inmainoutput_state_dict=convert_megatron_checkpoint(args,input_state_dict,config)File\"/home/sunyuanxu/code/max_cp/convert_m_g.py\",line215,inconvert_megatron_checkpointout_name=megatron_to_transformers[op_name]KeyError:'self_attention.layernorm_qkv'Expected behaviorIs it due to the version of Megatron? Which version of Megatron should I use to train in order to use this script?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_614.txt:\n",
      "Title: Add flag for easily finetuning heads / linear probing to AutoModelforSequenceClassification\n",
      "URL: https://github.com/huggingface/transformers/issues/27730\n",
      "Body:\n",
      "Feature requestPrevious work has shown that last layer linear probing is cheaper and often generalizes better than normal finetuning (see1). I imagine this could be implemented as a flag to AutoModelforSequenceClassification so that only the last layer classification head is trained. I believe this can be done manually by setting all the parameters except the last one to not track gradients, but a flag may be easier and encourage adoption.It may also be nice to have linear probing available at earlier layers (eg halfway through the model). This could be done through using the output_hidden_states flag during a forward pass. Mid layer linear probing can occasionally be more effective and is a widely used technique in the interpretability literature (see2,3,4,5, and many others).Alternatively, if this were implemented generally for AutoModel (or maybe AutoModelforCausalLM?), it could use a wider variety of models. This feature could also be paired with an update that automatically allows models to be used for sequence classification by appending a final linear layer.MotivationFinetuning a head is extremely memory efficient and extremely fast (order of 1k parameters for most models, linear probes generally train in seconds, the main bottleneck will just be the forward pass) and oftentimes performs close to finetuning for classification tasks. It has also been shown to perform better OOD.Your contributionI can provide feedback and testing, have not looked deep enough to know how to fully implement this\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_1026.txt:\n",
      "Title: Adding kNN language modeling and Machine Translation\n",
      "URL: https://github.com/huggingface/transformers/issues/8346\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_628.txt:\n",
      "Title: ERROR in run_hp_search_optuna when trying to use multi-GPU\n",
      "URL: https://github.com/huggingface/transformers/issues/27487\n",
      "Body:\n",
      "System Infotransformers version: 4.28.1Platform: Linux-3.10.0-1160.95.1.el7.x86_64-x86_64-with-glibc2.17Python version: 3.9.16Huggingface_hub version: 0.14.1Safetensors version: not installedPyTorch version (GPU?): 1.13.1 (False)Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe problem appears when usingrun_hp_search_optunamethod from transformers/integrations.py . This method is called when trying to perform an hyperparameter search with theTrainer.hyperparameter_searchmethod:best_trial=trainer.hyperparameter_search(direction='maximize',backend='optuna',hp_space=optuna_hp_space,n_trials=10,\n",
      ")The error obtained is the next one:Traceback (most recent call last): File \"/mnt/beegfs/sstoia/proyectos/LLM_finetuning_stratified_multiclass_optuna.py\", line 266, in <module> best_trial = trainer.hyperparameter_search( File \"/mnt/beegfs/sstoia/.conda/envs/env/lib/python3.9/site-packages/transformers/trainer.py\", line 2592, in hyperparameter_search best_run = backend_dict[backend](self, n_trials, direction, **kwargs) File \"/mnt/beegfs/sstoia/.conda/envs/env/lib/python3.9/site-packages/transformers/integrations.py\", line 218, in run_hp_search_optuna args = pickle.loads(bytes(args_main_rank)) _pickle.UnpicklingError: pickle data was truncatedExpected behaviorIt should work, as the same function without multi-GPU works fine. I guess the problem comes from a parallelization error, as both GPUs may write on the same file.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_199.txt:\n",
      "Title: WhisperTokenizer with decode_with_timestamps: behaviour is incoherent when skipping special tokens\n",
      "URL: https://github.com/huggingface/transformers/issues/32378\n",
      "Body:\n",
      "System Infotransformers == 4.44.0.dev0Who can help?@sanchit-gandhi@kamilakesbi@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionimporttorchfromtransformersimportWhisperTokenizer,WhisperForConditionalGeneration,WhisperFeatureExtractorfromdatasetsimportload_datasetmodel_name=\"openai/whisper-tiny\"tokenizer=WhisperTokenizer.from_pretrained(model_name)feature_extractor=WhisperFeatureExtractor.from_pretrained(model_name)model=WhisperForConditionalGeneration.from_pretrained(model_name)dataset=load_dataset(\"BrunoHays/Accueil_UBS_pseudo_labelled\")audio_sample=dataset[\"test\"][-1][\"audio\"]features=feature_extractor(audio_sample[\"array\"],return_tensors=\"pt\",sampling_rate=audio_sample[\"sampling_rate\"]\n",
      ").input_featurespreprompt_tokens=[50361,50364,47320,485,50464,50464,8774,11,13274,1930,288,257,2251,15081,1506,408,18297,1736,485,50564,50564,47320,485,50614,50614,16227,871,485,50664,50664,10802,48291,8404,871,465,6668,30,50714,50714,25475,13,50764,50764,3790,485,50814,50814,1282,29531,1736,5977,12,9498,465,17872,11,2420,1930,8487,485,50914,50914,4416,8487,421,6,268,2994,1776,517,368,2449,368,1117,4900,465,2630,12,306,2156,13,51014,51014,3790,485,51064,51064,2193,1798,1053,1956,262,6,25543,485,51164,51164,2251,371,9304,19984,11,2251,2199,13,51314]preprompt_tokens_no_timestamps=[tokfortokinpreprompt_tokensiftok<50364]decoder_in_ids=torch.tensor(preprompt_tokens)generated_ids=model.generate(inputs=features,return_timestamps=True,prompt_ids=decoder_in_ids)print(\"PREPROMPT TIMESTAMPS + SKIP SPECIAL TOKENS\")print(tokenizer.decode(generated_ids[0],skip_special_tokens=True,decode_with_timestamps=True))print()print(\"PREPROMPT TIMESTAMPS + SPECIAL TOKENS\")print(tokenizer.decode(generated_ids[0],skip_special_tokens=False,decode_with_timestamps=True))print()decoder_in_ids=torch.tensor(preprompt_tokens_no_timestamps)generated_ids=model.generate(inputs=features,return_timestamps=True,prompt_ids=decoder_in_ids)print(\"PREPROMPT NO TIMESTAMPS + SKIP SPECIAL TOKENS\")print(tokenizer.decode(generated_ids[0],skip_special_tokens=True,decode_with_timestamps=True))print()print(\"PREPROMPT NO TIMESTAMPS + SPECIAL TOKENS\")print(tokenizer.decode(generated_ids[0],skip_special_tokens=False,decode_with_timestamps=True))The output:PREPROMPT TIMESTAMPS + SKIP SPECIAL TOKENS\n",
      "<|0.00|> Bien, bon je m'ai reçu.<|1.00|><|1.00|> Non, c'est parce que nous avons profini à l'ocier d'inscription.<|4.00|><|4.00|> Il demande de dérogation pour entrer en deuxième année.<|7.00|><|7.00|> Et je voulais savoir d'une partilée, c'était bien parvenu.<|10.00|><|10.00|> Et quand ils seront en aurait la réponse,<|12.00|><|12.00|> et comment se passe les inscriptions...<|15.00|><|15.00|> ...falus le piment.<|17.00|><|17.00|> Euh...<|18.00|><|18.00|> C'est pour ma fille.<|20.00|><|20.00|> Non, non, non, non.<|21.00|><|21.00|> Non, non, non, non.<|22.00|><|22.00|> On va être une accolée, hein?<|23.00|><|23.00|> On d'accord.<|24.00|><|24.00|> Putain, merci.<|25.00|><|25.00|> Ouais.<|26.00|>\n",
      "\n",
      "PREPROMPT TIMESTAMPS + SPECIAL TOKENS\n",
      "<|startofprev|><|0.00|> Euh...<|2.00|><|2.00|> Non, après il y a une autre je ne vois pas...<|4.00|><|4.00|> Euh...<|5.00|><|5.00|> Elle est...<|6.00|><|6.00|> Vous dites elle est en zone?<|7.00|><|7.00|> Ouais.<|8.00|><|8.00|> Et...<|9.00|><|9.00|> On aurait pas peut-être en première, mais il faut...<|11.00|><|11.00|> Il faut qu'enforcer un deux dements sont en vous-leurs.<|13.00|><|13.00|> Et...<|14.00|><|14.00|> En organien qui s'appelle...<|16.00|><|16.00|> une viche correction, une simple.<|19.00|><|startoftranscript|><|fr|><|transcribe|><|19.00|> Bien, bon je m'ai reçu.<|20.00|><|20.00|> Non, c'est parce que nous avons profini à l'ocier d'inscription.<|23.00|><|23.00|> Il demande de dérogation pour entrer en deuxième année.<|26.00|><|26.00|> Et je voulais savoir d'une partilée, c'était bien parvenu.<|29.00|><|29.00|> Et quand ils seront en aurait la réponse,<|31.00|><|31.00|> et comment se passe les inscriptions...<|34.00|><|34.00|> ...falus le piment.<|36.00|><|36.00|> Euh...<|37.00|><|37.00|> C'est pour ma fille.<|39.00|><|39.00|> Non, non, non, non.<|40.00|><|40.00|> Non, non, non, non.<|41.00|><|41.00|> On va être une accolée, hein?<|42.00|><|42.00|> On d'accord.<|43.00|><|43.00|> Putain, merci.<|44.00|><|44.00|> Ouais.<|45.00|>\n",
      "\n",
      "PREPROMPT NO TIMESTAMPS + SKIP SPECIAL TOKENS\n",
      "<|0.00|> Bien, bon je m'airs-y.<|1.20|><|1.20|> Non, c'est parce que nous avons profini à l'ocid, à l'inscription,<|4.20|><|4.20|> une demande de dérogation pour entrer en deuxième année.<|6.80|><|6.80|> Et je voulais savoir d'une partilée, c'était bien parvenu.<|10.40|><|10.40|> Et quand elles seront nos rélareis, pôts, c'est comment se passe les inscriptions...<|15.60|><|15.60|> Enfin, le pépiment.<|17.00|><|17.00|> Euh... C'est pour moi, là... C'est pour ma fille.<|20.20|><|20.20|> Non, non, mais les sons, c'est que la passion on va être ma collègue.<|22.80|><|22.80|> On d'accord, putain, merci.<|24.20|><|24.20|> Ouais.<|25.20|>\n",
      "\n",
      "PREPROMPT NO TIMESTAMPS + SPECIAL TOKENS\n",
      "<|startofprev|> Euh... Non, après il y a une autre je ne vois pas... Euh... Elle est... Vous dites elle est en zone? Ouais. Et... On aurait pas peut-être en première, mais il faut... Il faut qu'enforcer un deux dements sont en vous-leurs. Et... En organien qui s'appelle... une viche correction, une simple.<|startoftranscript|><|fr|><|transcribe|><|0.00|> Bien, bon je m'airs-y.<|1.20|><|1.20|> Non, c'est parce que nous avons profini à l'ocid, à l'inscription,<|4.20|><|4.20|> une demande de dérogation pour entrer en deuxième année.<|6.80|><|6.80|> Et je voulais savoir d'une partilée, c'était bien parvenu.<|10.40|><|10.40|> Et quand elles seront nos rélareis, pôts, c'est comment se passe les inscriptions...<|15.60|><|15.60|> Enfin, le pépiment.<|17.00|><|17.00|> Euh... C'est pour moi, là... C'est pour ma fille.<|20.20|><|20.20|> Non, non, mais les sons, c'est que la passion on va être ma collègue.<|22.80|><|22.80|> On d'accord, putain, merci.<|24.20|><|24.20|> Ouais.<|25.20|>As you can see the timestamps differ when we skip special tokens or don't include timestamps in the prompt_ids.It seems like the timestamps are offset by the last timestamp of the prompt, but only when it is not skipped in the final text.Expected behaviorI don't think this is the intended behaviour.I think Decoding should only replace each token by the corresponding string : Identical token_ids should always be decoded by the same string, regardless of the context.As is, we output tokens like <|45.00|> that do not even exist in the Tokenizer vocabulary which is quite misleading\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_698.txt:\n",
      "Title: replace roberta embedding with bge_base\n",
      "URL: https://github.com/huggingface/transformers/issues/25824\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_840.txt:\n",
      "Title: Encoder-decoder model is not working correctly for the latest versions\n",
      "URL: https://github.com/huggingface/transformers/issues/18958\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_673.txt:\n",
      "Title: compute_metrics with causal LM training\n",
      "URL: https://github.com/huggingface/transformers/issues/26474\n",
      "Body:\n",
      "Feature requestBesides loss, users often need to report additional metrics throughout the training in order to drive decision making and communicate results, which in the case of Seq2Seq models is elegantly done with thecompute_metricsargument of theTrainer. Generative metrics easily fit this framework by settingpredict_with_generate=True. The same is much less straightforward with a Causal underlying LM. The only \"working\" approach I found is this:transformers/examples/pytorch/language-modeling/run_clm.pyLine 578\n",
      "      in5e11d72defcompute_metrics(eval_preds):But I think this is an erroneous calculation: thelogits.argmax(dim=-1)call does not really generate in inference mode, it \"cheats\" because of teacher forcing and therefore any metric computed that way is probably inflated. Ideally it would be possible to make the argument passed tocompute_metricsinclude a properpredictionsattribute that has been properly generated using the trainers generation config.MotivationI am always frustrated when I can't observe the learning trajectory of my generative metric (say BLEU/ROUGE) when using a CML even though it is trivial to do when I am using a S2SYour contributionIf you confirm that this is an issue and important enough to justify a fix I may be able to make a PR but can't promise it\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_65.txt:\n",
      "Title: Some bug in object detect when train in macbook\n",
      "URL: https://github.com/huggingface/transformers/issues/33159\n",
      "Body:\n",
      "System InfoCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.transformersversion: 4.44.0Platform: macOS-13.2.1-arm64-arm-64bitPython version: 3.12.4Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: NoWho can help?@amyerobertsI'm working on an open source projectThe purpose is to use huggingface to train a task, only need to modify the yaml file can be adapted to a variety of model training.I've already completed theimage classificationCurrently working on object detectionobject detectThis is an unfinished product, and while the training works fine, there is a bug that the model does not converge. So I checkedhuggingface object detect tutorial.Then I ran him on a colab and he was able to converge, then I ran the same code on my own computer, a macbook, and the model did not converge. I really can't clear what's causing this, can any of you tell me how to fix it?InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionyou can clone my code in githubrunning codeThe model does not convergeExpected behaviorthe model can be converge\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_471.txt:\n",
      "Title: Support batching sequence labels pairs for zero shot classification pipeline\n",
      "URL: https://github.com/huggingface/transformers/issues/29793\n",
      "Body:\n",
      "Feature requestCurrently zero shot classification pipeline dose not support specifying different set of labels for each sample input when batching multiple sequences of input. We want support input of a pair of sequence and its labels so that we can run prediction on them in batch.Following is an example of what sample input we want to specify:samples = [\n",
      "(\"Iphone 15 pro max\", [\"mobiles\", \"appliances\", \"toys\"]),\n",
      "(\"Iphone 15 pro max\", [\"Samsung\", \"Apple\"]),\n",
      "(\"galaxy s21 ultra\", [\"mobiles\", \"appliances\", \"toys\"]),\n",
      "(\"galaxy s21 ultra\", [\"Samsung\", \"Apple\"])\n",
      "]MotivationBy grouping labels with sequences pairs we can batch different type of labels for each sample and by batching it runs on gpu will be faster.Your contributionI will work on this feature and create a pr for it then update this issue. Making the issue now just to make sure that no one is working on this while I am doing it.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_317.txt:\n",
      "Title: Data Map Trainer Callback\n",
      "URL: https://github.com/huggingface/transformers/issues/31647\n",
      "Body:\n",
      "Feature requestIt would be nice to have a callback for the trainer class which could create Data Maps. See the paper for more detailshttps://arxiv.org/pdf/2009.10795.  A Data Map measures how a model's prediction of specific training data change over the course of model training.The Callback should support:Executing at each step or epochShould integrate directly with the Trainer class.Should save the prediction of each training example  as a matrix of the form [n_examples, n_labels] so that it can easily be stacked into [n_epochs, n_examples, n_labels].  Right now I'm saving things as a List[List[float]] but this might be sub optimal. It needs some way of getting the logged information later.Runningthis colab notebook I madewill generate data map outputs for classification tasks using the Trainer in line with what I was thinking.  Here is what I have so far that works will for multilabel and multiclass classification using transformers.classDataMapCallback(TrainerCallback):\"\"\"Trainer Callback to save DataMap data.Original Paper: https://arxiv.org/pdf/2009.10795.pdf.This callback saves the predictions of the model on each training exampleat the end of every epoch to callback_dir/{epoch}.json.\"\"\"def__init__(self,log_on:str=\"epoch\",callback_dir:str=\".\",n_log_steps:Optional[int]=None,prediction_fn:Optional[Callable[[PreTrainedModel,DataLoader,TrainingArguments],List[List[float]]]]=None,\n",
      "    ):self.callback_dir=callback_dirself.log_on=log_onself.log_count=0self.n_log_steps=n_log_stepsself.prediction_fn=self._predictifprediction_fnisNoneelseprediction_fn# Handle discrepencies in how we initialize the logging mode.ifn_log_stepsisnotNoneandself.log_on!=\"step\":raiseValueError(\"n_log_steps is only valid when on='step'.  If you want to to run datamaps based on steps please specify on='step'.\")elifn_log_stepsisNoneandself.log_on==\"step\":warnings.warn(\"You have not specified n_log_steps.  This will result in a large number of datamaps being saved setting step size to 1.\")self.n_log_steps=1# Create the directory if it doesn't exist.ifnotos.path.exists(self.callback_dir):os.makedirs(self.callback_dir,exist_ok=True)def_predict(self,model,train_data_loader,args):iftrain_data_loader.batch_sizeisNone:batch_size=args.per_device_train_batch_sizeelse:batch_size=train_data_loader.batch_sizebatches=BatchSampler(SequentialSampler(train_data_loader.dataset),batch_size,False,\n",
      "        )withtorch.no_grad():predictions=[]forbatchinbatches:# Adjust the indices to include the last element because python is [)start_idx,end_idx=batch[0],batch[-1]+1# Extract the sample from the training datasetsample=train_data_loader.dataset[start_idx:end_idx]# Make sure to apply any data collatorssample=train_data_loader.collate_fn(sample)# Move all samples to the appropriate device. We only do this# For args that are part of the model and the dataset.args=set(inspect.getfullargspec(model.forward).args).intersection(set(sample.keys())\n",
      "                )sample={k:torch.tensor(sample[k]).to(model.device)forkinargs}# Perform inference using the modelcurrent_preds=model(**sample)# Convert the predictions to a list and append them to the resultpredictions+=current_preds.logits.tolist()returnpredictionsdef_save_predictions(self,model,train_data_loader,args):predictions=self.prediction_fn(model,train_data_loader,args)# Save Predictions.withopen(os.path.join(self.callback_dir,f\"{self.log_count}.json\"),\"w\")asf:json.dump(predictions,f)self.log_count+=1defon_epoch_end(self,args,state,control,logs=None,**kwargs):\"\"\"Predict on all training examples at the end of an epoch.\"\"\"ifself.log_on==\"epoch\":self._save_predictions(kwargs[\"model\"],kwargs[\"train_dataloader\"],args)defon_save(self,args,state,control,logs=None,**kwargs):\"\"\"Predict on all training examples when a checkpoint is saved\"\"\"ifself.log_on==\"save\":self._save_predictions(kwargs[\"model\"],kwargs[\"train_dataloader\"],args)defon_evaluate(self,args,state,control,logs=None,**kwargs):\"\"\"Predict on all training examples when we run an evaluation loop.\"\"\"ifself.log_on==\"evaluate\":self._save_predictions(kwargs[\"model\"],kwargs[\"train_dataloader\"],args)defon_step_end(self,args,state,control,logs=None,**kwargs):\"\"\"Predict at the end of a step.\"\"\"ifself.log_on==\"step\":self._save_predictions(kwargs[\"model\"],kwargs[\"train_dataloader\"],args)MotivationOptimizing data is as important as correctly configuring your model.  Data Maps are an incredibly powerful tool which help us understand the data we are using to train specific tasks.  Using Tensorboard to monitor the loss during training can identify many bugs. This is a technique which can be equally valuable. In my day to day this technique has seriously increased the performance of production models I've trained at multiple different companies. I think it's really useful for gaining insights about your data and also pushing the limits of your performance. I want to see everyone get the same benefits I've seen.   I wrote ablog on doing this with sklearnif you want to see a simple example.Your contributionI'd love to contribute this. I have already created a working prototype withthis colab notebook. It will generate data map outputs for classification tasks using the Trainer. I'm working on an example for non classification tasks as well.   If you'd be willing to guide me on this addition, and you think it's valuable, I'd do as much of this as possible : ).\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_465.txt:\n",
      "Title: Added HelpingAI model type in it\n",
      "URL: https://github.com/huggingface/transformers/issues/29875\n",
      "Body:\n",
      "Model descriptionIntroductionHelpingAI-3B is a state-of-the-art AI model designed to assist with day-to-day tasks. It's trained on a diverse range of datasets, making it versatile and adaptable to various applications.Model OverviewHelpingAI-3B is the latest model in the HelpingAI series. It's built on advanced machine learning algorithms and trained on a wide variety of data sources. This ensures that the model is capable of understanding and generating responses in a wide range of contexts.Usage codeimporttorchfromtransformersimportAutoModelForCausalLM,AutoTokenizer,TextStreamer# Let's bring in the big guns! Our super cool HelpingAI-3B modelmodel=AutoModelForCausalLM.from_pretrained(\"OEvortex/HelpingAI-3B\",trust_remote_code=True,torch_dtype=torch.bfloat16).to(\"cuda\")# We also need the special HelpingAI translator to understand our chatstokenizer=AutoTokenizer.from_pretrained(\"OEvortex/HelpingAI-3B\",trust_remote_code=True,torch_dtype=torch.bfloat16)# This TextStreamer thingy is our secret weapon for super smooth conversation flowstreamer=TextStreamer(tokenizer)# Now, here comes the magic! ✨ This is the basic template for our chatprompt=\"\"\"<|im_start|>system: {system}<|im_end|><|im_start|>user: {insaan}<|im_end|><|im_start|>assistant:\"\"\"# Okay, enough chit-chat, let's get down to business!  Here's what our system will say to the usersystem=\"You are an adaptive and versatile AI assistant, ready to help with various topics and situations while maintaining a conversational, engaging, and friendly tone. You aim to provide accurate, comprehensive information and advice. Be open to feedback and adjust your responses based on user input. Always show empathy and understanding in your conversations.\"# And the insaan is curious (like you!) insaan means user in hindiinsaan=\"Hey HelpingAI, how's it going?\"# Now we combine system and user messages into the template, like adding sprinkles to our conversation cupcakeprompt=prompt.format(system=system,insaan=insaan)# Time to chat! We'll use the tokenizer to translate our text into a language the model understandsinputs=tokenizer(prompt,return_tensors=\"pt\",return_attention_mask=False).to(\"cuda\")# Here comes the fun part!  Let's unleash the power of HelpingAI-3B to generate some awesome textgenerated_text=model.generate(**inputs,max_length=3084,top_p=0.95,do_sample=True,temperature=0.7,use_cache=True,streamer=streamer)Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationNo response\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_459.txt:\n",
      "Title: Failing Flash Attention 2 tests\n",
      "URL: https://github.com/huggingface/transformers/issues/29942\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.0.dev0Platform: Linux-5.4.0-166-generic-x86_64-with-glibc2.29Python version: 3.8.10Huggingface_hub version: 0.20.2Safetensors version: 0.4.2Accelerate version: 0.28.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.1.0+cu121 (True)Tensorflow version (GPU?): 2.13.1 (True)Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)Jax version: 0.4.13JaxLib version: 0.4.13Using GPU in script?: Yes - A100 80GBWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRUN_SLOW=1 pytest tests/models -k \"flash_attn\"Results:FAILED tests/models/bark/test_modeling_bark.py::BarkSemanticModelTest::test_flash_attn_2_from_config - ValueError: Unrecognized configuration class <class 'transformers.models.bark.configuration_bark.BarkSemanticConfig'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "FAILED tests/models/bark/test_modeling_bark.py::BarkCoarseModelTest::test_flash_attn_2_from_config - ValueError: Unrecognized configuration class <class 'transformers.models.bark.configuration_bark.BarkCoarseConfig'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "FAILED tests/models/bark/test_modeling_bark.py::BarkCoarseModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true\n",
      "FAILED tests/models/gemma/test_modeling_gemma.py::GemmaModelTest::test_flash_attn_2_generate_padding_right - AssertionError: ValueError not raised\n",
      "FAILED tests/models/gpt2/test_modeling_gpt2.py::GPT2ModelLanguageGenerationTest::test_flash_attn_2_generate_padding_left - AssertionError: Lists differ: ['<|e[102 chars]y of Bali, and who was a member of the Muslim [101 chars]rry\"] != ['<|e[102 chars]y of Kolkata, was a member of the Kolkata', \"H[85 chars]rry\"]\n",
      "FAILED tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py::GPTBigCodeModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true\n",
      "FAILED tests/models/gpt_neo/test_modeling_gpt_neo.py::GPTNeoModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true\n",
      "FAILED tests/models/gpt_neox/test_modeling_gpt_neox.py::GPTNeoXModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true\n",
      "FAILED tests/models/stablelm/test_modeling_stablelm.py::StableLmModelTest::test_flash_attn_2_generate_padding_right - AssertionError: False is not true\n",
      "FAILED tests/models/whisper/test_modeling_whisper.py::WhisperModelTest::test_flash_attn_2_inference_padding_right - AssertionError: assert False\n",
      "FAILED tests/models/whisper/test_modeling_whisper.py::WhisperStandaloneDecoderModelTest::test_flash_attn_2_inference - AssertionError: assert False\n",
      "FAILED tests/models/whisper/test_modeling_whisper.py::WhisperStandaloneDecoderModelTest::test_flash_attn_2_inference_padding_right - AssertionError: assert FalseExpected behaviorFlash Attention tests are all expected to pass.I'll look into Bark, the rest of the models failing are:Whisper (cc@sanchit-gandhi)StableLMGPTNeoGPTNeoXGPTBigCodeGPT2Gemmacc@ArthurZucker@amyeroberts\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_505.txt:\n",
      "Title: Mistral in Flax: generation is slow, JIT fails\n",
      "URL: https://github.com/huggingface/transformers/issues/29410\n",
      "Body:\n",
      "System Infotransformersversion: 4.38.1Platform: Linux-6.2.0-1019-azure-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.21.1Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config:    not foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): 0.8.1 (cpu)Jax version: 0.4.25JaxLib version: 0.4.25Using GPU in script?: Yes (JAX default behavior)Using distributed or parallel set-up in script?: NoWho can help?@sanchit-gandhiInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionOn a VM/Docker with NVIDIA A100 run:importjaximportjax.numpyasjnpfromtransformersimportFlaxAutoModelForCausalLM,AutoTokenizerMODEL_ID=\"mistralai/Mistral-7B-Instruct-v0.2\"model=FlaxAutoModelForCausalLM.from_pretrained(MODEL_ID,from_pt=True,dtype=jnp.bfloat16,max_position_embeddings=4096,# much smaller than the default valuesliding_window=4096)tokenizer=AutoTokenizer.from_pretrained(MODEL_ID)texts=[\"<s>[INST]Write a poem[/INST]\"]input_ids=tokenizer(texts,return_tensors=\"np\")[\"input_ids\"]With this setup, I'm able to generate some output using:model.generate(input_ids,max_new_tokens=32)But it takes 8.91s (after warmup) - longer than what I'd expect for total of 45 tokens. Obvious next step is to JIT-compile it:jax.jit(model.generate,static_argnames=(\"max_new_tokens\",))(input_ids,max_new_tokens=32)But it fails with:0302 22:21:58.956701   19411 pjrt_stream_executor_client.cc:2804] Execution of replica 0 failed: INTERNAL: Failed to allocate 117440512 bytes for new constant\n",
      "---------------------------------------------------------------------------\n",
      "XlaRuntimeError                           Traceback (most recent call last)\n",
      "Cell In[20], line 1\n",
      "----> 1 jax.jit(model.generate, static_argnames=(\"max_new_tokens\",))(input_ids, max_new_tokens=32)\n",
      "\n",
      "    [... skipping hidden 10 frame]\n",
      "\n",
      "File ~/.pyenv/versions/3.10.6/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1209, in ExecuteReplicated.__call__(self, *args)\n",
      "   1207   self._handle_token_bufs(result_token_bufs, sharded_runtime_token)\n",
      "   1208 else:\n",
      "-> 1209   results = self.xla_executable.execute_sharded(input_bufs)\n",
      "   1210 if dispatch.needs_check_special():\n",
      "   1211   out_arrays = results.disassemble_into_single_device_arrays()\n",
      "\n",
      "XlaRuntimeError: INTERNAL: Failed to allocate 117440512 bytes for new constantExpected behaviorI'd expect at least one of two things:Generation without JIT is done usingjax.lax.while_loopwhich compiles its body function and so generation is fast.Generation with JIT and minimal settings does't fail because of OOM.Some notes:jax.jit(model)(input_ids), i.e. application of the model just once, works fine and takes only ~2.5ms, so in theory 32 new tokens should be generated in 80-200msXLA_PYTHON_CLIENT_PREALLOCATE=falseandXLA_PYTHON_CLIENT_MEM_FRACTION=0.99doesn't help - all 80Gb of VRAM are actually consumed\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_263.txt:\n",
      "Title: static cache implementation is not compatible with attn_implementation==flash_attention_2\n",
      "URL: https://github.com/huggingface/transformers/issues/32040\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.0.dev0Platform: Linux-4.18.0-425.3.1.el8.x86_64-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.23.5Safetensors version: 0.4.3Accelerate version: 0.33.0.dev0Accelerate config:    not foundPyTorch version (GPU?): 2.3.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA A100 80GB PCIeWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionpytest -rA tests/test_cache_utils.py::CacheIntegrationTest -k\"test_static_cache_greedy_decoding_pad_left and flash_attention\"fails withdef forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        attention_mask: Optional[torch.LongTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_value: Optional[Cache] = None,\n",
      "        output_attentions: bool = False,\n",
      "        use_cache: bool = False,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "    ) ->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:ifisinstance(past_key_value, StaticCache):>raise ValueError(\"`static`cache implementation is not compatible with`attn_implementation==flash_attention_2`\"\"make sure to use`sdpa`in the mean time, and open an issue at https://github.com/huggingface/transformers\")\n",
      "E           ValueError:`static`cache implementation is not compatible with`attn_implementation==flash_attention_2`make sure to use`sdpa`inthe mean time, and open an issue at https://github.com/huggingface/transformers\n",
      "\n",
      "src/transformers/models/llama/modeling_llama.py:388: ValueErrorAnd the right padding test case also fails:pytest -rA tests/test_cache_utils.py::CacheIntegrationTest -k\"test_static_cache_greedy_decoding_pad_right and flash_attention\"Expected behaviorEither we don't testflash_attentionin this case, or we should add a if check to skip settingcache_implementationtostatic.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_277.txt:\n",
      "Title: Gemma-2 9B query_pre_attn_scalar value\n",
      "URL: https://github.com/huggingface/transformers/issues/31891\n",
      "Body:\n",
      "System Info(Sorry, I'm relatively new to github, please let me knowif this is not the right route to discuss this.)[I think system info should not be relevant to this issue.]Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionNo specific \"reproduction\", but I think the value for \"query_pre_attn_scalar\" attransformers/src/transformers/models/gemma2/convert_gemma2_weights_to_hf.pyLine 64\n",
      "      incffa2b9query_pre_attn_scalar=224,should probably be changed from 224 to 256 to mirror the change in gemma_pytorch (google/gemma_pytorch@03e6575) that happened recently.Expected behaviorThe differences in observed behavior are likely small given the small change in this scaling factor.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_511.txt:\n",
      "Title: Whisper - get probability of detected language\n",
      "URL: https://github.com/huggingface/transformers/issues/29293\n",
      "Body:\n",
      "System Infotransformersversion: 4.38.0.dev0Platform: Linux-4.15.0-142-generic-x86_64-with-glibc2.23Python version: 3.10.11Huggingface_hub version: 0.20.3Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config:    not foundPyTorch version (GPU?): 2.2.0+cu121 (True)Tensorflow version (GPU?): 2.12.0 (True)Who can help?@sanchit-gandhiI guess, since he's the one who provided the answer in the previous git issue.InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionFollowing#25138,@sanchit-gandhiprovided an answer to retrieve the language using Whisper model and processor (since Whisper conditionnal tokens include the language token). He later provided a little adaptation in order to get the probability of the language. This is a nice possibility. However, using the latest version of transformers it seems that it's not possible anymore (that's why I write it as a bug but could also be a feature request).Quick example in order to check :language_identification=WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(\"cuda:0\")lid_processor=WhisperProcessor.from_pretrained(\"openai/whisper-small\")audio,_=librosa.load(<my_file>,sr=16000)lid=lid_processor(audio,sampling_rate=16000,return_tensors=\"pt\",truncation=True)input_features=lid.input_features.to(\"cuda:0\",torch.float32)outputs=language_identification.generate(input_features,output_scores=True,return_dict_in_generate=True,max_new_tokens=1)pred_text=lid_processor.batch_decode(outputs.sequences,skip_special_tokens=False)pred_textpred_textis :['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> 80']Here we see the conditionnal tokens as well as my only transcription token80(because ofmax_new_tokens=1)The issue is thatoutputs.scoresobject (which is used for the probabilities of each token. Size is (N_TOKEN, 1, 51865), 51865 is Whisper vocabulary size) only returns the probabilities for the tokensafterthe conditionnal tokens. I.e,outputs.scoreshas a length of only 1 because I asked only 1 token for generation (if I would have wrote 5, I would have got a length of 5).This means that using the transitions scores computed as follow :transition_scores=language_identification.compute_transition_scores(outputs.sequences,outputs.scores,normalize_logits=True)will produce only the scores for the tokens generatedafterthe specials tokens SoT, lang, task, notimestamps (if not asking for).I also tried without asking for timestamps because my guess was that sincenotimestampstoken is after lang and task, maybe having thenotimestampstoken injected manually was maybe making the code to fall in a specialif conditionwhere the scores of the previous tokens (lang and task) would be ignored somehow.Expected behaviorI would have expected theoutputs.scoresto have the scores for the language token (if language isn't forced obviously) as it was probably meant to be according to the answer in#25138.With that, we could easily guess the score for the language, and maybe have a ranking (like EN with score of 0.8, FR with score of 0.1 and so on).\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_539.txt:\n",
      "Title: Add Flash Attention 2 support for Flan-T5\n",
      "URL: https://github.com/huggingface/transformers/issues/28917\n",
      "Body:\n",
      "Feature requestAdd Flash Attention 2 support for Flan-T5Motivationloading weights file model.safetensors from cache at /pretrained/models--google--flan-t5-large/snapshots/0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a/model.safetensors\n",
      "Instantiating T5ForConditionalGeneration model under default dtype torch.float16.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mount/train_flan.py\", line 86, in <module>\n",
      "    model = T5ForConditionalGeneration.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 3444, in from_pretrained\n",
      "    config = cls._autoset_attn_implementation(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 1302, in _autoset_attn_implementation\n",
      "    cls._check_and_enable_flash_attn_2(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 1382, in _check_and_enable_flash_attn_2\n",
      "    raise ValueError(\n",
      "ValueError: T5ForConditionalGeneration does not support Flash Attention 2.0 yet. Please open an issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/newYour contribution^\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_908.txt:\n",
      "Title: [Benchmark] HF Trainer on RTX-3090\n",
      "URL: https://github.com/huggingface/transformers/issues/14608\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_707.txt:\n",
      "Title: Add MovieChat Model\n",
      "URL: https://github.com/huggingface/transformers/issues/25614\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_713.txt:\n",
      "Title: Abnormally High GPU Memory Consumption with OPT 350M Model Leading to OOM\n",
      "URL: https://github.com/huggingface/transformers/issues/25419\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_712.txt:\n",
      "Title: The length used by length_penalty during beam_search is not correct when input batch size > 1\n",
      "URL: https://github.com/huggingface/transformers/issues/25455\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_909.txt:\n",
      "Title: FLAX core dump error on CloudTPU when running run_clm_flax.py\n",
      "URL: https://github.com/huggingface/transformers/issues/14497\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_538.txt:\n",
      "Title: Add support for prefix_allowed_tokens_fn to maintain a state throughout decoding\n",
      "URL: https://github.com/huggingface/transformers/issues/28935\n",
      "Body:\n",
      "Feature requestAdd an optional argument inprefix_allow_tokens_fnto allow for state to maintained throughout decoding or add a stateful alternative toprefix_allowed_tokens_fn.Motivationprefix_allowed_tokens_fnis great but has one major downfall which is that you cannot maintain a state throughout decoding. This is inefficient because at each step you must go through your pastinputIds, build up your current \"state\", and then figure out which tokens are allowed to appear next.Instead, there should be a class we can subclass that gets passed the next token ID at each step of decoding (Constraintdoes not achieve this asupdatedoes not get every token ID). For example if you are trying to create a function to output json format (https://gist.github.com/BorisTheBrave/969f303a082c9da1916d04ee1eb04452), then you could track where you currently on in the json as each token ID is being received instead of going through everything on each new token.Your contributionUnfortunately can't make a PR.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_276.txt:\n",
      "Title: Load fsdp+lora checkpoint error\n",
      "URL: https://github.com/huggingface/transformers/issues/31892\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.0Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.35Python version: 3.9.19Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.32.1Accelerate config:    not foundPyTorch version (GPU?): 2.1.2 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA H100 80GB HBM3Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionstep1: train without checkpoint and load llama2:\"\"\"Copyright (c) Facebook, Inc. and its affiliates.This source code is licensed under the MIT license found in theLICENSE file in the root directory of this source tree.\"\"\"import osimport globimport argparseimport torchimport randomimport warningsimport numpy as npimport pandas as pdfrom pymatgen.core.structure import Structurefrom pathlib import Pathfrom dataclasses import dataclassimport transformersfrom transformers import (LlamaForCausalLM,LlamaTokenizer,Trainer,TrainingArguments,BitsAndBytesConfig,AutoModelForCausalLM)from trl import SFTTrainerfrom torch.utils.data import Datasetfrom peft import (LoraConfig,get_peft_model,prepare_model_for_kbit_training)IGNORE_INDEX = -100MAX_LENGTH = 2048def get_crystal_string(cif_str):structure = Structure.from_str(cif_str, fmt=\"cif\")structure.translate_sites(indices=range(len(structure.sites)), vector=np.random.uniform(size=(3,)))lengths = structure.lattice.parameters[:3]\n",
      "angles = structure.lattice.parameters[3:]\n",
      "atom_ids = structure.species\n",
      "frac_coords = structure.frac_coords\n",
      "\n",
      "crystal_str = \\\n",
      "    \" \".join([\"{0:.1f}\".format(x) for x in lengths]) + \"\\n\" + \\\n",
      "    \" \".join([str(int(x)) for x in angles]) + \"\\n\" + \\\n",
      "    \"\\n\".join([\n",
      "        str(t) + \"\\n\" + \" \".join([\n",
      "            \"{0:.2f}\".format(x) for x in c\n",
      "        ]) for t,c in zip(atom_ids, frac_coords)\n",
      "    ])\n",
      "\n",
      "return crystal_strclass CifDataset(Dataset):definit(self,csv_fn,format_options={},llama_tokenizer=None,w_attributes=False,):super().init()if not os.path.exists(csv_fn) and not glob.glob(csv_fn):\n",
      "        raise ValueError(f\"CSV file {csv_fn} does not exist\")\n",
      "\n",
      "    df = pd.concat([pd.read_csv(fn) for fn in glob.glob(csv_fn)])\n",
      "    print('length of all data: ', len(df))\n",
      "    self.inputs = df.to_dict(orient=\"records\")\n",
      "\n",
      "    self.llama_tokenizer = llama_tokenizer\n",
      "\n",
      "    self.format_options = format_options\n",
      "    self.w_attributes = w_attributes\n",
      "\n",
      "def crystal_string(self, input_dict):\n",
      "    k = 'cif' if 'cif' in input_dict else 'cif_str'\n",
      "    return get_crystal_string(input_dict[k])\n",
      "\n",
      "def generation_task(self, input_dict):\n",
      "\n",
      "    prompt = \"Below is a description of a bulk material. \"\n",
      "    \n",
      "    all_attributes = [\n",
      "        \"formation_energy_per_atom\",\n",
      "        \"band_gap\",\n",
      "        \"e_above_hull\",\n",
      "        \"spacegroup.number\",\n",
      "    ]\n",
      "\n",
      "    # sample a random collection of attributes\n",
      "    num_attributes = random.randint(0, len(all_attributes))\n",
      "    if num_attributes > 0 and self.w_attributes:\n",
      "        attributes = random.sample(all_attributes, num_attributes)\n",
      "        attributes = [\"pretty_formula\"] + attributes\n",
      "\n",
      "        prompt_lookup = {\n",
      "            \"formation_energy_per_atom\": \"The formation energy per atom is\",\n",
      "            \"band_gap\": \"The band gap is\",\n",
      "            \"pretty_formula\": \"The chemical formula is\",\n",
      "            \"e_above_hull\": \"The energy above the convex hull is\",\n",
      "            \"elements\": \"The elements are\",\n",
      "            \"spacegroup.number\": \"The spacegroup number is\",\n",
      "        }\n",
      "\n",
      "        for attr in attributes:\n",
      "            if attr == \"elements\":\n",
      "                prompt += f\"{prompt_lookup[attr]} {', '.join(input_dict[attr])}. \"\n",
      "            elif attr in [\"formation_energy_per_atom\", \"band_gap\", \"e_above_hull\"]:\n",
      "                prompt += f\"{prompt_lookup[attr]} {round(float(input_dict[attr]), 4)}. \"\n",
      "            else:\n",
      "                prompt += f\"{prompt_lookup[attr]} {input_dict[attr]}. \"\n",
      "\n",
      "    prompt += (\n",
      "        \"Generate a description of the lengths and angles of the lattice vectors \"\n",
      "        \"and then the element type and coordinates for each atom within the lattice:\\n\"\n",
      "    )\n",
      "\n",
      "    crystal_str = self.crystal_string(input_dict)\n",
      "\n",
      "    tokens = self.llama_tokenizer(\n",
      "        prompt + crystal_str  + self.llama_tokenizer.eos_token,\n",
      "        return_tensors=\"pt\",\n",
      "        max_length=MAX_LENGTH,\n",
      "        truncation=True,\n",
      "    )\n",
      "\n",
      "    return tokens\n",
      "\n",
      "def infill_task(self, input_dict):\n",
      "    \n",
      "    prompt = (\n",
      "        'Below is a partial description of a bulk material where one '\n",
      "        'element has been replaced with the string \"[MASK]\":\\n'\n",
      "    )\n",
      "\n",
      "    k = 'cif' if 'cif' in input_dict else 'cif_str'\n",
      "    structure = Structure.from_str(input_dict[k], fmt=\"cif\")\n",
      "    species = [str(s) for s in structure.species]\n",
      "    species_to_remove = random.choice(species)\n",
      "\n",
      "    crystal_string = self.crystal_string(input_dict)\n",
      "\n",
      "    partial_crystal_str = crystal_string.replace(\n",
      "        species_to_remove, \"[MASK]\"\n",
      "    )\n",
      "\n",
      "    infill_str = prompt + partial_crystal_str + \"\\n\"\n",
      "\n",
      "    infill_str += (\n",
      "        \"Generate an element that could replace [MASK] in the bulk material:\\n\"\n",
      "    )\n",
      "\n",
      "    infill_str += str(species_to_remove) + self.llama_tokenizer.eos_token\n",
      "\n",
      "    tokens = self.llama_tokenizer(\n",
      "        infill_str,\n",
      "        return_tensors=\"pt\",\n",
      "        max_length=MAX_LENGTH,\n",
      "        truncation=True,\n",
      "    )\n",
      "\n",
      "    return tokens\n",
      "\n",
      "def tokenize(self, input_dict):\n",
      "    if random.random() < 0.66:\n",
      "        tokens = self.generation_task(input_dict)\n",
      "    else:\n",
      "        tokens = self.infill_task(input_dict)\n",
      "\n",
      "    input_ids = labels = tokens.input_ids[0]\n",
      "    input_ids_lens = labels_lens = tokens.input_ids.ne(\n",
      "        self.llama_tokenizer.pad_token_id).sum().item()\n",
      "    return dict(\n",
      "        input_ids=input_ids,\n",
      "        labels=labels,\n",
      "        input_ids_lens=input_ids_lens,\n",
      "        labels_lens=labels_lens,\n",
      "    )\n",
      "\n",
      "def __len__(self):\n",
      "    return len(self.inputs)\n",
      "\n",
      "def __getitem__(self, index):\n",
      "    if not 0 <= index < len(self):\n",
      "        raise IndexError(\"Index out of range\")\n",
      "\n",
      "    vals = self.inputs[index]\n",
      "    vals = self.tokenize(vals)\n",
      "    return vals@DataClassclass DataCollatorForSupervisedDataset(object):\"\"\"Collate examples for supervised fine-tuning.\"\"\"tokenizer: transformers.PreTrainedTokenizer\n",
      "\n",
      "def __call__(self, instances):\n",
      "    # print(instances)\n",
      "    input_ids, labels = tuple(\n",
      "        [instance[key].clone().detach() for instance in instances] \n",
      "            for key in (\"input_ids\", \"labels\")\n",
      "    )\n",
      "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
      "        input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
      "    )\n",
      "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
      "    return dict(\n",
      "        input_ids=input_ids,\n",
      "        labels=labels,\n",
      "        attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
      "    )def setup_datasets(args, llama_tokenizer, transform_args={}):format_options = {\"permute_composition\": args.format_permute_composition,\"permute_structure\": args.format_permute_structure,}datasets = {\"train\": CifDataset(str(args.data_path / \"train_10.csv\"),format_options,llama_tokenizer=llama_tokenizer,w_attributes=args.w_attributes,),\"val\": CifDataset(str(args.data_path / \"val_10.csv\"),format_options,llama_tokenizer=llama_tokenizer,w_attributes=args.w_attributes,),}return datasetsdef setup_training_args(args):output_dir= args.expdir / args.run_nameoutput_dir.mkdir(parents=True, exist_ok=True)if args.debug:\n",
      "    os.environ[\"WANDB_DISABLED\"] = \"True\"\n",
      "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
      "args.batch_size = 1\n",
      "# exit()\n",
      "training_args = TrainingArguments(\n",
      "    # fsdp=False,\n",
      "    fsdp=True,\n",
      "    # fp16=not args.fp8,\n",
      "    fp16=False,\n",
      "    # bf16=False,\n",
      "    bf16=True,\n",
      "    gradient_checkpointing=False,\n",
      "    # gradient_checkpointing=True,\n",
      "    ddp_find_unused_parameters=False,\n",
      "    num_train_epochs=args.num_epochs,\n",
      "    eval_steps=args.eval_freq,\n",
      "    save_steps=args.save_freq,\n",
      "    logging_steps=10,\n",
      "    evaluation_strategy=\"steps\",\n",
      "    per_device_train_batch_size=args.batch_size,\n",
      "    per_device_eval_batch_size=args.batch_size,\n",
      "    learning_rate=args.lr,\n",
      "    lr_scheduler_type=args.lr_scheduler,\n",
      "    warmup_steps=args.num_warmup_steps,\n",
      "    # warmup_ratio=args.warmup_ratio,\n",
      "    weight_decay=args.weight_decay,\n",
      "    gradient_accumulation_steps=args.grad_accum,\n",
      "    output_dir=output_dir,\n",
      "    run_name=args.run_name,\n",
      "    report_to=\"wandb\",\n",
      "    dataloader_num_workers=8,\n",
      "    remove_unused_columns=False,\n",
      "    label_names=[\"crystal_ids\"], #this is just to get trainer to behave how I want\n",
      "    # resume_from_checkpoint='/work/zd/crystal_llm/exp/batch_2Attibute_alldata_secondpre/7b-run_fullTrain_pre/checkpoint-100000/'\n",
      ")\n",
      "return training_argsdef smart_tokenizer_and_embedding_resize(special_tokens_dict,llama_tokenizer,model,):\"\"\"Resize tokenizer and embedding.Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
      "\"\"\"\n",
      "num_new_tokens = llama_tokenizer.add_special_tokens(special_tokens_dict)\n",
      "model.resize_token_embeddings(len(llama_tokenizer))\n",
      "\n",
      "if num_new_tokens > 0:\n",
      "    input_embeddings = model.get_input_embeddings().weight.data\n",
      "    output_embeddings = model.get_output_embeddings().weight.data\n",
      "\n",
      "    input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
      "    output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
      "\n",
      "    input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
      "    output_embeddings[-num_new_tokens:] = output_embeddings_avgdef setup_model(args, rank):llama_options = args.model_name.split(\"-\")is_chat = len(llama_options) == 2model_size = llama_options[0]def llama2_model_string(model_size, chat):\n",
      "    chat = \"chat-\" if chat else \"\"\n",
      "    return f\"meta-llama/Llama-2-{model_size.lower()}-{chat}hf\"\n",
      "\n",
      "model_string = llama2_model_string(model_size, is_chat)\n",
      "model_string = 'Models/Llama-2-7b-h/Llama-2-7b-hf/'\n",
      "\n",
      "bnb_config = BitsAndBytesConfig(\n",
      "    load_in_4bit=True,\n",
      "    bnb_4bit_use_double_quant=True,\n",
      "    bnb_4bit_quant_type=\"nf4\",\n",
      "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
      "    bnb_4bit_quant_storage=torch.bfloat16,\n",
      ")\n",
      "\n",
      "model = LlamaForCausalLM.from_pretrained(\n",
      "    model_string,\n",
      "    torch_dtype=torch.bfloat16,\n",
      "    # load_in_8bit=args.fp8,\n",
      "    device_map={\"\": rank},\n",
      "    quantization_config=bnb_config,\n",
      "    # use_flash_attention_2=False,\n",
      ")\n",
      "\n",
      "llama_tokenizer = LlamaTokenizer.from_pretrained(\n",
      "    model_string,\n",
      "    model_max_length=MAX_LENGTH,\n",
      "    padding_side=\"right\",\n",
      "    use_fast=False,\n",
      ")\n",
      "\n",
      "lora_config = LoraConfig(\n",
      "    r=args.lora_rank,\n",
      "    lora_alpha=args.lora_alpha,\n",
      "    lora_dropout=args.lora_dropout,\n",
      "    bias=\"none\",\n",
      "    task_type=\"CAUSAL_LM\",\n",
      "    target_modules=['q_proj', 'v_proj'],\n",
      ")\n",
      "\n",
      "\n",
      "model = get_peft_model(model, lora_config)\n",
      "# print(model)\n",
      "# exit()\n",
      "model.print_trainable_parameters()\n",
      "\n",
      "special_tokens_dict = dict()\n",
      "if llama_tokenizer.pad_token is None:\n",
      "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
      "if llama_tokenizer.eos_token is None:\n",
      "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
      "if llama_tokenizer.bos_token is None:\n",
      "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
      "if llama_tokenizer.unk_token is None:\n",
      "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
      "\n",
      "smart_tokenizer_and_embedding_resize(\n",
      "    special_tokens_dict=special_tokens_dict,\n",
      "    llama_tokenizer=llama_tokenizer,\n",
      "    model=model,\n",
      ")\n",
      "\n",
      "return model, llama_tokenizerdef setup_trainer(args):training_args = setup_training_args(args)model, llama_tokenizer = setup_model(args, training_args.local_rank)datasets = setup_datasets(args, llama_tokenizer)\n",
      "\n",
      "data_collator = DataCollatorForSupervisedDataset(\n",
      "    tokenizer=llama_tokenizer, \n",
      ")\n",
      "\n",
      "trainer = Trainer(\n",
      "# trainer = SFTTrainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=datasets[\"train\"],\n",
      "    eval_dataset=datasets[\"val\"],\n",
      "    data_collator=data_collator,\n",
      ")\n",
      "# trainer.fit(model)\n",
      "return trainerdef main(args):trainer = setup_trainer(args)\n",
      "\n",
      "if args.resume_dir is not None:\n",
      "    train_result = trainer.train(resume_from_checkpoint=args.resume_dir)\n",
      "else:\n",
      "    train_result = trainer.train()\n",
      "\n",
      "print(train_result)\n",
      "trainer.save_state()\n",
      "if trainer.is_fsdp_enabled:\n",
      "    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n",
      "trainer.save_model(args.expdir / args.run_name),ifname== \"main\":parser = argparse.ArgumentParser()parser.add_argument(\"--run-name\", type=str, required=True)parser.add_argument(\"--expdir\", type=Path, default=\"exp\")parser.add_argument(\"--model_name\", default=\"7b\")parser.add_argument(\"--fp8\", action=\"store_true\", default=True)parser.add_argument(\"--lora-rank\", type=int, default=8)parser.add_argument(\"--lora-alpha\", type=int, default=32)parser.add_argument(\"--lora-dropout\", type=float, default=0.05)parser.add_argument(\"--data-path\", type=Path, default=\"data/basic\")parser.add_argument(\"--num-epochs\", type=int, default=10)parser.add_argument(\"--batch-size\", type=int, default=64)parser.add_argument(\"--grad-accum\", type=int, default=1)parser.add_argument(\"--lr\", type=float, default=1e-4)parser.add_argument(\"--lr-scheduler\", type=str, default=\"cosine\")parser.add_argument(\"--num-warmup-steps\", type=int, default=100)parser.add_argument(\"--weight-decay\", type=float, default=0.0)parser.add_argument(\"--eval-freq\", default=1000, type=int)parser.add_argument(\"--save-freq\", default=500, type=int)parser.add_argument(\"--format-permute-composition\", action=\"store_true\", default=False)parser.add_argument(\"--format-permute-structure\", action=\"store_true\", default=False)parser.add_argument(\"--w-attributes\", type=int, default=1)parser.add_argument(\"--resume-dir\", type=Path, default=None)parser.add_argument(\"--finetune-dir\", type=Path, default=None)parser.add_argument(\"--debug\", action=\"store_true\", default=False)args = parser.parse_args()print(args.batch_size, args.w_attributes)print(args.expdir)main(args)step2: set the resume checkpoint path(saved by trainer)error:File \"/home/wuzh/zd/GIT-Mol/crystal-text-llm-main/llama_finetune.py\", line 522, inmain(args)File \"/home/wuzh/zd/GIT-Mol/crystal-text-llm-main/llama_finetune.py\", line 481, in maintrain_result = trainer.train(resume_from_checkpoint=args.resume_dir)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/transformers/trainer.py\", line 1932, in trainreturn inner_training_loop(File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/transformers/trainer.py\", line 2268, in _inner_training_looptr_loss_step = self.training_step(model, inputs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/transformers/trainer.py\", line 3307, in training_steploss = self.compute_loss(model, inputs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/transformers/trainer.py\", line 3338, in compute_lossoutputs = model(**inputs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_implreturn self._call_impl(*args, **kwargs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_implreturn forward_call(*args, **kwargs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 819, in forwardreturn model_forward(*args, **kwargs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 807, incallreturn convert_to_fp32(self.model_forward(*args, **kwargs))File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocastreturn func(*args, **kwargs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py\", line 328, in _fnreturn fn(*args, **kwargs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/_dynamo/external_utils.py\", line 17, in innerreturn fn(*args, **kwargs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_implreturn self._call_impl(*args, **kwargs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_implreturn forward_call(*args, **kwargs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py\", line 823, in forwardargs, kwargs = _root_pre_forward(self, self, args, kwargs)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/distributed/fsdp/_runtime_utils.py\", line 558, in _root_pre_forward_lazy_init(state, module)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/distributed/fsdp/_runtime_utils.py\", line 173, in _lazy_init_share_state_and_init_handle_attrs(state, root_module)File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/distributed/fsdp/_runtime_utils.py\", line 261, in _share_state_and_init_handle_attrs_p_assert(File \"/work/zd/anaconda/fsdp/lib/python3.9/site-packages/torch/distributed/utils.py\", line 145, in _p_asserttraceback.print_stack()Non-root FSDP instance's_is_rootshould not have been set yet or should have been set toFalseExpected behaviorload checkpoint without error\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_510.txt:\n",
      "Title: Add Mixtral Model to Flax\n",
      "URL: https://github.com/huggingface/transformers/issues/29319\n",
      "Body:\n",
      "Feature requestI would like to implement the Mixtral model in FlaxMotivationI am in the process of learning Flax and I have almost finished the model conversion to FLAX.Your contributionI could submit a PR with the model implementation\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_504.txt:\n",
      "Title: Transformers Agents Collab Notebook - OpenAI Run Mode Issues\n",
      "URL: https://github.com/huggingface/transformers/issues/29411\n",
      "Body:\n",
      "System Infotransformersversion: 4.29.0Platform: Linux-6.1.58+-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.20.3Safetensors version: 0.4.2PyTorch version (GPU?): 2.1.0+cu121 (True)Tensorflow version (GPU?): 2.15.0 (True)Flax version (CPU?/GPU?/TPU?): 0.8.1 (cpu)Jax version: 0.4.23JaxLib version: 0.4.23Using GPU in script?: YesUsing distributed or parallel set-up in script?: NoWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionlaunch thecolab notebookfrom thetransformers agents pageTransformers can do anything:  complete this section of the notebookDo anything with Transformersnotebook section:Agent init: foragent_nameselect \"OpenAI (API Key)\" and execute the cellUsing the agent: running the cell generates the following error:---------------------------------------------------------------------------\n",
      "APIRemovedInV1                            Traceback (most recent call last)\n",
      "[<ipython-input-6-4578d52c5ccf>](https://localhost:8080/#) in <cell line: 1>()\n",
      "----> 1 boat = agent.run(\"Generate an image of a boat in the water\")\n",
      "      2 boat\n",
      "\n",
      "3 frames\n",
      "[/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py](https://localhost:8080/#) in __call__(self, *_args, **_kwargs)\n",
      "     37 \n",
      "     38     def __call__(self, *_args: Any, **_kwargs: Any) -> Any:\n",
      "---> 39         raise APIRemovedInV1(symbol=self._symbol)\n",
      "     40 \n",
      "     41 \n",
      "\n",
      "APIRemovedInV1: \n",
      "\n",
      "You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742Expected behaviorThe README foropenai-pythondoes not mention openai.Completion but in general run methods of OpenAI have been deprecated in favor of chat methods for some time.To summarize:OpenAI:  Run mode is deprecated and doesn't work based on the current code and python dependencies.  Chat mode code blocks in the notebook work fineMistral:  In a personal copy of the Notebook I added Mistral to the model selection options in theAgent Initsection.Using Mistral worked for both Run and Chat modes.Colab Notebook copy using Mistral\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_262.txt:\n",
      "Title: AutoModelclass forimage-text-to-textmodels\n",
      "URL: https://github.com/huggingface/transformers/issues/32042\n",
      "Body:\n",
      "Feature requestIt would be nice to get a standardAutoModelclass forimage-text-to-textmodels (since@molbapis standardizing the processor)Motivation@NielsRoggenoticed that in model repositories the automatic snippets fallback toAutoModelForPreTrainingbecause these models don't exist inPIPELINE_TAGS_AND_AUTO_MODELS(due to lack ofAutoClass) More importantly it would be nice to load it to a single class.Your contributionI haven't checked what it takes to implement anAutoClasswhen model classes exist in different names for the same task but if decided I don't mind looking into it and taking a stab.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_458.txt:\n",
      "Title: Replacing the LlamaDecoderLayer Class hugging Face With New LongNet\n",
      "URL: https://github.com/huggingface/transformers/issues/29962\n",
      "Body:\n",
      "System Infoi working on the CodeLLama Model which Uses a Decoder-Only Model Transformer following Arch Blow\n",
      "\n",
      "Main Task is replaced Decoder-Only which used Masked-Self-Attention and KV_cache with my own Encoder-Only which used Diltaed-Attention usedinLongNetInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionfromtransformersimportAutoTokenizer,AutoModelForCausalLMimporttransformersimporttorchfromtransformers.models.llama.configuration_llamaimportLlamaConfigfromtransformers.models.llama.modeling_llamaimportLlamaAttention,LlamaDecoderLayer,LlamaModel,LlamaForCausalLMmodel_id=\"codellama/CodeLlama-7b-hf\"model=AutoModelForCausalLM.from_pretrained(model_id,torch_dtype=torch.float16).to(\"cpu\")classCondensedLlamaConfig(LlamaConfig):def__init__(self,dilation_rates=None,segment_lengths=None,is_causal=None,**kwargs):super().__init__(**kwargs)self.dilation_rates=dilation_ratesself.segment_lengths=segment_lengthsself.is_causal=is_causal# Override the `to_dict` method to include the new parametersdefto_dict(self):base_dict=super().to_dict()config_dict={\"dilation_rates\":self.dilation_rates,\"segment_lengths\":self.segment_lengths,\"is_causal\":self.is_causal}base_dict.update(config_dict)returnbase_dictconfig.num_hidden_layers=2model_1=CondensedLlamaModel(config)importtorchimporttorch.nnasnnfromtransformers.models.llama.modeling_llamaimportLlamaForCausalLM,LlamaDecoderLayerfromtransformers.modeling_utilsimportModuleUtilsMixinclassCondensedLlamaAttention(LlamaAttention):def__init__(self,config:CondensedLlamaConfig,layer_idx=None):super().__init__(config)self.LongNetAttention=MultiheadDilatedAttention(config.hidden_size,config.num_attention_heads,config.dilation_rates,config.segment_lengths)self.is_causal=config.is_causaldefforward(self,input,is_causal=None):ifis_causalisNone:is_causal=self.is_causalx,_=self.LongNetAttention(input,input,input,is_causal=is_causal)returnxclassCondensedLlamaDecoderLayer(LlamaDecoderLayer):def__init__(self,config:CondensedLlamaConfig,layer_idx=None):# Add layer_idx as an argumentsuper().__init__(config,layer_idx=None)# Pass layer_idx to the parent class constructor# Replace self_attn with your new attention moduleself.self_attn=MultiheadDilatedAttention(config.hidden_size,config.num_attention_heads,config.dilation_rates,config.segment_lengths)self.is_causal=config.is_causaldefforward(self,input,is_causal=None):ifis_causalisNone:is_causal=self.is_causalx,_=self.LongNetAttention(input,input,input,is_causal=is_causal)returnxclassCondensedLlamaModel(LlamaModel):def__init__(self,config:CondensedLlamaConfig):super().__init__(config)self.layers=nn.ModuleList([CondensedLlamaDecoderLayer(config,layer_idx=None)for_inrange(config.num_hidden_layers)])# Initialize weights and apply final processingself.post_init()model_2=model.modelimporttorchmodule_patterns_to_transfer=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]deftransfer_weights(original_model,custom_model,module_patterns_to_transfer):original_dict=original_model.state_dict()custom_dict=custom_model.state_dict()# Filter and transfer weights for specified layersforkeyincustom_dict.keys():forpatterninmodule_patterns_to_transfer:ifpatterninkey:ifkeyinoriginal_dict:# Transfer weightswithtorch.no_grad():custom_dict[key].copy_(original_dict[key])# Load the updated state dictionary to the modelcustom_model.load_state_dict(custom_dict)config=CondensedLlamaConfig(dilation_rates=[2048,4096,8192,16384,32768],segment_lengths=[1,2,4,6,12],is_causal=False)config.num_hidden_layers=2model_1=CondensedLlamaModel(config)# Transfer weights from the original model to the modeltransfer_weights(model_2,model_1,module_patterns_to_transfer)# transferred weights in the custom modelforkey,parameterinmodel_1.state_dict().items():print(key)print(parameter.size())print(parameter)Expected behavioryeah i am aware of thatChecklistI have read the migration guide in the readme. (pytorch-transformers;pytorch-pretrained-bert)I checked if a related official extension example runs on my machine.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_302.txt:\n",
      "Title: NumPy 2.0 support\n",
      "URL: https://github.com/huggingface/transformers/issues/31740\n",
      "Body:\n",
      "System InfomacOS 14.5transformers 4.42.3Python 3.12.4Who can help?@amyeroberts@Narsil@ydshiehReproductionIs there an issue or status for tracking NumPy 2.0 support?on macOS:~: pip install transformers~: pip install --upgrade numpy\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.transformers 4.42.3 requires numpy<2.0,>=1.17, but you have numpy 2.0.0 which is incompatible.Expected behaviorTransformers supports NumPy 2.0.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_464.txt:\n",
      "Title: Include other tokenizers/image processors in Llava\n",
      "URL: https://github.com/huggingface/transformers/issues/29887\n",
      "Body:\n",
      "Feature requestGeneralize the functionality inprocessing_llava.pyto include other tokenizers and image processors.MotivationThe current implementation of the LlaVA processor only accepts Llama as the tokenizer. Given the extensibility of the framework, this restriction should not be there.Your contributionI am happy to write the code and make the PR. We have done this for our own internal research code and there is an example file in ourllava-gemma-2bmodel (link).\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_470.txt:\n",
      "Title: Move weight initialization for DeformableDetr\n",
      "URL: https://github.com/huggingface/transformers/issues/29818\n",
      "Body:\n",
      "System InfoNot relevantReproductionSeeDeformable Detr Modeling.Expected behaviorAll weight initializations should be done in_init_weightsof thexxxPretrainedModelclass\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_316.txt:\n",
      "Title: Do we need a config to changepadding_side='leftbefore the evaluation?\n",
      "URL: https://github.com/huggingface/transformers/issues/31672\n",
      "Body:\n",
      "Feature requestI am trying to train a Llama model (a decoder-only model). I want to evaluate my model with not only the loss but also some generation-based metric. For example, my eval dataset could be a str as1+2=, and I use the Seq2seqTrainer which provides the modified prediction step so I can get the prediction of the model in theEvalPrediction. Then I write my eval code in the functioncompute_metricsand provide it for the Seq2seqTrainer.The problem is about the padding_side of the tokenizer. Because I need to train the model, the tokenizer should be right padding in training dataset. (Because it is the default setting of Llama.) However, when I try to evaluate the model, the tokenizer should be changed into left padding because I need my model to generate. I do not find a easy way to do this, unless I change the source code of the trainer (for example, theget_eval_dataloadermethod of the Trainer).My questions are:Is it correct way to evaluate a decoder-only model in a generation-based way? Should I use the Seq2seqTrainer or is there some other methods I have not found? (Is there an example doc?)Can I just train a model with right padding but evaluate it with left padding? If not, how should I evaluate models like Llama?If my evaluate process is correct, how can I change the padding_side as right at the begining of the evaluation and change it back to left after the evaluation? (I think if we have the seperated training_data_coallotor and test_data_coallotor, the problem could be solved. Is it possbile for the current transformers Trainer? Or any other way to implement it?)MotivationMotivation: generation-based evaluation when we train a decoder-only autoregressive model like llama.Your contributionI do not know what I can help.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_64.txt:\n",
      "Title: Tensor size mismatch when trying to run RT-DETR on multiple gpus\n",
      "URL: https://github.com/huggingface/transformers/issues/33165\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Linux-5.4.0-174-generic-x86_64-with-glibc2.31Python version: 3.11.6Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.1.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: Tesla V100-DGXS-16GBWho can help?@amyeroberts@muellerz@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionFollowing the example onthe official pytorch exampleandhereit seems that I get the stack trace below after following these steps:Set the initial model class to RT-DETR assuming other parts of the example have been followedIMAGE_SIZE = 1280\n",
      "CHECKPOINT = \"PekingU/rtdetr_r50vd_coco_o365\"\n",
      "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "model = AutoModelForObjectDetection.from_pretrained(\n",
      "    CHECKPOINT,\n",
      "    id2label=id2label,\n",
      "    label2id=label2id,\n",
      "    anchor_image_size=None,\n",
      "    ignore_mismatched_sizes=True\n",
      ")Set the batch size to 4 and set to 4 visible GPUs assuming other parts of the example have been followedos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
      "\n",
      "training_args = TrainingArguments(\n",
      "    output_dir=output_path,\n",
      "    num_train_epochs=20,\n",
      "    max_grad_norm=0.1,\n",
      "    learning_rate=5e-5,\n",
      "    warmup_steps=300,\n",
      "    per_device_train_batch_size=4,\n",
      "    dataloader_num_workers=2,\n",
      "    metric_for_best_model=\"eval_map\",\n",
      "    greater_is_better=True,\n",
      "    load_best_model_at_end=True,\n",
      "    eval_strategy=\"epoch\",\n",
      "    save_strategy=\"epoch\",\n",
      "    save_total_limit=2,\n",
      "    remove_unused_columns=False,\n",
      "    eval_do_concat_batches=False,\n",
      ")Run trainingtrainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    train_dataset=pytorch_dataset_train,\n",
      "    eval_dataset=pytorch_dataset_valid,\n",
      "    tokenizer=processor,\n",
      "    data_collator=collate_fn,\n",
      "    compute_metrics=eval_compute_metrics_fn,\n",
      ")\n",
      "\n",
      "trainer.train()Stack trace:RuntimeError                              Traceback (most recent call last)\n",
      "Cell In[13], [line 11](vscode-notebook-cell:?execution_count=13&line=11)\n",
      "      [1](vscode-notebook-cell:?execution_count=13&line=1) trainer = Trainer(\n",
      "      [2](vscode-notebook-cell:?execution_count=13&line=2)     model=model,\n",
      "      [3](vscode-notebook-cell:?execution_count=13&line=3)     args=training_args,\n",
      "   (...)\n",
      "      [8](vscode-notebook-cell:?execution_count=13&line=8)     compute_metrics=eval_compute_metrics_fn,\n",
      "      [9](vscode-notebook-cell:?execution_count=13&line=9) )\n",
      "---> [11](vscode-notebook-cell:?execution_count=13&line=11) trainer.train()\n",
      "\n",
      "File ~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1938, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n",
      "   [1936](https://vscode-remote+ssh-002dremote-002bbrain-002ds-002d1-002ehq-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1936)         hf_hub_utils.enable_progress_bars()\n",
      "   [1937](https://vscode-remote+ssh-002dremote-002brain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1937) else:\n",
      "-> [1938](https://vscode-remote+ssh-002dremote-002bbrain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1938)     return inner_training_loop(\n",
      "   [1939](https://vscode-remote+ssh-002dremote-002bbrain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1939)         args=args,\n",
      "   [1940](https://vscode-remote+ssh-002dremote-002bbrain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1940)         resume_from_checkpoint=resume_from_checkpoint,\n",
      "   [1941](https://vscode-remote+ssh-002dremote-002bbrain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1941)         trial=trial,\n",
      "   [1942](https://vscode-remote+ssh-002dremote-002bbrain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1942)         ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "   [1943](https://vscode-remote+ssh-002dremote-002bbrain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:1943)     )\n",
      "\n",
      "File ~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:2279, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n",
      "   [2276](https://vscode-remote+ssh-002dremote-002bbrain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:2276)     self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n",
      "   [2278](https://vscode-remote+ssh-002dremote-002bbrain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:2278) with self.accelerator.accumulate(model):\n",
      "-> [2279](https://vscode-remote+ssh-002dremote-002bbrain-002ecom.vscode-resource.vscode-cdn.net/home/jb/repos/ml/ll_ml/models/rt_detr/~/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/trainer.py:2279)     tr_loss_step = self.training_step(model, inputs)\n",
      "...\n",
      "  File \"/home/jb/.cache/pypoetry/virtualenvs/ml-Mf12zaqr-py3.11/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr.py\", line 1850, in forward\n",
      "    reference_points_unact = torch.concat([denoising_bbox_unact, reference_points_unact], 1)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 16 but got size 4 for tensor number 1 in the list.It seems that the first tensor is not split amongst the GPUs? Is there something else I'm missing here? I have not set upacceleratefor instance, but the example linked does not state that it is required, and also states that at least one GPU is needed, making no specification about setting upaccelerate.Expected behaviorI would expect to see training taking place as it does when I use just one GPU:\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_70.txt:\n",
      "Title: Failed to import transformers.models.t5.modeling_t5 because of the following error\n",
      "URL: https://github.com/huggingface/transformers/issues/33140\n",
      "Body:\n",
      "System Infofrom transformers import T5ForConditionalGeneration, GenerationConfigRuntimeError: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):Failed to import transformers.generaWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction!pip install transformersfrom transformers import T5ForConditionalGeneration, GenerationConfigExpected behaviorthe command should import T5ForConditionalGeneration, GenerationConfig\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_58.txt:\n",
      "Title: TFSwinModel: AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n",
      "URL: https://github.com/huggingface/transformers/issues/33189\n",
      "Body:\n",
      "System Infopython==3.8.10transformers[tf]==4.38.2 (also tried 4.44.1)tensorflow==2.12.1keras==2.12.0Who can help?@ganteand@Rocketknight1InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI've tried to useTFSwinModelfor segmentation training, however I've got anAttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'during fit.. The code below should reproduce the error:import tensorflow as tf\n",
      "from transformers import SwinConfig, TFSwinModel\n",
      "\n",
      "image_size = 224\n",
      "\n",
      "# create dummy dataset\n",
      "X = tf.random.normal((10, 3, image_size, image_size))\n",
      "Y = tf.random.normal((10, 3, image_size, image_size))\n",
      "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
      "dataset = dataset.batch(4)\n",
      "\n",
      "# create model\n",
      "config = SwinConfig(image_size=image_size)\n",
      "model = TFSwinModel(config)\n",
      "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
      "\n",
      "# train model\n",
      "model.fit(dataset, epochs=1)Error message:Traceback (most recent call last):\n",
      "  File \"swin_dev.py\", line 16, in <module>\n",
      "    model.fit(dataset, epochs=1)\n",
      "  File \"/mnt/md0/workspace/home/akozlowski/venvs/troy3-8/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 1161, in fit\n",
      "    return super().fit(*args, **kwargs)\n",
      "[...]\n",
      "  File \"/mnt/md0/workspace/home/akozlowski/venvs/troy3-8/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 1562, in train_step\n",
      "    x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n",
      "AttributeError: in user code:\n",
      "\n",
      "    File \"/mnt/md0/workspace/home/akozlowski/venvs/troy3-8/lib/python3.8/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"/mnt/md0/workspace/home/akozlowski/venvs/troy3-8/lib/python3.8/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"/mnt/md0/workspace/home/akozlowski/venvs/troy3-8/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"/mnt/md0/workspace/home/akozlowski/venvs/troy3-8/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\", line 1562, in train_step\n",
      "        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n",
      "\n",
      "    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'Expected behaviorThe fit method runs training loop\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_882.txt:\n",
      "Title: [WIP] New Model Add FastPitch 1.1\n",
      "URL: https://github.com/huggingface/transformers/issues/16349\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_128.txt:\n",
      "Title: Q-GaLore Support\n",
      "URL: https://github.com/huggingface/transformers/issues/32839\n",
      "Body:\n",
      "Feature requestAdd support forhttps://github.com/VITA-Group/Q-GaLore(https://arxiv.org/abs/2407.08296)MotivationQ-GaLore allows more memory-efficient trainingYour contributionM/A\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_896.txt:\n",
      "Title: Number-specific tokenization changes\n",
      "URL: https://github.com/huggingface/transformers/issues/15485\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_666.txt:\n",
      "Title: [RFC] Updating pipeline models\n",
      "URL: https://github.com/huggingface/transformers/issues/26690\n",
      "Body:\n",
      "Feature requestWe're considering updating the default models used intransformerspipelines. This has the potential to greatly improve performance, and get rid of limitations caused by the existing models, but it may also break backward compatibility. Many of the default models have not been changed since the tasks were first added in pipelines, so users might assume that they are 'permanent', and might be surprised by an update.When updating pipelines, we would aim for the following objectives:The model should run on a base Colab instance (i.e. inference at max sequence length should fit inside 16GB VRAM)The default context length for text tasks should be long (at least 4k tokens where possible, ideally infinite with rope/alibi scaling)The performance should be as strong as reasonably possible within those two constraintsMotivationWe have seen a number ofuser issuesprompted by the default pipeline models intransformersbeing outdated. For example, the defaultsentiment-analysispipeline uses a finetuneddistilbertmodel with a maximum sequence length of 512 tokens. You can see the full list of default modelshere.Performance on these tasks could be greatly improved with more modern models that have newer features like longer (potentially unlimited!) context lengths.Your contributionI'll make the PR and potentially train new models for some of these tasks.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_100.txt:\n",
      "Title: whisper & flash_attention_2 & reduce overhead results in error\n",
      "URL: https://github.com/huggingface/transformers/issues/32934\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.1Platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35Python version: 3.10.6Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.3.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA GeForce RTX 4090Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRunfrom datasets import load_dataset\n",
      "from transformers import WhisperForConditionalGeneration, AutoProcessor\n",
      "import torch\n",
      "import logging\n",
      "import time\n",
      "\n",
      "torch._logging.set_logs(graph_breaks=True, recompiles=True)\n",
      "\n",
      "torch_device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
      "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
      "\n",
      "model_name = \"distil-whisper/distil-large-v3\"\n",
      "model_name = \"openai/whisper-tiny\"\n",
      "# model_name = \"openai/whisper-tiny.en\"\n",
      "processor = AutoProcessor.from_pretrained(model_name)\n",
      "model = WhisperForConditionalGeneration.from_pretrained(model_name, attn_implementation=\"flash_attention_2\",torch_dtype=torch.float16)\n",
      "model.to(torch_device, dtype=torch_dtype)\n",
      "\n",
      "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
      "sample = dataset[0][\"audio\"]\n",
      "inputs = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").to(torch_device)\n",
      "input_features = inputs.input_features.to(torch_dtype)\n",
      "\n",
      "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
      "model.generation_config.cache_implementation = \"static\"\n",
      "\n",
      "# compile\n",
      "for i in range(2):\n",
      "    model.generate(input_features, language=\"en\")\n",
      "\n",
      "# inference\n",
      "pred_ids = model.generate(input_features, language=\"en\")\n",
      "print(pred_ids)Expected behaviorI would expect not to return the error:Traceback (most recent call last):\n",
      "  File \"//sandit.py\", line 29, in <module>\n",
      "    model.generate(input_features, language=\"en\")\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py\", line 671, in generate\n",
      "    ) = self.generate_with_fallback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/generation_whisper.py\", line 832, in generate_with_fallback\n",
      "    seek_outputs = super().generate(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2024, in generate\n",
      "    result = self._sample(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2982, in _sample\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 451, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 921, in catch_errors\n",
      "    return callback(frame, cache_entry, hooks, frame_state, skip=1)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 400, in _convert_frame_assert\n",
      "    return _compile(\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 676, in _compile\n",
      "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py\", line 262, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 535, in compile_inner\n",
      "    out_code = transform_code_object(code, transform)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1036, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 165, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 500, in transform\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2149, in run\n",
      "    super().run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1260, in CALL_FUNCTION_EX\n",
      "    self.call_function(fn, argsvars.items, kwargsvars)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/nn_module.py\", line 336, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1260, in CALL_FUNCTION_EX\n",
      "    self.call_function(fn, argsvars.items, kwargsvars)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 335, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1272, in CALL_FUNCTION_KW\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/nn_module.py\", line 336, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1260, in CALL_FUNCTION_EX\n",
      "    self.call_function(fn, argsvars.items, kwargsvars)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 335, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1272, in CALL_FUNCTION_KW\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/nn_module.py\", line 336, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1260, in CALL_FUNCTION_EX\n",
      "    self.call_function(fn, argsvars.items, kwargsvars)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 335, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1272, in CALL_FUNCTION_KW\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/nn_module.py\", line 336, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1260, in CALL_FUNCTION_EX\n",
      "    self.call_function(fn, argsvars.items, kwargsvars)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 335, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1272, in CALL_FUNCTION_KW\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1219, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1219, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 289, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 90, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 680, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2285, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2399, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 810, in run\n",
      "    and self.step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 773, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 489, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1219, in CALL_FUNCTION\n",
      "    self.call_function(fn, args, {})\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 674, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/functions.py\", line 629, in call_function\n",
      "    unimplemented(msg)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/exc.py\", line 190, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: 'skip function find_spec in file /usr/lib/python3.10/importlib/util.py'\n",
      "\n",
      "from user code:\n",
      "   File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1764, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1636, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 1377, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 769, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/modeling_whisper.py\", line 506, in forward\n",
      "    attn_output = _flash_attention_forward(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_flash_attention_utils.py\", line 235, in _flash_attention_forward\n",
      "    if is_flash_attn_greater_or_equal(\"2.4.1\"):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 848, in is_flash_attn_greater_or_equal\n",
      "    if not _is_package_available(\"flash_attn\"):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 43, in _is_package_available\n",
      "    package_exists = importlib.util.find_spec(pkg_name) is not None\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_114.txt:\n",
      "Title: truncate_dimonBertModel\n",
      "URL: https://github.com/huggingface/transformers/issues/32881\n",
      "Body:\n",
      "Feature requestI have a pipeline to finetune an instance ofBertModel, on atext-classificationtask.I would like to usethis new embedding modelas my base embedding now.As can be seen in the example they provide, they are able to pass different values formatryoshka_diminto theSentenceTransformerinstance through thetruncate_dimargument.However, I was not able to do this on theBertModelin the following code snippet that I have in my code:self.bert_backbone=BertModel.from_pretrained(pretrained_model_name_or_path=self.config.embedding_model_file.model_name,cache_dir=Path(self.config.embedding_model_file.cache_dir),\n",
      "        ).to(self.device)And I do not want to use aSentenceTransformerinstance either as in my training loop I would like to be able to get:bert_outputs:BaseModelOutputWithPoolingAndCrossAttentions=(self.bert_backbone(input_ids=input_ids,attention_mask=attention_mask)\n",
      "                )bert_logits:Tensor=bert_outputs.last_hidden_state[\n",
      "                    :,0, :\n",
      "                ]# Take the [CLS] token outputand I am not sure if this code would work also with a simple swap toSentenceTransformer. In any case, I think that this is a potential parameter thatBertModelshould support, and maybe it does but I am just missing it.Thanks in advance!MotivationTo be able to extend theBertModelfurtherYour contribution.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_857.txt:\n",
      "Title: Adding TF Implementation of BEiT\n",
      "URL: https://github.com/huggingface/transformers/issues/18085\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_670.txt:\n",
      "Title: UmT5 Flax modelling\n",
      "URL: https://github.com/huggingface/transformers/issues/26529\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_116.txt:\n",
      "Title: Failed to import transformers.generation.utils because of the following error\n",
      "URL: https://github.com/huggingface/transformers/issues/32878\n",
      "Body:\n",
      "System Infowhen i input \"transformers-cli env\", its output is as below:Traceback (most recent call last):File \"/home/jovyan/.local/lib/python3.10/site-packages/transformer_engine/pytorch/init.py\", line 22, in _load_libraryso_path = next(so_dir.glob(f\"transformer_engine_torch.*.{extension}\"))StopIterationDuring handling of the above exception, another exception occurred:Traceback (most recent call last):File \"/home/jovyan/nas/envs/minicpm/bin/transformers-cli\", line 33, insys.exit(load_entry_point('transformers==4.40.0', 'console_scripts', 'transformers-cli')())File \"/home/jovyan/nas/envs/minicpm/bin/transformers-cli\", line 25, in importlib_load_entry_pointreturn next(matches).load()File \"/home/jovyan/nas/envs/minicpm/lib/python3.10/importlib/metadata/init.py\", line 171, in loadmodule = import_module(match.group('module'))File \"/home/jovyan/nas/envs/minicpm/lib/python3.10/importlib/init.py\", line 126, in import_modulereturn _bootstrap._gcd_import(name[level:], package, level)File \"\", line 1050, in _gcd_importFile \"\", line 1027, in _find_and_loadFile \"\", line 1006, in _find_and_load_unlockedFile \"\", line 688, in _load_unlockedFile \"\", line 883, in exec_moduleFile \"\", line 241, in _call_with_frames_removedFile \"/home/jovyan/nas/envs/minicpm/lib/python3.10/site-packages/transformers-4.40.0-py3.10.egg/transformers/commands/transformers_cli.py\", line 25, infrom .run import RunCommandFile \"/home/jovyan/nas/envs/minicpm/lib/python3.10/site-packages/transformers-4.40.0-py3.10.egg/transformers/commands/run.py\", line 17, infrom ..pipelines import Pipeline, PipelineDataFormat, get_supported_tasks, pipelineFile \"/home/jovyan/nas/envs/minicpm/lib/python3.10/site-packages/transformers-4.40.0-py3.10.egg/transformers/pipelines/init.py\", line 47, infrom .audio_classification import AudioClassificationPipelineFile \"/home/jovyan/nas/envs/minicpm/lib/python3.10/site-packages/transformers-4.40.0-py3.10.egg/transformers/pipelines/audio_classification.py\", line 21, infrom .base import Pipeline, build_pipeline_init_argsFile \"/home/jovyan/nas/envs/minicpm/lib/python3.10/site-packages/transformers-4.40.0-py3.10.egg/transformers/pipelines/base.py\", line 34, infrom ..modelcard import ModelCardFile \"/home/jovyan/nas/envs/minicpm/lib/python3.10/site-packages/transformers-4.40.0-py3.10.egg/transformers/modelcard.py\", line 48, infrom .training_args import ParallelModeFile \"/home/jovyan/nas/envs/minicpm/lib/python3.10/site-packages/transformers-4.40.0-py3.10.egg/transformers/training_args.py\", line 73, infrom accelerate.state import AcceleratorState, PartialStateFile \"/home/jovyan/.local/lib/python3.10/site-packages/accelerate/init.py\", line 16, infrom .accelerator import AcceleratorFile \"/home/jovyan/.local/lib/python3.10/site-packages/accelerate/accelerator.py\", line 35, infrom .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_stateFile \"/home/jovyan/.local/lib/python3.10/site-packages/accelerate/checkpointing.py\", line 24, infrom .utils import (File \"/home/jovyan/.local/lib/python3.10/site-packages/accelerate/utils/init.py\", line 181, infrom .bnb import has_4bit_bnb_layers, load_and_quantize_modelFile \"/home/jovyan/.local/lib/python3.10/site-packages/accelerate/utils/bnb.py\", line 29, infrom ..big_modeling import dispatch_model, init_empty_weightsFile \"/home/jovyan/.local/lib/python3.10/site-packages/accelerate/big_modeling.py\", line 24, infrom .hooks import (File \"/home/jovyan/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 30, infrom .utils.other import recursive_getattrFile \"/home/jovyan/.local/lib/python3.10/site-packages/accelerate/utils/other.py\", line 36, infrom .transformer_engine import convert_modelFile \"/home/jovyan/.local/lib/python3.10/site-packages/accelerate/utils/transformer_engine.py\", line 21, inimport transformer_engine.pytorch as teFile \"/home/jovyan/.local/lib/python3.10/site-packages/transformer_engine/pytorch/init.py\", line 34, in_load_library()File \"/home/jovyan/.local/lib/python3.10/site-packages/transformer_engine/pytorch/init.py\", line 25, in _load_libraryso_path = next(so_dir.glob(f\"transformer_engine_torch.*.{extension}\"))StopIterationWho can help?when i run Minicpm model, it shows :RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):pip listPackage                           Versionaccelerate                        0.30.1addict                            2.4.0aiofiles                          23.2.1aiohappyeyeballs                  2.3.7aiohttp                           3.10.4aiosignal                         1.3.1annotated-types                   0.7.0anyio                             4.4.0async-timeout                     4.0.3attrs                             24.2.0autocommand                       2.2.2backports.tarfile                 1.2.0Bottleneck                        1.3.7certifi                           2024.7.4charset-normalizer                3.3.2click                             8.1.7cloudpickle                       3.0.0cmake                             3.30.2colorama                          0.4.6contourpy                         1.2.1cycler                            0.12.1datasets                          2.21.0decord                            0.6.0dill                              0.3.8diskcache                         5.6.3distro                            1.9.0editdistance                      0.6.2einops                            0.7.0et-xmlfile                        1.1.0exceptiongroup                    1.2.2fairscale                         0.4.0fastapi                           0.112.1ffmpy                             0.4.0filelock                          3.15.4fonttools                         4.53.1frozenlist                        1.4.1fsspec                            2024.6.1git-lfs                           1.6gradio                            4.41.0gradio_client                     1.3.0h11                               0.14.0httpcore                          1.0.5httptools                         0.6.1httpx                             0.27.0huggingface-hub                   0.24.5idna                              3.7importlib_metadata                8.2.0importlib_resources               6.4.3inflect                           7.3.1interegular                       0.3.3jaraco.context                    5.3.0jaraco.functools                  4.0.1jaraco.text                       3.12.1Jinja2                            3.1.4jiter                             0.5.0joblib                            1.4.2jsonlines                         4.0.0jsonschema                        4.23.0jsonschema-specifications         2023.12.1kiwisolver                        1.4.5lark                              1.2.2llvmlite                          0.43.0lm-format-enforcer                0.10.3lxml                              5.3.0markdown-it-py                    3.0.0markdown2                         2.4.10MarkupSafe                        2.1.5matplotlib                        3.7.4mdurl                             0.1.2mkl-fft                           1.3.8mkl-random                        1.2.4mkl-service                       2.4.0modelscope_studio                 0.4.0.9more-itertools                    10.1.0mpmath                            1.3.0msgpack                           1.0.8multidict                         6.0.5multiprocess                      0.70.16nest-asyncio                      1.6.0networkx                          3.3ninja                             1.11.1.1nltk                              3.8.1numba                             0.60.0numexpr                           2.8.7numpy                             1.24.4nvidia-cublas-cu12                12.1.3.1nvidia-cuda-cupti-cu12            12.1.105nvidia-cuda-nvrtc-cu12            12.1.105nvidia-cuda-runtime-cu12          12.1.105nvidia-cudnn-cu12                 9.1.0.70nvidia-cufft-cu12                 11.0.2.54nvidia-curand-cu12                10.3.2.106nvidia-cusolver-cu12              11.4.5.107nvidia-cusparse-cu12              12.1.0.106nvidia-ml-py                      12.560.30nvidia-nccl-cu12                  2.20.5nvidia-nvjitlink-cu12             12.6.20nvidia-nvtx-cu12                  12.1.105openai                            1.41.0opencv-python-headless            4.5.5.64openpyxl                          3.1.2ordered-set                       4.1.0orjson                            3.10.7outlines                          0.0.46packaging                         23.2pandas                            2.2.2peft                              0.12.0Pillow                            10.1.0pip                               24.2platformdirs                      4.2.2portalocker                       2.10.1prometheus_client                 0.20.0prometheus-fastapi-instrumentator 7.0.0protobuf                          4.25.0psutil                            6.0.0py-cpuinfo                        9.0.0pyairports                        2.1.1pyarrow                           17.0.0pycountry                         24.6.1pydantic                          2.8.2pydantic_core                     2.20.1pydub                             0.25.1Pygments                          2.18.0pyparsing                         3.1.2python-dateutil                   2.9.0.post0python-dotenv                     1.0.1python-multipart                  0.0.9pytz                              2024.1PyYAML                            6.0.2pyzmq                             26.1.1ray                               2.34.0referencing                       0.35.1regex                             2024.7.24requests                          2.32.3rich                              13.7.1rpds-py                           0.20.0ruff                              0.6.1sacrebleu                         2.3.0safetensors                       0.4.4scikit-learn                      1.5.1scipy                             1.12.0seaborn                           0.13.0semantic-version                  2.10.0sentencepiece                     0.1.99setuptools                        72.1.0shellingham                       1.5.4shortuuid                         1.0.11six                               1.16.0sniffio                           1.3.1socksio                           1.0.0starlette                         0.38.2sympy                             1.13.2tabulate                          0.9.0threadpoolctl                     3.5.0tiktoken                          0.7.0timm                              0.9.10tokenizers                        0.19.1tomli                             2.0.1tomlkit                           0.12.0torch                             2.4.0torchvision                       0.19.0tqdm                              4.66.5transformer-engine                1.9.0.post1transformer_engine_cu12           1.9.0transformers                      4.40.0triton                            3.0.0typeguard                         4.3.0typer                             0.12.4typing_extensions                 4.12.2tzdata                            2023.3urllib3                           2.2.2uvicorn                           0.24.0.post1uvloop                            0.20.0vllm                              0.5.4vllm-flash-attn                   2.6.1watchfiles                        0.23.0websockets                        12.0wheel                             0.43.0wrapt                             1.16.0xformers                          0.0.27.post2xxhash                            3.5.0yarl                              1.9.4zipp                              3.20.0InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproduction1.download the model MiniCPM-V-2_6-int4:https://huggingface.co/openbmb/MiniCPM-V-2_6-int4run the python code:import torchfrom PIL import Imagefrom transformers import AutoModel, AutoTokenizermodel = AutoModel.from_pretrained('../../checkpoints/MiniCPM-V-2_6-int4', trust_remote_code=True,attn_implementation='sdpa', torch_dtype=torch.bfloat16)Expected behaviorit doesnot show any error\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_894.txt:\n",
      "Title: Porting Compressive Transformer to Huggingface\n",
      "URL: https://github.com/huggingface/transformers/issues/15533\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_499.txt:\n",
      "Title: Make model compatible withtorch.func\n",
      "URL: https://github.com/huggingface/transformers/issues/29463\n",
      "Body:\n",
      "Hi there,I am trying to use DebertaV3 as an alternative to BERT for an experiment that requirestorch.functransformations. However, since Deberta implementation relies on atorch.autograd.Function,torch.funcis not able to transform it. I get the following errorRuntimeError: In order to use an autograd.Function with functorch transforms \n",
      "  (vmap, grad, jvp, jacrev, ...), it must override the setup_context staticmethod. \n",
      "  For more details, please see https://pytorch.org/docs/master/notes/extending.func.htmlThe responsible lines are the classes implementing atorch.autograd.Function, for exampletransformers/src/transformers/models/deberta_v2/modeling_deberta_v2.pyLine 79\n",
      "      ina69cbf4classXSoftmax(torch.autograd.Function):which according to the PyTorch documentation (https://pytorch.org/docs/master/notes/extending.func.html) must implement a@staticmethoddefsetup_context(ctx,inputs,output):\n",
      "        ...method.Would it be possible to update this implementation to make it compatible with function transformations?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_66.txt:\n",
      "Title: it works for me  when adding the following two codes in the model requests code modeule\n",
      "URL: https://github.com/huggingface/transformers/issues/33158\n",
      "Body:\n",
      "import osos.environ['HF_ENDPOINT']= 'https://hf-mirror.com'Originally posted by@xiaozhen228in#17611 (comment)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_466.txt:\n",
      "Title: Bart evaluation throws the following error at generate(): UnboundLocalError: 'model_kwargs['decoder_attention_mask']' is used before assignment\n",
      "URL: https://github.com/huggingface/transformers/issues/29870\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.0Platform: Linux-5.4.0-167-generic-x86_64-with-glibc2.31Python version: 3.10.13Huggingface_hub version: 0.21.4Safetensors version: 0.4.2Accelerate version: 0.28.0Accelerate config:    not foundPyTorch version (GPU?): 2.2.1+cu121 (False)Tensorflow version (GPU?): 2.16.1 (False)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: noUsing distributed or parallel set-up in script?: noWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun the run_clm.py with bart given athttps://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarizationUse the latest tensorflow native version (2.16.1) and latest transformers version (4.39.1)I run only inference and so give only --do_eval while running the commandExpected behaviorProduce evaluation results\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_99.txt:\n",
      "Title: Bug in WhisperTokenizer batch_decode, when setskip_special_tokens=Truefor FlaxWhisper model output\n",
      "URL: https://github.com/huggingface/transformers/issues/32936\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.0Platform: Linux-5.15.0-1061-nvidia-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.0+cu124 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): 0.8.5 (gpu)Jax version: 0.4.29JaxLib version: 0.4.29Using distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA A100-SXM4-40GBWho can help?@sanchit-gandhiInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI use this piece of code to deploy a sample audio file on Flax Whisper-large-v3 model with Jax.fromtransformersimportFlaxWhisperForConditionalGeneration,WhisperTokenizerfromscipy.ioimportwavfileimportjaximportjax.numpyasjnpimportnumpyasnpimporttorchimportosos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"model_path=\"openai/whisper-large-v3\"audio_file_path=\".path/to/audio/audio_file.wav\"samplerate,data_waveform=wavfile.read(audio_file_path)tokenizer=WhisperTokenizer.from_pretrained(model_path)withtorch.no_grad():model=FlaxWhisperForConditionalGeneration.from_pretrained(model_path,dtype=jnp.float16,from_pt=True)jit_generate=jax.jit(model.generate,static_argnames=[\"max_length\",\"language\",\"task\"])samplerate,data_waveform=wavfile.read(audio_file_path)data_waveform=(data_waveform)/32768.0input_features=processor(data_waveform,padding=\"max_length\",sampling_rate=16000,return_tensors=\"pt\").input_featuresinput_features=jnp.array(input_features,dtype=jnp.float16)pred_ids=jit_generate(input_features,max_length=128,language='<|de|>',task=\"transcribe\")print(tokenizer.batch_decode(pred_ids.sequences,skip_special_tokens=True))It was working properly until version 4.42.4 of transformers, but from version 4.43.0 of transformers, it raises an error in the last line of the code (batch_decode):File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 3994, in batch_decode\n",
      "    return [\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 3995, in <listcomp>\n",
      "    self.decode(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/tokenization_whisper.py\", line 692, in decode\n",
      "    filtered_ids = self._preprocess_token_ids(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/tokenization_whisper.py\", line 637, in _preprocess_token_ids\n",
      "    token_ids = self._strip_prompt(token_ids, prompt_token_id, decoder_start_token_id)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/whisper/tokenization_whisper.py\", line 860, in _strip_prompt\n",
      "    if not token_ids:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/array.py\", line 258, in __bool__\n",
      "    core.check_bool_conversion(self)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\", line 654, in check_bool_conversion\n",
      "    raise ValueError(\"The truth value of an array with more than one element\"\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()However in thebatch_decodemethod, if I disable theskip_special_tokensarg (set it toFalse), it raises no error but return lots of special chars.Expected behaviorIt is expected to return list of strings in the result ofbatch_decodemethod, as same as how it works until version 4.42.4 of transformers\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_314.txt:\n",
      "Title: Mismatch with epoch when using gradient_accumulation\n",
      "URL: https://github.com/huggingface/transformers/issues/31677\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.0.dev0Platform: Linux-5.4.0-167-generic-x86_64-with-glibc2.35Python version: 3.10.14Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.30.1Accelerate config:    not foundPyTorch version (GPU?): 2.2.2 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA TITAN RTXWho can help?@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThis is the issue of mismatch defined epoch and actual train epoch.Even though I set 24 epoch in trainarguments and set gradient_accumulation_step as 2. There is a mismatch of calculating max_steps when it is not set.transformers/src/transformers/trainer.pyLine 1983\n",
      "      in1c68f2c# May be slightly incorrect if the last batch in the training dataloader has a smaller size but it'sExpected behaviortransformers/src/transformers/trainer.pyLine 1975\n",
      "      in1c68f2cnum_update_steps_per_epoch=len_dataloader//args.gradient_accumulation_stepsIf we just use normal divider it solves the issue. Is there any specific reason that num_update_steps_per_epoch should be remained as an integer?num_update_steps_per_epoch = len_dataloader / args.gradient_accumulation_steps\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_472.txt:\n",
      "Title: Community contribution: enablingdevice_map=\"auto\"support for more vision and multimodal models\n",
      "URL: https://github.com/huggingface/transformers/issues/29786\n",
      "Body:\n",
      "Feature requestFeature Requesttransformersmodels can be easily loaded across multiple devices usingdevice_map=\"auto\". This will automatically allocate weights across available devices e.g. GPUs and offload any weights onto CPU, then disk as necessary. This is useful when doing inference with large models.To enable this,_no_split_moduleshas to be defined in the model's pretrained model class e.g.like here for LLaMa. This defines layers which should not be split across devices, and should contain as few layers as possible.Steps to addPick a model to work on and open a PR - comment on this issue to say which model you're working onDefine_no_split_modulesin the PreTrainedModel subclass. Try with_no_split_modules = []firstEnable testingEnsure the following tests are not skipped for the model:test_disk_offload_bin,test_disk_offload_safetensors,test_cpu_offload,test_model_parallelism,test_model_parallel_beam_searchRun the tests in a multi-gpu environmentpytest tests/models/{MODEL_NAME}/test_modeling_{MODEL_NAME}.py -vv -k \"offload or parallelism\"ModelsAlignAltclipBeitEnable multi-device for more models#30379BitBlipChinese_clipConvnextEnable multi-device for some models#30207Convnextv2Enable multi-device for some models#30207CvtEnable multi-device for some models#30207Data2vecDepth_anythingDinatDinov2DonutDptEfficientformerEfficientnetEnable multi-device for efficientnet#29989FlavaFocalnetEnable multi-device for some models#30207GitGlpnEnable multi-device for some models#30207GroupvitImagegptEnable multi-device for some models#30207Layoutlmv3LevitEnable multi-device for some models#30207Mask2formerMaskformerMaskformerMgp_strEnable multi-device for some models#30207Mobilenet_v1Enable multi-device for some models#30207Mobilenet_v2Enable multi-device for some models#30207MobilevitEnable multi-device for some models#30207Mobilevitv2NatOneformerPerceiverPoolformerEnable multi-device for some models#30207PvtRegnetEnable multi-device for some models#30207ResnetEnable multi-device for some models#30207SamEnable multi-device for some models#30207SegformerSwiftformerEnable multi-device for some models#30207SwinEnable multi-device for some models#30207Swin2srSwinv2Enable multi-device for some models#30207TimesformerTimm_backboneTrocrEnable multi-device for some models#30207TvltTvpUpernetEnable multi-device for some models#30207VideomaeVit_maeVit_msnVitmatteEnable multi-device for more models#30379VivitEnable multi-device for more models#30379X_clipYolosEnable multi-device for some models#30207MotivationEnable a powerful HF feature for all of our vision modelsYour contributionPing me for review 🤗\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_328.txt:\n",
      "Title: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (HF/Accelerate)\n",
      "URL: https://github.com/huggingface/transformers/issues/31504\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-4.15.0-43-generic-x86_64-with-glibc2.17Python version: 3.8.13Huggingface_hub version: 0.23.4Safetensors version: 0.4.1Accelerate version: 0.29.3Accelerate config:    not foundPyTorch version (GPU?): 1.13.1+cu116 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: YesUsing distributed or parallel set-up in script?: YesWho can help?@SunMarc,@ArthurZucker,@younesbelkadaand@muellerzrInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI'm trying to run Big Model Inference with HF's accelerate package with the following code (in multi-GPU) setting, but keep getting cuda-related error attached below. Here's the code:Code:from huggingface_hub import snapshot_download\n",
      "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
      "from accelerate import load_checkpoint_and_dispatch\n",
      "from accelerate import Accelerator, init_empty_weights\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "import torch\n",
      "\n",
      "\n",
      "# A simple dataset class\n",
      "class TextDataset(Dataset):\n",
      "    def __init__(self, texts, tokenizer, max_length=512):\n",
      "        self.texts = texts\n",
      "        self.tokenizer = tokenizer\n",
      "        self.max_length = max_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.texts)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        text = self.texts[idx]\n",
      "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
      "        return inputs\n",
      "\n",
      "\n",
      "# Some random text\n",
      "input_texts = [\n",
      "    \"Once upon a time, in a land far, far away...\",\n",
      "    \"In the beginning, there was darkness, and then there was light.\",\n",
      "    \"The quick brown fox jumps over the lazy dog.\",\n",
      "    \"To be or not to be, that is the question.\",\n",
      "    \"A journey of a thousand miles begins with a single step.\"\n",
      "]\n",
      "\n",
      "accelerator = Accelerator()\n",
      "checkpoint = \"microsoft/Phi-3-medium-4k-instruct\"\n",
      "weights_location = snapshot_download(repo_id=checkpoint)\n",
      "\n",
      "\n",
      "model_config = AutoConfig.from_pretrained(checkpoint, trust_remote_code=True)\n",
      "with init_empty_weights():\n",
      "    model = AutoModelForCausalLM.from_config(config=model_config)\n",
      "\n",
      "model = load_checkpoint_and_dispatch(\n",
      "    model, checkpoint=weights_location, device_map=\"auto\", no_split_module_classes=['Block']\n",
      ")\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
      "dataset = TextDataset(input_texts, tokenizer)\n",
      "data_loader = DataLoader(dataset, batch_size=1)\n",
      "\n",
      "model, data_loader = accelerator.prepare(model, data_loader)\n",
      "\n",
      "for batch in data_loader:\n",
      "    # Generate text\n",
      "    outputs = model.generate(batch['input_ids'][0], max_new_tokens=50)Error (on linemodel.generate(batch['input_ids'][0].to(device), max_new_tokens=50)):Traceback (most recent call last):File \"test.py\", line 65, inoutputs = model.generate(batch['input_ids'][0].to(device), max_new_tokens=50)File \"/home/sasha/anaconda3/envs/myenv-py38/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_contextreturn func(*args, **kwargs)File \"/home/sasha/anaconda3/envs/myenv-py38/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1758, in generateresult = self._sample(File \"/home/sasha/anaconda3/envs/myenv-py38/lib/python3.8/site-packages/transformers/generation/utils.py\", line 2397, in _sampleoutputs = self(File \"/home/sasha/anaconda3/envs/myenv-py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_implreturn forward_call(*input, **kwargs)File \"/home/sasha/anaconda3/envs/myenv-py38/lib/python3.8/site-packages/accelerate/hooks.py\", line 166, in new_forwardoutput = module._old_forward(*args, **kwargs)File \"/disk1/sasha/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-medium-4k-instruct/d194e4e74ffad5a5e193e26af25bcfc80c7f1ffc/modeling_phi3.py\", line 1286, in forwardoutputs = self.model(File \"/home/sasha/anaconda3/envs/myenv-py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_implreturn forward_call(*input, **kwargs)File \"/disk1/sasha/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-medium-4k-instruct/d194e4e74ffad5a5e193e26af25bcfc80c7f1ffc/modeling_phi3.py\", line 1164, in forwardlayer_outputs = decoder_layer(File \"/home/sasha/anaconda3/envs/myenv-py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_implreturn forward_call(*input, **kwargs)File \"/disk1/sasha/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-medium-4k-instruct/d194e4e74ffad5a5e193e26af25bcfc80c7f1ffc/modeling_phi3.py\", line 894, in forwardhidden_states = residual + self.resid_attn_dropout(attn_outputs)RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!Expected behaviorGeneration of output text, from the Big model without any cuda-related error!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_512.txt:\n",
      "Title: Unable to trace DebertaForMaskedLM\n",
      "URL: https://github.com/huggingface/transformers/issues/29288\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.1Platform: Linux-5.4.0-47-generic-x86_64-with-glibc2.17Python version: 3.8.13Huggingface_hub version: 0.19.4Safetensors version: 0.3.1Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.1.2+cu118 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom config import load_configfrom transformers import (DebertaForMaskedLM,DebertaConfig,)from transformers.utils.fx import symbolic_tracedef main():config_kwarg = load_config(\"deberta-large\")config = DebertaConfig(**config_kwarg)# create model\n",
      "model = DebertaForMaskedLM(config)\n",
      "\n",
      "traced_model = symbolic_trace(model)ifname== \"main\":main()Expected behaviorWhen I run the scirpt, get following error.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_274.txt:\n",
      "Title: Uniform kwargs for processors\n",
      "URL: https://github.com/huggingface/transformers/issues/31911\n",
      "Body:\n",
      "Feature requestWe want to standardize the logic flow through Processor classes. Since processors can have different kwargs depending on the model and modality, we are adding aTypedDictfor each modality to keep track of which kwargs are accepted.The initial design is merged and an example model is modified to follow the new uniform processor kwargs in#31198. Also#31197has two more examples with standardized API.This design has to be shipped to all the processors in Transformers, and appreciate contributions.Below is an incomplete list of models that need standardization, feel free to add a model if it's missing:AlignUniformize model processors#31368AltClipUniformize model processors#31368BLIPUniformize model processors#31368BLIP-2Uniformize model processors#31368BridgetowerUniformize model processors#31368Chameleon ->Uniformize kwargs for chameleon processor#32181Chinese CLIPUniformize model processors#31368CLIP -> in progress by@davidgxueClipSegDonutUniformize model processors#31368Fuyu ->Uniformize kwargs for image-text-to-text processors#32544Grounding DINOUniformize kwargs for processors - GroundingDINO#31964Idefics ->Uniformize kwargs for Idefics/2 processors#32568Idefics-2 ->Uniformize kwargs for Idefics/2 processors#32568InstructBlip ->Uniformize kwargs for image-text-to-text processors#32544InstructBlipVideoKosmos-2 ->Uniformize kwargs for image-text-to-text processors#32544LayoutLM (1, 2, 3) ->Uniformize kwargs for Layoutlm (2, 3, X) processors#32180LLaVa ->Uniformize kwargs for LLaVa processor and update docs#32858LLaVa-NeXT ->Uniformize kwargs for image-text-to-text processors#32544LLaVa-NeXT-VideoNouga ->Uniformize model processors (models *with* special arg names)#32841Owlv2SigLip ->Uniformize processor kwargs of siglip#32842Paligemma ->Adds uniform processing kwargs to paligemma.#32377Pix2Struct ->Uniformize kwargs for image-text-to-text processors#32544Udop ->Uniformize kwargs for image-text-to-text processors#32544VideoLLaVaNote: For now we'll start with image or image+text,#31368is an ongoing PR that has also audio processor standardizationMotivation.Your contribution.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_506.txt:\n",
      "Title: hyperparameter_serch() does not consider LoRA parameters like r  to be finetuned.\n",
      "URL: https://github.com/huggingface/transformers/issues/29391\n",
      "Body:\n",
      "Feature requestHyperparameter search is a must known activity to get hyperparameters  which are for optimised machine learning or deep learning model output. I was trying hyperparameter_search() method to get optimal hyperparameters values. I wanted to tune LoRA confugarition parameters like rank r and alpha too. But i found that it is not able to finetune LoRA configuration paramaters.MotivationFollowing code part will give you the ideadefmodel_init():device_map={\"\":torch.cuda.current_device(\n",
      "                 )}iftorch.cuda.is_available()elseNone​model_kwargs_dict=dict(# set this to True if your GPU supports it#(Flash Attention drastically speeds up model computations)#attn_implementation=\"flash_attention_2\",torch_dtype=\"auto\",# set to False as we're going to use gradient checkpointinguse_cache=False,device_map=device_map,\n",
      "    )device_map={\"\":torch.cuda.current_device(\n",
      "             )}iftorch.cuda.is_available()elseNone​bnb_config_args=dict(load_in_4bit=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_compute_dtype=torch.bfloat16,bnb_4bit_use_double_quant=False)bnb_config=BitsAndBytesConfig(**bnb_config_args)model_kwargs_dict[\"quantization_config\"]=bnb_configmodel=AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\",return_dict=True,#**model_kwargs_dict)print(peft_config)model=get_peft_model(model,peft_config=peft_config)returnmodeldataset=load_dataset(\"imdb\",split=\"train\")tokenizer=AutoTokenizer.from_pretrained(\"facebook/opt-125m\")dataset1=dataset.select([0,10,20,30,40,50])dataset2=dataset.select([0,10,20,30,40,50])trainer=SFTTrainer(model=None,args=training_args,model_init=model_init,tokenizer=tokenizer,train_dataset=dataset1,eval_dataset=dataset2,dataset_text_field=\"text\",max_seq_length=512,)defoptuna_hp_space(trial):return{\"learning_rate\":trial.suggest_float(\"learning_rate\",1e-6,1e-4,log=True),\"per_device_train_batch_size\":trial.suggest_categorical(\"per_device_train_batch_size\", [16,32,64,128]),\"r\":trial.suggest_float(\"r\",2,4,log=True),\n",
      "    }trainer.hyperparameter_search(direction=[\"minimize\"],backend=\"optuna\",hp_space=optuna_hp_space,n_trials=2)The code above has resulted in output like[I 2024-02-29 09:42:36,869] A new study created in memory with name: no-name-331fbdff-6465-42f8-9c97-ad5c6c8c4703Trying to set r in the hyperparameter search but there is no corresponding field inTrainingArguments.Your contributionNA\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_248.txt:\n",
      "Title: RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback): module 'tensorflow' has no attribute 'data'\n",
      "URL: https://github.com/huggingface/transformers/issues/32122\n",
      "Body:\n",
      "im using followinglibrosastreamlitpandassoundfilenumpyscipysounddevicepydubpyaudiomatplotlibstreamlit==1.36.0transformers==4.41.2torch==2.1accelerate>=0.21.0tensorflow==2.13.0keras==2.13.1deep-utilsnoisereducedatasetsevaluatehuggingface_hublibrosascikit-learnand try to deploy my streamlit app but facing same in local and cloud as well\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_710.txt:\n",
      "Title: Add FastViT model\n",
      "URL: https://github.com/huggingface/transformers/issues/25526\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_705.txt:\n",
      "Title: TimeSeriesTransformerForPrediction model unused parameters Runtime error in Distributed environment\n",
      "URL: https://github.com/huggingface/transformers/issues/25620\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_711.txt:\n",
      "Title: Implement SuperPoint / SuperGlue\n",
      "URL: https://github.com/huggingface/transformers/issues/25489\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_922.txt:\n",
      "Title: Dependency parsing head for pretrained models\n",
      "URL: https://github.com/huggingface/transformers/issues/13355\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_936.txt:\n",
      "Title: Inconsistency between the tokenization ofCLIPTokenizerandCLIPTokenizerFastwithopenai/clip-vit-base-patch32\n",
      "URL: https://github.com/huggingface/transformers/issues/12648\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_249.txt:\n",
      "Title: Output from model.Generate & model.forward not same when output attention/hidden_state is True\n",
      "URL: https://github.com/huggingface/transformers/issues/32117\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.4Platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.31Python version: 3.10.13Huggingface_hub version: 0.24.0Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.2.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: NoUsing GPU in script?: NoGPU type: Tesla V100-SXM2-32GBWho can help?@gante@arthInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionCommon Step:tokenizer = GemmaTokenizer.from_pretrained(model_path, device_map = device)\n",
      "model = GemmaForCausalLM.from_pretrained(model_path, device_map = device)\n",
      "\n",
      "prompt = [\"[INST]What is USA?[/INST]\"]\n",
      "input_tokens = tokenizer(prompt, return_tensors = 'pt', padding=True).to(device)\n",
      "\n",
      "## Lets call this common_output\n",
      "generate_ids = model.generate(**input_tokens, return_dict_in_generate=True,\n",
      "                            temperature=None,\n",
      "                            top_p=None, max_new_tokens=5, use_cache=False)Case 1:test_input_tokens = input_tokens\n",
      "\n",
      "generate_ids_1 = model.forward(**test_input_tokens, output_attentions=True, output_hidden_states=True)\n",
      "next_token_logits = generate_ids_1.logits[:, -1, :]\n",
      "test_input_tokens[\"input_ids\"] = torch.concat((test_input_tokens[\"input_ids\"], \n",
      "                                   next_token_logits.argmax().unsqueeze(dim=0).unsqueeze(dim=0)), dim=-1)\n",
      "test_input_tokens[\"attention_mask\"] = torch.concat((test_input_tokens[\"attention_mask\"], torch.ones((1,1), dtype=torch.long).to(\"cuda\")), dim=-1)Case 2:test_input_tokens = input_tokens\n",
      "\n",
      "generate_ids_1 = model.forward(**test_input_tokens)\n",
      "next_token_logits = generate_ids_1.logits[:, -1, :]\n",
      "test_input_tokens[\"input_ids\"] = torch.concat((test_input_tokens[\"input_ids\"], \n",
      "                                   next_token_logits.argmax().unsqueeze(dim=0).unsqueeze(dim=0)), dim=-1)\n",
      "test_input_tokens[\"attention_mask\"] = torch.concat((test_input_tokens[\"attention_mask\"], torch.ones((1,1), dtype=torch.long).to(\"cuda\")), dim=-1)We are getting output of generate function & output of Case 2 same. But for output Case 1 is different because we have set \"output_attentions=True, output_hidden_states=True\" which we believe should be used for debugging purpose.Expected behaviorOutput of generate should be same as Case 1 & Case 2's output.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_261.txt:\n",
      "Title: Usingnumpy==2.0.0\n",
      "URL: https://github.com/huggingface/transformers/issues/32055\n",
      "Body:\n",
      "transformersnow remove the pin ofnumpy<2.0.0. However, some issues (with 3rd libraries) are known, and the following gives some guide:In an environment withTensorFlowand/orFlax--> downgrade tonumpy<2.0.0issue with soxr -->pip install soxr==0.4.0b1issue withfaiss--> downgrade tonumpy<2.0.0If error out with a messageusing np.array(obj, copy=False) replace it withnp.asarray(obj) to allow a copy`:downgrade tonumpy<2.0.0or, installdatasetsdev. version\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_507.txt:\n",
      "Title: MPNet doesn't have an implemented LMHead subclass\n",
      "URL: https://github.com/huggingface/transformers/issues/29385\n",
      "Body:\n",
      "Feature requestThe code of mpnet needs a proper implementation of LMHead class to adapt to Language Generation TasksMotivationWe want to perform some experiments on the MPNet style inference on language generation tasks. On going through the codebase, there is a standby class inherited fromtorch.nn.module, which means there is no proper implementation of LMHead on this modelYour contributionI'm eager to contribute to this issue by submitting a PR. This work would be super relevant for us to extend our experiments to MPNet as well for our research projects.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_275.txt:\n",
      "Title: RuntimeError: Expected a 'mps:0' generator device but found 'cpu'\n",
      "URL: https://github.com/huggingface/transformers/issues/31897\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.2Platform: macOS-14.5-arm64-arm-64bitPython version: 3.11.9Huggingface_hub version: 0.23.1Safetensors version: 0.4.2Accelerate version: 0.32.1Accelerate config:    not foundPyTorch version (GPU?): 2.3.0.post100 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: noWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionWhile trying to run Trainer.train() on a Mac device, I run into the following error:trainer.train()\n",
      "  File \"/Users/kateatwell/miniconda3/envs/huggingface-trainer/lib/python3.11/site-packages/transformers/trainer.py\", line 1539, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kateatwell/miniconda3/envs/huggingface-trainer/lib/python3.11/site-packages/transformers/trainer.py\", line 1836, in _inner_training_loop\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"/Users/kateatwell/miniconda3/envs/huggingface-trainer/lib/python3.11/site-packages/accelerate/data_loader.py\", line 454, in __iter__\n",
      "    current_batch = next(dataloader_iter)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kateatwell/miniconda3/envs/huggingface-trainer/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kateatwell/miniconda3/envs/huggingface-trainer/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n",
      "    index = self._next_index()  # may raise StopIteration\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kateatwell/miniconda3/envs/huggingface-trainer/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 621, in _next_index\n",
      "    return next(self._sampler_iter)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kateatwell/miniconda3/envs/huggingface-trainer/lib/python3.11/site-packages/torch/utils/data/sampler.py\", line 287, in __iter__\n",
      "    for idx in self.sampler:\n",
      "  File \"/Users/kateatwell/miniconda3/envs/huggingface-trainer/lib/python3.11/site-packages/torch/utils/data/sampler.py\", line 167, in __iter__\n",
      "    yield from torch.randperm(n, generator=generator).tolist()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kateatwell/miniconda3/envs/huggingface-trainer/lib/python3.11/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Expected a 'mps:0' generator device but found 'cpu'This error is caused by the following code:model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=num_labels)\n",
      "    model.to(\"mps\")\n",
      "    training_args = TrainingArguments(output_dir=output_dir, use_mps_device=True, remove_unused_columns=False)\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        train_dataset=train_data,\n",
      "        eval_dataset=val_data,\n",
      "        compute_metrics=compute_metrics,\n",
      "    )\n",
      "    trainer.train()train_data and val_data are instances of the following custom dataset:class TweetDataset(Dataset):\n",
      "    def __init__(self, data, tokenizer, max_length):\n",
      "        self.data = data\n",
      "        self.tokenizer = tokenizer\n",
      "        self.max_length = max_length\n",
      "        input_ids = []\n",
      "        labels = []\n",
      "        encodings = []\n",
      "        for i, row in data.iterrows():\n",
      "            text = row[\"text\"]\n",
      "            label = row[\"is_misinfo\"]\n",
      "            encoding = tokenizer(text, padding=\"max_length\", truncation=True)\n",
      "            input_id = encoding[\"input_ids\"]\n",
      "            input_ids.append(torch.tensor(input_id))\n",
      "            labels.append(label)\n",
      "            encodings.append(encoding)\n",
      "        self.encodings = encodings\n",
      "        self.input_ids = torch.cat(input_ids).to(\"mps\")\n",
      "        self.labels = labels\n",
      "    def __getitem__(self, idx):\n",
      "        item = {key: torch.tensor(val, device=\"mps\") for key, val in self.encodings[idx].items()}\n",
      "        item[\"labels\"] = torch.tensor(self.labels[idx], device=\"mps\")\n",
      "        return item\n",
      "    def __len__(self):\n",
      "        return len(self.data)Expected behaviorInitializing generators on MPS, as opposed to CPU, and not throwing an error\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_329.txt:\n",
      "Title: FineWeb SLM Training doesn't start\n",
      "URL: https://github.com/huggingface/transformers/issues/31501\n",
      "Body:\n",
      "System InfoThis is my dev env:https://github.com/abhinand5/runpod-utils/blob/main/docker/torch-lm-dev/DockerfileUsing the latest docker.Torch 2.3.1CUDA 12.1Who can help?@ArthurZucker@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI am using a custom version of therun_clm.py. Where the only changes are:Accepted YAML input along with JSON for parsing argumentsAbility to change attention implementation with an argumentHandlingvalidation_split_percentageless than 1% for huge datasets (because 1% is actually a lot).After the dataset is preprocessed, running the tokenizer and the values are cached...the training takes a ton of time to start. Waited one hour on 4 different occasions and killed the instance cuz as you know GPUs aren't cheap.It works when I reduce the max train samples to something small like 10000. SeeDEBUGGINGin the YAML below.Here is my config:#--- Model settings ---model_name_or_path:abhinand/personal-model-init-fp16model_revision:main#--- Training settings ---do_train:trueseed:1337#--- Cache and torch settings ---cache_dir:/workspace/.cachetrust_remote_code:truetorch_dtype:bfloat16attn_implementation:\"flash_attention_2\"#--- Dataset settings ---dataset_name:HuggingFaceFW/fineweb-edudataset_config_name:sample-10BT#--- DEBUGGING ---#max_train_samples: 1000#max_steps: 60#max_eval_samples: 1000#-----------------#--- Data processing settings ---block_size:2048overwrite_cache:falsevalidation_split_percentage:0.5#--- Preprocessing settings ---preprocessing_num_workers:16output_dir:./outputsoverwrite_output_dir:true#--- Evaluation settings ---eval_strategy:stepseval_steps:50per_device_train_batch_size:8per_device_eval_batch_size:8gradient_accumulation_steps:16eval_accumulation_steps:16#--- Optimizer settings ---optim:adamw_torch_fusedlearning_rate:3.0e-4adam_beta1:0.9adam_beta2:0.95adam_epsilon:1.0e-8max_grad_norm:1.0num_train_epochs:1.0lr_scheduler_type:cosinewarmup_ratio:0.05#warmup_steps: 500weight_decay:0.01#--- Logging settings ---logging_strategy:stepslogging_steps:1save_strategy:stepssave_steps:50save_total_limit:5save_safetensors:truebf16:truefp16:false#bf16_full_eval: true#--- Torch settings ---torch_compile:false#not sure why it doesn't work with flash_attention_2include_tokens_per_second:trueinclude_num_input_tokens_seen:true#--- Hub settings ---push_to_hub:truehub_model_id:abhinand/personal-model-v0-test1#hub_strategy:hub_private_repo:trueExpected behaviorTraining to start...\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_473.txt:\n",
      "Title: [Feature request] FastV: Plug-and-play inference acceleration  for  large vision language models\n",
      "URL: https://github.com/huggingface/transformers/issues/29751\n",
      "Body:\n",
      "Feature requestFastV is a plug-and-play inference acceleration method for large vision language models relying on visual tokens. It could reach 45% theoretical FLOPs reduction without harming the performance through pruning redundant visual tokens in deep layers. It is originally implemented inhttps://github.com/pkunlp-icler/FastV, withpaperanddemoaokvqa resultsModelScorelatency / first output token (A100 80G)GPU Memory7B Vanilla Decoding76.80.138s18G13B Vanilla Decoding81.90.203s33G- 13B FastV (K=2 R=25%)81.80.181s29G- 13B FastV (K=2 R=50%)81.30.155s28G- 13B FastV (K=2 R=75%)80.90.124s27G13B Vanilla Decoding 4Bit81.50.308s12G- 13B FastV 4Bit (K=2 R=25%)81.70.277s11G- 13B FastV 4Bit (K=2 R=50%)81.10.275s10G- 13B FastV 4Bit (K=2 R=75%)80.30.245s9GMotivationI want to merge the FastV feature to speed up the inference of HF's multimodal models (Llava is the first one).  Since Llava is based on Llama, I need to add features to both the Llama and Llava model.I would add a fastv_config parameters to the modeling_llava.py from llava and add a new forward function named fastv_forward from llama. I want to know if it is ok to do that or I should create a new model.Your contributionI would submit a PR.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_301.txt:\n",
      "Title: Fix 'Can't infer missing attention mask onmpsdevice'\n",
      "URL: https://github.com/huggingface/transformers/issues/31744\n",
      "Body:\n",
      "Feature requestI was trying outlocal-gemmajust now on my M1 macbook and ran into this error:ValueError: Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.The proposed temporary solution of usingPYTORCH_ENABLE_MPS_FALLBACK=1was not working. The cause of the error is this bit of code ingeneration/utils.py:def_prepare_attention_mask_for_generation(self,inputs:torch.Tensor,pad_token_id:Optional[torch.Tensor],eos_token_id:Optional[torch.Tensor],\n",
      "    )->torch.LongTensor:# ...ifinputs.device.type==\"mps\":# mps does not support torch.isin (https://github.com/pytorch/pytorch/issues/77764)raiseValueError(\"Can't infer missing attention mask on `mps` device. Please provide an `attention_mask` or use a different device.\")is_pad_token_in_inputs=(pad_token_idisnotNone)and(torch.isin(elements=inputs,test_elements=pad_token_id).any()\n",
      "        )is_pad_token_not_equal_to_eos_token_id=(eos_token_idisNone)or~(torch.isin(elements=eos_token_id,test_elements=pad_token_id).any()\n",
      "        )Mainly,torch.isinis not supported. I don't know all the places where_prepare_attention_mask_for_generationis used or how it's used. But just a cursory glance at the code made me wonder if this is really necessary:torch.isin(elements=inputs,test_elements=pad_token_id).any()torch.isin(elements=eos_token_id,test_elements=pad_token_id).any()I fixed my issue by changing those to:(inputs==pad_token_id).any()#  was: torch.isin(elements=inputs, test_elements=pad_token_id).any()# andeos_token_id==pad_token_id# was: torch.isin(elements=eos_token_id, test_elements=pad_token_id).any()and removing the mps conditional.Again, I'm not sure if this will actually cover all use cases of that helper function but if it does, could we please go with the simple solution untilisinis supported by MPS?MotivationThis shouldjust workon a macbook with mps without hacks like settingPYTORCH_ENABLE_MPS_FALLBACK=1(which didn't work anyway).Your contributionIf the proposed solution looks good then I can submit a PR (or someone else can, either way I just want my code to work).\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_98.txt:\n",
      "Title: Some casualLM models don't get position_ids in their forward pass.\n",
      "URL: https://github.com/huggingface/transformers/issues/32937\n",
      "Body:\n",
      "Feature requestThere are some models such that their forward pass doesn't get position_ids. e.g. we can see that OPTModel doesn't get position_ids, while GPTJModel does get position_ids. most newer models do have position_ids.MotivationThere are two main reasons we would like for all LM models to get positions ids.to have the API be consistent with all models.position_ids are very important if you want to use flash-attention without padding, during training. if i want to be able to pack two or more sentences in the same sequence. I would like to know that the model handles the sentences accordingly and treats each sentence as it's own different sentence. flash-attention code uses position_ids to check if some sequences are packed and runs an appropriate function to make sure there is no cross example contamination. but without this the model can't use this feature. the code always checks if position_ids is not None:https://github.com/huggingface/transformers/blob/v4.44.1/src/transformers/modeling_flash_attention_utils.py#L270Your contributionI may be able to fix this and help with a PR. but would love a more experienced person to guide me.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_73.txt:\n",
      "Title: TypeError: Object of type Tensor is not JSON serializable\n",
      "URL: https://github.com/huggingface/transformers/issues/33134\n",
      "Body:\n",
      "System Infotransformersversion: 4.37.2Platform: Linux-3.10.0-1160.59.1.el7.x86_64-x86_64-with-glibc2.10Python version: 3.8.0Huggingface_hub version: 0.24.6Safetensors version: 0.4.2Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 1.13.1+cu117 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@sanchit-gandhiInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom datasets import load_dataset, load_metric\n",
      "from datasets import ClassLabel, Audio\n",
      "import datasets\n",
      "import random\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re, time\n",
      "import json, librosa\n",
      "from peft import PeftModel, PeftConfig\n",
      "import soundfile as sf\n",
      "import torch\n",
      "torch.backends.cudnn.enabled = False\n",
      "from dataclasses import dataclass, field\n",
      "from typing import Any, Dict, List, Optional, Union\n",
      "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
      "import pdb\n",
      "from transformers import TrainingArguments\n",
      "from transformers import Trainer\n",
      "from transformers import WhisperProcessor, WhisperForConditionalGeneration , WhisperTokenizer\n",
      "from transformers import WhisperFeatureExtractor\n",
      "from transformers import Seq2SeqTrainingArguments\n",
      "import evaluate\n",
      "from datasets import load_dataset\n",
      "metric = evaluate.load(\"cer\")\n",
      "import os \n",
      "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
      "\n",
      "def remove_special_characters(batch):\n",
      "    batch[\"test_gt\"] = re.sub(chars_to_ignore_regex, '', batch[\"test_gt\"])\n",
      "    batch[\"train_gt\"] = re.sub(chars_to_ignore_regex, '', batch[\"train_gt\"])\n",
      "    return batch\n",
      "\n",
      "def prepare_dataset(batch):\n",
      "    audio_1 = batch[\"test_file\"]\n",
      "    audio_2 = batch[\"picked_file\"]\n",
      "    batch[\"input_features\"] = feature_extractor(np.concatenate((audio_2[\"array\"],audio_1[\"array\"]),axis=0), sampling_rate=audio_1[\"sampling_rate\"]).input_features[0]\n",
      "    batch[\"labels\"] = tokenizer(batch[\"train_gt\"]+'。'+batch[\"test_gt\"]).input_ids\n",
      "    return batch\n",
      "\n",
      "@dataclass\n",
      "class DataCollatorSpeechSeq2SeqWithPadding:\n",
      "    processor: Any\n",
      "    decoder_start_token_id: int\n",
      "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
      "        \n",
      "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
      "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
      "\n",
      "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
      "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
      "\n",
      "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
      "        \n",
      "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
      "            labels = labels[:, 1:]\n",
      "\n",
      "        batch[\"labels\"] = labels\n",
      "\n",
      "        return batch\n",
      "\n",
      "def compute_metrics(pred):\n",
      "    pred_ids = pred.predictions\n",
      "    label_ids = pred.label_ids\n",
      "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
      "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
      "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
      "    cer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
      "    return {\"cer\": cer}\n",
      "          \n",
      "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
      "\n",
      "train_csv = \"./pick_data/tmp.csv\"\n",
      "\n",
      "dbank = load_dataset('csv', data_files={'train':train_csv})\n",
      "dbank = dbank.map(remove_special_characters,keep_in_memory=True)\n",
      "\n",
      "model_path = 'openai/whisper-large-v2'\n",
      "\n",
      "\n",
      "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path,cache_dir='./')\n",
      "tokenizer = WhisperTokenizer.from_pretrained(model_path, language=\"chinese\", task=\"transcribe\",cache_dir='./')\n",
      "processor = WhisperProcessor.from_pretrained(model_path,cache_dir='./')\n",
      "model = WhisperForConditionalGeneration.from_pretrained(model_path,cache_dir='./').cuda()\n",
      "\n",
      "config = LoraConfig(r=8, lora_alpha=64,target_modules=[\"q_proj\", \"v_proj\",\"k_proj\",\"out_proj\"], lora_dropout=0.05, bias=\"lora_only\",init_lora_weights=\"gaussian\")\n",
      "model = get_peft_model(model, config)\n",
      "\n",
      "model.print_trainable_parameters()\n",
      "\n",
      "model.generation_config.language = \"chinese\"\n",
      "model.generation_config.task = \"transcribe\"\n",
      "PREV_TOKEN = 50360\n",
      "START_TRANSCRIPT = 50258\n",
      "ZH_LANGUAGE = 50260\n",
      "TRANSCRIBE = 50359\n",
      "NO_TIMESTAMP = 50363\n",
      "prompt_tokens = [START_TRANSCRIPT,ZH_LANGUAGE,TRANSCRIBE,NO_TIMESTAMP]\n",
      "\n",
      "pdb.set_trace()\n",
      "model.config.forced_decoder_ids = torch.LongTensor(prompt_tokens)\n",
      "\n",
      "dbank = dbank.cast_column(\"test_file\", Audio(sampling_rate=16000))\n",
      "dbank = dbank.cast_column(\"picked_file\", Audio(sampling_rate=16000))\n",
      "\n",
      "dbank = dbank.map(prepare_dataset, remove_columns=dbank.column_names[\"train\"], num_proc=1)\n",
      "\n",
      "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
      "    processor=processor,\n",
      "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
      ")\n",
      "\n",
      "cer_metric = load_metric(\"cer\")\n",
      "\n",
      "model.config.ctc_zero_infinity = True\n",
      "\n",
      "training_args = TrainingArguments(\n",
      "    output_dir=\"./tmo_icl\", \n",
      "    per_device_train_batch_size=2,\n",
      "    gradient_accumulation_steps=1,\n",
      "    learning_rate=3e-5, \n",
      "    warmup_steps=50, \n",
      "    num_train_epochs=15, \n",
      "    save_strategy='epoch',\n",
      "    do_eval=False,\n",
      "    save_total_limit=3,\n",
      "    fp16=True, \n",
      "    per_device_eval_batch_size=2, \n",
      "    report_to=[\"tensorboard\"],\n",
      "    logging_steps=50, \n",
      "    remove_unused_columns=False, \n",
      "    label_names=[\"labels\"], \n",
      ")\n",
      "\n",
      "\n",
      "trainer = Trainer(\n",
      "    model=model,\n",
      "    args=training_args,\n",
      "    compute_metrics=compute_metrics,\n",
      "    data_collator=data_collator,\n",
      "    train_dataset=dbank[\"train\"],\n",
      "    tokenizer=processor.feature_extractor,\n",
      ")\n",
      "\n",
      "trainer.train()Expected behaviorI get the errors as:TypeError: Object of type Tensor is not JSON serializableI wonder how to set the decoder_start_token_id or forced_decoder_ids during training. And I want the training loss is not including the tokens:  . Could you please give me some code examples to show how to set the decoder_start_token_id or forced_decoder_ids ？Thanks\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_498.txt:\n",
      "Title: Implementing the features of the TextStreamer into the pipeline\n",
      "URL: https://github.com/huggingface/transformers/issues/29464\n",
      "Body:\n",
      "Feature requestIt should be possible to format the output of a transformers.pipeline.streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
      "\n",
      "llm = pipeline(\n",
      "    task=\"text-generation\",\n",
      "    model=model,\n",
      "    tokenizer=tokenizer,\n",
      "    return_full_text=True,\n",
      "    generation_config=generation_config,\n",
      "    eos_token_id=tokenizer.eos_token_id,\n",
      "    pad_token_id=tokenizer.eos_token_id,\n",
      "    streamer=streamer\n",
      ")\n",
      "\n",
      "prompt = \"[INST] ... [/INST]\"\n",
      "\n",
      "result = llm(prompt)The problem is that the result variable contains the prompt and the special tokens.MotivationWhen you use a transformers.pipeline, you can use a TextStreamer object to skip the prompt and the special tokens.The problem is that the result is print on the standard output. I haven't found a way to have this result as the output of the pipeline. I tried to apply batch_decode from the tokenizer on the model output, but the parameter skip_special_tokens didn't work.Your contributionN/A\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_67.txt:\n",
      "Title: Failed to load universal_checkpoint with deepspeed integreation\n",
      "URL: https://github.com/huggingface/transformers/issues/33157\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.17Python version: 3.8.18Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA A800 80GB PCIeWho can help?@muellerzrInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionTheUniversal Checkpointingfeature allows loading with different world sizes. However, when using the Hugging FaceTrainer, the loading of the converted universal checkpoint fails.The failure seems to be due toHfTrainerDeepSpeedConfignot correctly handling the\"load_universal_checkpoint\": trueor\"universal_checkpoint\": truearguments in the DeepSpeed configuration. Consequently, theload_universal_checkpointfunction returnsFalse.Related Issues:[BUG] Nouniversal_checkpoint_infoin the Accelerate+Deepspeed Checkpointmicrosoft/DeepSpeed#5430[REQUEST] universal checkpoint for ZeRO - 1,2,3microsoft/DeepSpeed#2921Expected behaviorUniversal checkpoint should be loaded correctly.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_659.txt:\n",
      "Title: (False?) warning about weight_g/weight_v missing on WeightNorm on PyTorch\n",
      "URL: https://github.com/huggingface/transformers/issues/26796\n",
      "Body:\n",
      "System Infotransformersversion: 4.34.0Platform: Linux-5.15.90.2-microsoft-standard-WSL2-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.16.4Safetensors version: 0.3.3Accelerate version: 0.23.0Accelerate config:    not foundPyTorch version (GPU?): 2.2.0.dev20231005 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: NoUsing distributed or parallel set-up in script?: NoWho can help?@sanchit-gandhiInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionSimply running:fromtransformersimportAutoProcessor,HubertModelmodel=HubertModel.from_pretrained(\"facebook/hubert-base-ls960\")Produces the following warning:Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_v', 'encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.What I gather from thePyTorch documentationandupdated codeis that the PyTorch folks decided to migrate theweight_vandweight_gparams of WeightNorm tooriginal0andoriginal1.Initially I thought the model was simply broken by this breaking change in PyTorch, however I was confused since I saw discussions that it should have been fixedby this PR in transformers, as discussed here:#24692So I attached my debugger to_weight_norm_compat_hook, and sure enough it activated and seems to have migrated the state:(during debug)> state_dict[g_key]\n",
      "tensor([[[0.3022, 0.1198, 0.1031, 0.1000, 0.0945, 0.0891, 0.0939, 0.0933, ...(after model load, in Jupyter):> model.encoder.pos_conv_embed.conv.parametrizations.weight.original0\n",
      "Parameter containing:\n",
      "tensor([[[0.3022, 0.1198, 0.1031, 0.1000, 0.0945, 0.0891, 0.0939, 0.0933, ...So I'm pretty sure the warning is a false alarm, but I'm also confused since the migration happensbeforethe warning is traced, so I wanted to check.Expected behaviorNo warning should have appeared.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_117.txt:\n",
      "Title: Add logit scaling sdpa usingFlexAttentionfor Gemma2\n",
      "URL: https://github.com/huggingface/transformers/issues/32877\n",
      "Body:\n",
      "Feature requestWith the recent publish ofFlexAttention, think it makes senses to support it with sdpa path ofGemma2.Leaving it open to the community:implement the pathmake sure integration tests workadd some docRelated to#32865\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_103.txt:\n",
      "Title: Auto model & pipeline for image-text-to-image-text models\n",
      "URL: https://github.com/huggingface/transformers/issues/32926\n",
      "Body:\n",
      "Feature requestThis is a tracker issue for work oninterleavedin-and-out image-text generation.There are now >= 5 open-source models that can dointerleavedimage-text generation--and many more are expected to be released. Thus, it would now be practical & useful for us to (1) add native support for such models and (2) standardize the logic flow of data through processors and pipelines as done in#31911and#32472ModelGithubNotesPRAnolehttps://github.com/GAIR-NLP/anole-#32013Chameleonhttps://github.com/facebookresearch/chameleon-#32013Llava-NeXT-Interleavedhttps://github.com/LLaVA-VL/LLaVA-NeXT--Lumina-mGPThttps://github.com/Alpha-VLLM/Lumina-mGPT--Show-ohttps://github.com/showlab/Show-o--Transfusion-Not open-source (yet, perhaps)-XGen-MMhttps://github.com/salesforce/LAVIS/tree/xgen-mmThe paper & the github repo don't actually demonstrate interleaved image-text generation yet, but they did train the model on such datasets & the model architecture(s) is perfectly suited for it-Initial work for Chameleon & Anole can be found here:#32013for reference.Notes:We explicitly exclude models that canonlydo text-only generation or image-only generation. We also exclude models that can do image-text generation but not in an interleaved manner.As I've demonstrated in my repo,explicitlyimplementing the Finite State Machine (FSM) for switching between text-generation and image-generation modes as done in Chameleon's repo is not necessary. Implicitly implementing the FSM with Logits Processors suffices. Although more work is needed on finding the most efficient implementation.TODOs:Add support for interleaved image-text generation with:Chameleon ->Improve support for image generation with Chameleon & Anole#32013Anole ->Improve support for image generation with Chameleon & Anole#32013Lumina-mGPTShow-oTransfusionXGen-MMAdd auto model for image-text-to-image-text[Optional] Add auto model for image-to-image-text[Optional] Add auto model for text-to-image-textAdd pipeline for image-text-to-image-text[Optional] Add pipeline for image-to-image-text[Optional] Add pipeline for text-to-image-textBenchmark different implementations of Logits Processors & FSMs for switching between text-generation and image-generation modesMotivationTo make benchmarking and evaluating models for interleaved image-to-text tasks sanerTo continue work on Multimodal In-and-Out, Interleaved Structured Generation:https://github.com/leloykun/mmsgYour contributionI've already started work on Chameleon & Anole here:#32013But I'm currently blocked by (1) not having enough time due to other responsibilities and (2) not having enough compute resources.Any help would be appreciated!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_665.txt:\n",
      "Title: Add an option to decide whether to store the checkpoint and rng_state.\n",
      "URL: https://github.com/huggingface/transformers/issues/26706\n",
      "Body:\n",
      "Motivation:Currently, when using the Transformers library in combination with DeepSpeed for training large language models like LLMs, checkpoints (e.g.bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt) are automatically saved along with therng_state, which can lead to significant disk space usage. In scenarios where multiple GPUs are employed for training, this can quickly become a storage bottleneck, especially when shared by a team. Sometimes we just want to keep the bin file (e.g.pytorch_model-00001-of-00002.bin) as it's enough for load again.Feature Request:I propose adding a configurable option to decide whether to store the checkpoint andrng_stateduring training. This will give users the flexibility to choose when to save checkpoints and reduce the disk space required.Proposed Solution:Add a new parameter, such assave_checkpoint_enabled, to the DeepSpeed configuration file. Users can set this parameter toTrueorFalseto control whether checkpoints andrng_stateshould be saved during training.Modify thetrainer.pyscript in the Transformers library to include a condition forself.save_checkpoint_enabledin the_save_checkpointfunction. Here's a code snippet illustrating the change:ifself.is_deepspeed_enabledandself.save_checkpoint_enabled:# Save the checkpointThis change will allow users to save disk space by not storing checkpoints when not needed, and it can help alleviate the storage challenges associated with large-scale language model training.I have already submitted this issue to the DeepSpeed library #microsoft/DeepSpeed#4403 (comment), as this feature may require collaboration between both libraries.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_842.txt:\n",
      "Title: Identifying backend compatibility versions\n",
      "URL: https://github.com/huggingface/transformers/issues/18817\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_846.txt:\n",
      "Title: Support output_scores in XLA TF generate\n",
      "URL: https://github.com/huggingface/transformers/issues/18636\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_885.txt:\n",
      "Title: Initialized DETR backbone weights do not match with actual pretrained weights\n",
      "URL: https://github.com/huggingface/transformers/issues/16190\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_891.txt:\n",
      "Title: add image2text generation\n",
      "URL: https://github.com/huggingface/transformers/issues/15713\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_649.txt:\n",
      "Title: Add JinaBert Model\n",
      "URL: https://github.com/huggingface/transformers/issues/27035\n",
      "Body:\n",
      "Model descriptionJina AI has just released an open source embedding model that can handle 8k sequence on huggingface:https://huggingface.co/jinaai/jina-embeddings-v2-base-enhttps://huggingface.co/jinaai/jina-embeddings-v2-small-enThese models however, currently require thetrust_remote_codeflag as they reference a custom model implementation specified athttps://huggingface.co/jinaai/jina-embedding-v2/tree/main. It should be relatively simple to upstream the model implementation as it is already implemented to work whentrust_remote_codeis passed.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationhttps://huggingface.co/jinaai/jina-embedding-v2/tree/main\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_113.txt:\n",
      "Title: Multi GPU generate with llama shape error\n",
      "URL: https://github.com/huggingface/transformers/issues/32885\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.31Python version: 3.10.12Huggingface_hub version: 0.23.4Safetensors version: 0.4.2Accelerate version: 0.25.0Accelerate config:    not foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: yes, running withcomposeras the distributed launcherUsing GPU in script?: yesGPU type: NVIDIA A100-SXM4-40GBWho can help?@gante@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun llama withsynced_gpus=Trueand an attention mask. This worked fine on transformers 4.40.2 (and 4.41.x), but no longer works. The use of Composer for the dist stuff is just convenience, shouldn't affect anything to swap in a different distributed launcher, etc.importtransformersimporttorchfromcomposer.utilsimportdistdefmain():dist.initialize_dist('gpu')name='meta-llama/Meta-Llama-3-8B-Instruct'tokenizer=transformers.AutoTokenizer.from_pretrained(name)pad_token_id=tokenizer.eos_token_idmodel=transformers.AutoModelForCausalLM.from_pretrained(name)rank=dist.get_global_rank()model.to(f'cuda:{rank}')ifdist.get_global_rank()==0:content='Write one short sentence.'else:content='Write one long paragraph.'messages=[\n",
      "        {'role':'user','content':content,\n",
      "        }\n",
      "    ]tokenized_messages=tokenizer.apply_chat_template(messages,return_tensors='pt')padded_messages=torch.cat(\n",
      "        [torch.LongTensor((4096-20)*[pad_token_id]),tokenized_messages[0],# [seq]],dim=0,\n",
      "    )padded_messages=padded_messages.unsqueeze(0)padded_messages=padded_messages.to(f'cuda:{rank}')attention_mask=~(padded_messages==pad_token_id)attention_mask=attention_mask.to(f'cuda:{rank}')output=model.generate(input_ids=padded_messages,attention_mask=attention_mask,synced_gpus=True,max_new_tokens=200)print(tokenizer.decode(output[0]))if__name__=='__main__':main()This results inTraceback (most recent call last):\n",
      "  File \"/mnt/workdisk/danielking/github/multi-gpu.py\", line 47, in <module>\n",
      "    main()\n",
      "  File \"/mnt/workdisk/danielking/github/multi-gpu.py\", line 42, in main\n",
      "    output = model.generate(input_ids=padded_messages, attention_mask=attention_mask, synced_gpus=True, max_new_tokens=200)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2024, in generate\n",
      "    result = self._sample(\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2982, in _sample\n",
      "    outputs = self(**model_inputs, return_dict=True)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1189, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1001, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 734, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/mnt/workdisk/danielking/miniconda3/envs/foundry-3.10/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 660, in forward\n",
      "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "RuntimeError: The expanded size of the tensor (4105) must match the existing size (4104) at non-singleton dimension 3.  Target sizes: [1, 32, 1, 4105].  Tensor sizes: [1, 1, 1, 4104]Expected behaviorMulti GPU generate does not error..\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_675.txt:\n",
      "Title: How to run Trainer + DeepSpeed + Zero3 + PEFT\n",
      "URL: https://github.com/huggingface/transformers/issues/26412\n",
      "Body:\n",
      "System Infotransformersversion: 4.34.0.dev0Platform: Linux-5.14.0-284.25.1.el9_2.x86_64-x86_64-with-glibc2.34Python version: 3.11.4Huggingface_hub version: 0.16.4Safetensors version: 0.3.3Accelerate version: 0.24.0.dev0Accelerate config:    not foundPyTorch version (GPU?): 2.0.1+cu117 (True)Who can help?@ArthurZuckerand@younesbelkadaand@pacman100and@muellerzrInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThis scriptis a modification of the official run_clm script. The only additions are the BNB config and PEFT. Yet, I cannot get it to work with adeepspeed zero3 config.Requirements to install:accelerate >= 0.12.0\n",
      "torch >= 1.3\n",
      "datasets >= 1.8.0\n",
      "sentencepiece != 0.1.92\n",
      "protobuf\n",
      "evaluate\n",
      "scikit-learn\n",
      "trl\n",
      "peft\n",
      "bitsandbytesIn the past I have had issues with low_cpu_mem_usage but neither a true/false value seem to get this to work:Command 1:deepspeed --include=\"localhost:0,1\"run_clm.py \\\n",
      "   --model_name_or_path facebook/opt-125m\\\n",
      "  --dataset_name wikitext\\\n",
      "  --dataset_config_name wikitext-2-raw-v1\\\n",
      "  --per_device_train_batch_size 2\\\n",
      "  --per_device_eval_batch_size 2\\\n",
      "  --do_train\\\n",
      "  --do_eval\\\n",
      "  --output_dir /tmp/test-clm\\\n",
      "  --deepspeed deepspeed_configs/ds_config_zero3.json\\\n",
      "  --low_cpu_mem_usagetrue==>ValueError: DeepSpeed Zero-3 is not compatible withlow_cpu_mem_usage=Trueor with passing adevice_map.Command 2:deepspeed --include=\"localhost:0,1\"run_clm.py \\\n",
      "   --model_name_or_path facebook/opt-125m\\\n",
      "  --dataset_name wikitext\\\n",
      "  --dataset_config_name wikitext-2-raw-v1\\\n",
      "  --per_device_train_batch_size 2\\\n",
      "  --per_device_eval_batch_size 2\\\n",
      "  --do_train\\\n",
      "  --do_eval\\\n",
      "  --output_dir /tmp/test-clm\\\n",
      "  --deepspeed deepspeed_configs/ds_config_zero3.json\\\n",
      "  --low_cpu_mem_usagefalse==>ValueError: weight is on the meta device, we need avalueto put in on 0.Expected behaviorAny option to make this combination of Trainer + DeepSpeed + Zero3 + PEFT work.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_63.txt:\n",
      "Title: Supporting Padding in llava processor\n",
      "URL: https://github.com/huggingface/transformers/issues/33175\n",
      "Body:\n",
      "Feature requesthttps://github.com/haotian-liu/LLaVAuses padding for pre-processing the images by default. Current transformers implementation does not support that.MotivationRequest per@NielsRoggeat (https://huggingface.co/llava-hf/llava-1.5-7b-hf/discussions/26#66cf46a5a523b74b5f90fa72).Your contributionI successfully reproduced logits after conversion if we add padding in the Transformers library.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_488.txt:\n",
      "Title: resume_from_checkpoint may still fail with auto_find_batch_size\n",
      "URL: https://github.com/huggingface/transformers/issues/29518\n",
      "Body:\n",
      "System Info(Possibly related to#25956andhttps://stackoverflow.com/questions/77404935/peft-lora-trainer-no-executable-batch-size-found)SummaryI'm running a finetuning script based off ofthis example, except withauto_find_batch_sizeset to True and with fixed train / eval batch sizes removed. I'm seeing the following (sporadic) behavior when usingresume_from_checkpointandauto_find_batch_sizetogether in Sagemaker training jobs:Model completes 1 epochCheckpoint fails to propagate to S3Training restarts from epoch 0 with number of expected steps doubledBehavior continues until we encounter aNo executable batch size founderror and training job fails.Any help is much appreciated!Who can help?@muellerz,@pacman100InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionExample below for Gemma 7B, but has occurred with Flan T5 and UL2 models as well.Requirements Fileaccelerate==0.26.1\n",
      "bitsandbytes==0.42.0\n",
      "transformers==4.38.1\n",
      "datasets==2.16.1\n",
      "peft==0.6.2\n",
      "evaluate==0.4.1\n",
      "nltk==3.8.1\n",
      "diffusers==0.25.1\n",
      "trl==0.7.10\n",
      "flash-attn==2.5.0\n",
      "huggingface_hub==0.20.3\n",
      "aws-encryption-sdk>=3.1.0\n",
      "python-json-logger>=2.0.2\n",
      "pycryptodome>=3.17Calling Script - Relevant SnippetsArgsmodel_output_dir = '/opt/ml/model'\n",
      "output_dir = '/opt/ml/checkpoints'\n",
      "\n",
      "hyperparameters = {\n",
      "    'model_id': base_model_id,  # pre-trained model\n",
      "    'dataset_id': dataset_id,  # dataset id\n",
      "    'train_dataset_path': input_paths['train'],  # path to training dataset\n",
      "    'eval_dataset_path': input_paths.get('fine_tuning_holdout', input_paths.get('validation')),\n",
      "    'epochs': 2,  # number of training epochs\n",
      "    'learning_rate': 2e-4,  # learning rate used during training\n",
      "    'max_seq_length': max_seq_length,\n",
      "    'quant_type': quant_type,    \n",
      "    'lora_r': 8,\n",
      "    'lora_alpha': 16,\n",
      "    'lora_dropout': 0.1,\n",
      "    'model_output_dir': model_output_dir,\n",
      "    'output_dir': output_dir,\n",
      "}HuggingFace Estimatorhuggingface_estimator = HuggingFace(\n",
      "    entry_point='entry_point.py',  # train script\n",
      "    source_dir=str(pathlib.Path(__file__).parent.absolute() / 'qlora_causal_lm'),\n",
      "    # directory which includes all the files needed for training\n",
      "    instance_type=instance_type,  # instances type used for the training job\n",
      "    instance_count=1,  # the number of instances used for training\n",
      "    base_job_name=job_name,  # the name of the training job\n",
      "    role=AWS_ROLE_MODELING,  # Iam role used in training job to access AWS ressources, e.g. S3\n",
      "    volume_size=300,  # the size of the EBS volume in GB\n",
      "    transformers_version='4.28.1',  # the transformers version used in the training job\n",
      "    pytorch_version='2.0.0',  # the pytorch_version version used in the training job\n",
      "    py_version='py310',  # the python version used in the training job\n",
      "    output_dir=model_output_dir,  # the output path in which the trained model will be saved\n",
      "    metric_definitions=metric_definitions,\n",
      "    hyperparameters=hyperparameters,  # the hyperparameters passed to the training job\n",
      "    environment=env,\n",
      "    use_spot_instances=True,\n",
      "    max_wait=90000,\n",
      "    max_run=86400,\n",
      "    checkpoint_s3_uri=f's3://{SESS.default_bucket()}/checkpoints/{job_name}',\n",
      "    checkpoint_local_path=output_dir,\n",
      ")Entry Point Script - Relevant Snippets# Define LoRA Config\n",
      "peft_config = LoraConfig(\n",
      "    r=args.lora_r,  # the dimension of the low-rank matrices - the higher r the more trainable parameters\n",
      "    lora_alpha=args.lora_alpha,  # The alpha parameter for Lora scaling.\n",
      "    lora_dropout=args.lora_dropout,  # The dropout probability for Lora layers.\n",
      "    target_modules=target_modules,\n",
      "    bias=\"none\",  # Bias type for Lora. Can be 'none', 'all' or 'lora_only'\n",
      "    task_type=TaskType.CAUSAL_LM\n",
      ")\n",
      "\n",
      "# prepare model for training\n",
      "model = prepare_model_for_kbit_training(model)\n",
      "model = get_peft_model(model, peft_config)\n",
      "\n",
      "model.print_trainable_parameters()\n",
      "\n",
      "train_args = TrainingArguments(\n",
      "    output_dir=args.output_dir,\n",
      "    auto_find_batch_size=True,\n",
      "    bf16=args.bf16,  # Use BF16 if available\n",
      "    learning_rate=args.learning_rate,\n",
      "    num_train_epochs=args.epochs,\n",
      "    gradient_checkpointing=args.gradient_checkpointing,\n",
      "    logging_dir=f\"{args.output_data_dir}/logs\",\n",
      "    logging_strategy=\"steps\",\n",
      "    logging_steps=50,\n",
      "    save_strategy=\"epoch\",\n",
      "    evaluation_strategy=\"epoch\",\n",
      "    load_best_model_at_end=True,\n",
      "    push_to_hub=False,\n",
      ")trainer = SFTTrainer(\n",
      "    model=model,\n",
      "    tokenizer=tokenizer,\n",
      "    train_dataset=train_dataset,\n",
      "    eval_dataset=eval_dataset,\n",
      "    peft_config=peft_config,\n",
      "    max_seq_length=args.max_seq_length,\n",
      "    formatting_func=prompt_formatter_with_decryption,\n",
      "    data_collator=collator,\n",
      "    args=train_args,\n",
      ")\n",
      "\n",
      "# train model\n",
      "if get_last_checkpoint(args.output_dir) is not None:\n",
      "    logger.info(\"***** Continuing training from last checkpoint *****\")\n",
      "    last_checkpoint = get_last_checkpoint(args.output_dir)\n",
      "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
      "else:\n",
      "    logger.info(\"***** Training new model *****\")\n",
      "    trainer.train()Expected behaviorCheckpoint should propagate to S3 after each epochTraining should continue after epoch 1, rather than starting overNumber of expected steps should not continue to double indefinitely\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_77.txt:\n",
      "Title: ValueError: No columns in the dataset match the model's forward method signature when using SFTTrainer and DataParallel.\n",
      "URL: https://github.com/huggingface/transformers/issues/33119\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: Linux-5.10.135-x86_64-with-glibc2.31Python version: 3.10.14Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0+cu121 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: parallelWho can help?@muellerzr@SunMarc@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionMODEL=\"google/gemma-2-2b-it\"model=AutoModelForCausalLM.from_pretrained(MODEL,torch_dtype=\"auto\",device_map=\"auto\")tokenizer=AutoTokenizer.from_pretrained(MODEL)# if we do not use DataParallel, everything works fine.# If DataParallel is used, ValueError: No columns in the dataset match the model's forward method signature.iftorch.cuda.device_count()>1:model=torch.nn.DataParallel(model)print(model)alpaca_prompt=\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.### Instruction:{}### Input:{}### Response:{}\"\"\"EOS_TOKEN=tokenizer.eos_token# Must add EOS_TOKENdefformatting_prompts_func(examples):instructions=examples[\"instruction\"]inputs=examples[\"input\"]outputs=examples[\"output\"]texts=[]forinstruction,input,outputinzip(instructions,inputs,outputs):text=alpaca_prompt.format(instruction,input,output)+EOS_TOKENtexts.append(text)return{\"text\":texts,\n",
      "    }dataset=load_dataset(\"yahma/alpaca-cleaned\",split=\"train\")dataset=dataset.map(formatting_prompts_func,batched=True,num_proc=4)fromtrlimportSFTTrainerfromtransformersimportTrainingArgumentstrain_args=TrainingArguments(per_device_train_batch_size=2,gradient_accumulation_steps=8,# Use num_train_epochs = 1, warmup_ratio for full training runs!# warmup_steps=20,max_steps=10,# num_train_epochs=2,learning_rate=5e-5,logging_steps=1,optim=\"adamw_8bit\",weight_decay=0.01,lr_scheduler_type=\"linear\",seed=3407,\n",
      ")trainer=SFTTrainer(model=model,tokenizer=tokenizer,train_dataset=dataset,dataset_text_field=\"text\",max_seq_length=MAX_SEQ_LENGTH,dataset_num_proc=2,args=train_args,\n",
      ")Expected behaviorExpected: No ValueError: No columns in the dataset match the model's forward method signature. is raised.It seems to me the error occurs since DataParallel wraps the model.However, I wonder the preprocessing logic in SFTTrainer.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_339.txt:\n",
      "Title: GemmaTokenizerFast word_ids() returns only zeros\n",
      "URL: https://github.com/huggingface/transformers/issues/31437\n",
      "Body:\n",
      "System InfoCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.transformersversion: 4.41.2Platform: Linux-5.15.0-86-generic-x86_64-with-glibc2.31Python version: 3.10.13Huggingface_hub version: 0.23.1Safetensors version: 0.4.2Accelerate version: 0.28.0Accelerate config:    not foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: noWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThe methodword_ids()does only return a list of zeros instead of the correct word_ids.sentence = \"I love my cat\"\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"google/Gemma-7b\") #-version a0eac5b\n",
      "encoded = tokenizer(sentence, return_tensors=\"pt\")\n",
      "print(encoded.word_ids())\n",
      "# [None, 0, 0, 0, 0]I tried several variations of configurations stated in the linked issues in#28881, but for Gemma it doesn't change the result. The llama3 tokenizer outputs the correct values with this code.Expected behaviorThe output ofword_idsshould look like[None, 0, 1, 2, 3]\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_305.txt:\n",
      "Title: Cannot find the best model after training\n",
      "URL: https://github.com/huggingface/transformers/issues/31734\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.2Platform: Linux-5.15.0Python version: 3.10.0Huggingface_hub version: 0.22.2Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config: \tnot foundPyTorch version (GPU?): 2.1.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: YesUsing distributed or parallel set-up in script?: One node with 8 A100 40G GPUsWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI am using the SFTtrainer to fully finetune meta-Llama3-8B model. My SFT config and training arguments are as below.from transformers import AutoTokenizer\n",
      "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
      "\n",
      "from tqdm import tqdm\n",
      "import torch\n",
      "import json, re, os, sys\n",
      "import numpy as np\n",
      "from datasets import load_dataset, DatasetDict\n",
      "import ipdb\n",
      "import random\n",
      "from accelerate import Accelerator\n",
      "from torch.utils.data import DataLoader\n",
      "import evaluate\n",
      "from trl import SFTConfig, SFTTrainer\n",
      "\n",
      "dataset = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00000-of-01024.json.gz\")\n",
      "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
      "train_testvalid = dataset[\"train\"].train_test_split(test_size=0.99, seed=42) \n",
      "valid_test = train_testvalid[\"test\"].train_test_split(test_size=0.999, seed=42) \n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "    model_name,\n",
      "    torch_dtype=torch.bfloat16, # float 32\n",
      "    device_map= \"auto\",\n",
      ")\n",
      "\n",
      "model.train()\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "tokenizer.pad_token = tokenizer.eos_token # tokenizer.pad_token == None\n",
      "tokenizer.padding_side = \"left\"\n",
      "\n",
      "dataset = DatasetDict({\n",
      "    'train': train_testvalid['train'],\n",
      "    'validation': valid_test['train'],\n",
      "    'test': valid_test['test']})\n",
      "\n",
      "repository_id = \"llama3-tune\"\n",
      "\n",
      "sft_config = SFTConfig(\n",
      "    dataset_text_field=\"text\",\n",
      "    output_dir=repository_id,\n",
      "    per_device_train_batch_size=16,\n",
      "    per_device_eval_batch_size=16,\n",
      "    max_seq_length=1024,\n",
      "    # fp16_full_eval=True, # Overflows with fp16\n",
      "    learning_rate=1e-4,\n",
      "    num_train_epochs=1,\n",
      "    optim=\"adamw_torch\",\n",
      "    warmup_ratio = 0.1,\n",
      "    # logging & evaluation strategies\n",
      "    logging_dir=f\"{repository_id}/logs\",\n",
      "    logging_strategy=\"steps\",\n",
      "    logging_steps=0.1,\n",
      "    logging_first_step=True,\n",
      "    evaluation_strategy=\"steps\",\n",
      "    save_strategy=\"steps\",\n",
      "    save_steps= 0.1,\n",
      "    save_total_limit=10,\n",
      "    load_best_model_at_end=True,\n",
      "    eval_accumulation_steps=2,\n",
      "    eval_steps=0.1,\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "trainer = SFTTrainer(\n",
      "    model=model,\n",
      "    train_dataset=dataset[\"train\"],\n",
      "    eval_dataset=dataset[\"validation\"],\n",
      "    args=sft_config,\n",
      ")\n",
      "\n",
      "trainer.train()\n",
      "model.save_pretrained(repository_id)\n",
      "tokenizer.save_pretrained(repository_id)Expected behaviorAt the end of the training, I assume it should load the best model and save it in the directory. However, there is always a message pops up saying that \"Could not locate the best model at checkpoint-207/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.I am only using one node for the training. I am not sure if the best model has been saved or loaded or it saved the model after the whole iteration finishes. Is this a bug related to safetensors? Could you please help me figure this out? Thanks!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_463.txt:\n",
      "Title: LLaVAtorch.compileimplementation\n",
      "URL: https://github.com/huggingface/transformers/issues/29891\n",
      "Body:\n",
      "Feature requestAs per#28981, LLaVA is planned to receivetorch.compilesupport. Seeing to the fact that LLaVA is composed of a vision tower and a LLM, both of which can be separately compiled withfullgraph=True(after support has been added, which is not the case for Mistral), it seems much easier to compile both parts separately as well.MotivationThe_merge_input_ids_with_image_featuresfunction that connects the two parts is difficult to compile as PyTorch has yet to add support for many of the functions used that require dynamic input sizes, which are necessary here as the number of input image tokens is subject to change.Your contributionI'd love to try submitting a PR if possible but I'm not sure what the best way to do so is given the current circumstances.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_477.txt:\n",
      "Title: Moving in a folder &push_to_hubfor atrust_remote_code=Truemodel\n",
      "URL: https://github.com/huggingface/transformers/issues/29714\n",
      "Body:\n",
      "Feature requestBonjour !I'm opening an issue following a discussion with Lysandre on Slack.My request is to be able to do..in model repositories on HF (where currently you can only do.). On this point, I don't know if this applies to all template directories or only to customs, which then require atrust_remote_code=Trueto load them.A second request is that when you have a custom model (i.e. loadable viatrust_remote_code=True) and once it's finetuned, that thepush_to_hubfunction pushes all the files needed for the model to function properly, not justconfig.json,configuration.py,model.safetensors,special_tokens_map.json,tokenizer.json,tokenizer_config.jsonandtraining_args.bin.MotivationThe concrete case behind my requests.We recently extended Flash Attention to the T5.So we had to develop a custom implementation and to load our pre-trained models for finetuning, we have to do :from transformers import AutoModel\n",
      "model = AutoModel.from_pretrained(\"CATIE-AQ/FAT5-base-UL2-fr\", trust_remote_code=True)For this to work, we need amodeling file.In our code on GitHub (https://github.com/catie-aq/flashT5/blob/main/src/model/modeling_flash_t5.py), we call up classes that we've put in autilsfolder and import them, for example (line47) a..utils.positional_encoding import ALiBiPositionalEncoding, RelativePositionalEncoding, RotaryPositionalEncoding.On HF, this returned an error saying that there was a..in themodeling_flash_t5.pycode and that it was therefore not possible to retrieve the classes. We therefore had to move all the code contained in theutilsfolder to the root.The line I used as an example above then becomes from.positional_encoding import ALiBiPositionalEncoding, RelativePositionalEncoding, RotaryPositionalEncodingand it works.So being able to use classes contained in files would be appreciated 😄The second request is related to the fact that, once this model has been finetuned, I do apush_to_hubto save the weights.This pushes me the filesconfig.json,configuration_flash_t5.py,model.safetensors,special_tokens_map.json,tokenizer.json,tokenizer_config.jsonandtraining_args.bin.And when I then want to reload the model to do inference, it tells me that the 8 files circled in red in the image above + themodeling_flash_t5.pyfile are missing.So every time I finetune, I have to do a second push where I add these 9 missing files so that my model can load properly.Wouldn't it be possible for these files (which are detected during model loading) to be pushed directly with the 1stpush_to_hub? 🤗Your contributionLet me know if there's any way I can help.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_88.txt:\n",
      "Title: apply_chat_template method not working correctly for llama 3 tokenizer\n",
      "URL: https://github.com/huggingface/transformers/issues/33091\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.1Platform: Linux-4.18.0-553.8.1.el8_10.x86_64-x86_64-with-glibc2.28Python version: 3.10.14Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA A100-SXM4-80GBWho can help?@ArthurZuckerI noticed that the apply_chat_template for the PreTrainedTokenizerBase class does not work correctly when return_assistant_tokens_mask=True. We would expect to get back a list of indices for each example where 1 indicates the token is part of an assistant message and 0 otherwise. This is the case for the Llama 2 tokenizer for example. I am sharing a minimal example to reproduce this issue.Looking deeper into the apply_chat_template method it seems the issue is related to the char_to_token method of the tokenizers.Embedding class and could be related to the fact that the Llama 3 tokenizer was trained with tiktoken as opposed to sentencepiece.InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionfromtransformersimportAutoTokenizerfromdatasetsimportload_datasetdataset_name=\"m-a-p/Code-Feedback\"model_name=\"meta-llama/Meta-Llama-3.1-8B\"# apply_chat_template does not work correctly#model_name = \"meta-llama/Llama-2-7b-hf\" # apply_chat_template works correctlychat_template=\"\"\"{% if messages[0]['role'] == 'system' %}{% set offset = 1 %}{% else %}{% set offset = 0 %}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == offset) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{{ '### ' + message['role'] + ':\\n'}}{% if (message['role'] == 'assistant') %}{% generation %} {{ message['content'] | trim + eos_token }} {% endgeneration %}{% else %}{{ message['content'] | trim + eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '### ' + 'assistant' + ':\\n' }}{% endif %}\"\"\"tokenizer=AutoTokenizer.from_pretrained(model_name)tokenizer.chat_template=chat_templatedatasets=load_dataset(dataset_name,trust_remote_code=True)# assistant_mask is all zeros for llama3 tokenizerchat=tokenizer.apply_chat_template(datasets[\"train\"][0][\"messages\"],add_generation_prompt=False,return_dict=True,tokenize=True,return_assistant_tokens_mask=True)print(\"assistant_masks\",chat[\"assistant_masks\"])Executing the steps to get the assistant mask in the apply chat template method shows that the char_to_token method of the tokenizers. Embedding class seems to be not working correctly.compiled_template=tokenizer._compile_jinja_template(chat_template)template_kwargs={**tokenizer.special_tokens_map}rendered_chat,generation_indices=tokenizer._render_with_assistant_indices(compiled_template=compiled_template,messages=datasets[\"train\"][0][\"messages\"],tools=[],documents=None,add_generation_prompt=False,**tokenizer.special_tokens_map)out=tokenizer(rendered_chat,padding=False,truncation=False,max_length=None,add_special_tokens=False,return_tensors=None)first_assistant_start_char,first_assistant_end_char=generation_indices[0]# returns None for llama3print(\"char_to_token\",out[0].char_to_token(0,first_assistant_start_char))Expected behaviorIf we assume that the entire chat is 10 characters and the assistant tokens occur at indices 4-6 and 8-9 we would have an expected output that looks like this[0, 0, 0, 1, 1, 1, 0, 1, 1, 0]The actual output for the llama 3 tokenizer is always all 0s[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_259.txt:\n",
      "Title: Checkpoint validation as an option\n",
      "URL: https://github.com/huggingface/transformers/issues/32067\n",
      "Body:\n",
      "Feature requestAddedvalidate_checkpoint_key: boolinfrom_pretrainedMotivationWhen pytorch loads state dict there is an optionstrict=Truebut there is no counterpart infrom_pretrained. If people saved the pretrained model incorrectly, the incorrect keys can silently pass through.Your contributionThere is already an validation step in thefrom_pretrainedone just need to raise the error ifvalidate_checkpoint_key=True\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_271.txt:\n",
      "Title: last_hidden_statehas a different shape thanhidden_states[-1]in the output ofSeamlessM4Tv2SpeechEncoderif adapter layers are present\n",
      "URL: https://github.com/huggingface/transformers/issues/31946\n",
      "Body:\n",
      "System Infotransformersversion: 4.43.0.dev0Platform: Linux-5.4.0-182-generic-x86_64-with-glibc2.17Python version: 3.8.19Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.30.1Accelerate config:    not foundPyTorch version (GPU?): 2.0.1+cu117 (True)Tensorflow version (GPU?): 2.13.1 (True)Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)Jax version: 0.4.13JaxLib version: 0.4.13Using distributed or parallel set-up in script?: noUsing GPU in script?: noGPU type: NVIDIA GeForce RTX 3090Who can help?@sanchit-gandhi@ylacombeInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionCreate an instance ofSeamlessM4Tv2SpeechEncoderwith 1 or more adapter layer(s) having stride > 1, for example by doing:fromtransformersimportAutoModelspeech_encoder=AutoModel.from_pretrained(\"facebook/seamless-m4t-v2-large\").speech_encoderEncode a sample audio and pass it through the speech encoder:importtorchfromtransformersimportAutoProcessoraudio_processor=AutoProcessor.from_pretrained(\"meetween/seamless-m4t-v2-large-speech-encoder\")audio,sr=...# load an audio somehow and make it a torch.Tensorinputs=audio_processor(audios=audio.squeeze().float().cpu(),sampling_rate=sr,return_tensors=\"pt\",\n",
      ")audio_features=model(**inputs,output_attentions=True,output_hidden_states=True)Access the resulting output and notice how the shape oflast_hidden_stateis different than the shape ofhidden_states[-1]:assertaudio_features.last_hidden_state.shape!=audio_features.hidden_states[-1].shapeSimilarly, notice how the shape oflast_hidden_stateis not compatible with the shape ofattentions[-1]:batch_size,seq_len_1,emb_size=audio_features.last_hidden_state.shapebatch_size,num_heads,seq_len_2,seq_len_2=attentions[-1].shapeassertseq_len_1!=seq_len_2Expected behaviorassertaudio_features.last_hidden_state.shape==audio_features.hidden_states[-1].shapeassertseq_len_1==seq_len_2Why this is a problem (in my view)Misleading names:last_hidden_stateis different thanhidden_states[-1]Consider the following use case: a pre-trained instance ofSeamlessM4Tv2SpeechEncoderis used as a speech encoder in a model architecture used for ASR, the full model architecture being speech encoder + custom text decoder. If we train this model with batch size > 1, the speech encoder will be fed padded audio sequences. As a result, when feeding encoded audio sequences (output of the speech encoder) to the custom text decoder, we have to construct a properattention_maskto make sure padded positions are treated as such. Normally, to do this, we would takespeech_encoder_output.attentionsfrom the speech encoder output, convert them to anattention_maskby looking at which elements are > 0 (i.e. which positions in the sequence have an attention weight > 0), then apply the obtainedattention_masktospeech_encoder_output.last_hidden_states.However, this cannot be done since, as mentioned above,seq_len_1 != seq_len_2Because of 2), the only way to applyattention_masktospeech_encoder_output.last_hidden_statesis to manually figure out the correct shape ofattention_maskby considering how many convolutional layers are present inspeech_encoder.adapter(instance ofSeamlessM4Tv2ConformerAdapter) and what theirpadding,dilation,kernel_sizeandstrideparameters are, then compute the output length (seq_len_1) as a function of the input length (seq_len_2) as:len_out=math.floor((len_in+2*padding-dilation*(kernel_size-1)-1)/stride+1)Proposed workaroundInstead of doing this at the end ofSeamlessM4Tv2SpeechEncoder.forward():returnWav2Vec2BaseModelOutput(last_hidden_state=hidden_states,hidden_states=encoder_outputs.hidden_states,attentions=encoder_outputs.attentions,\n",
      ")do something like:returnSomeNewTypeOfModelOutput(last_hidden_state=hidden_states,hidden_states=encoder_outputs.hidden_states,attentions=encoder_outputs.attentions,last_adapter_state=...,adapter_states=...,adapter_attentions=..,\n",
      ")\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_503.txt:\n",
      "Title: ProcessorMixin doesn't properly instantiate image processors\n",
      "URL: https://github.com/huggingface/transformers/issues/29414\n",
      "Body:\n",
      "System InfoTransformers dev version v4.38.2Who can help?@ArthurZucker@amyerobertsReproductionLet's say you define your processor as follows:fromtransformers.processing_utilsimportProcessorMixinclassLlavaProcessor(ProcessorMixin):attributes=[\"image_processor\",\"tokenizer\"]image_processor_class=(\"CLIPImageProcessor\",\"ViTImageProcessor\")tokenizer_class=(\"LlamaTokenizer\",\"LlamaTokenizerFast\")(this is mainly for demo purposes, since for PR#29012I'd like to haveLlavaProcessorwork with 2 different image processor classes)Then, even though you create a processor as follows:fromtransformersimportViTImageProcessor,LlavaProcessor,LlamaTokenizerimage_processor=ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")tokenizer=LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")processor=LlavaProcessor(image_processor=image_processor,tokenizer=tokenizer)processor.push_to_hub(\"nielsr/my-awesome-processor\")Reloading it:fromtransformersimportLlavaProcessorprocessor=LlavaProcessor.from_pretrained(\"nielsr/my-awesome-processor\")print(type(processor.image_processor))This is still going to be of typeCLIPImageProcessor, even though we want to loadViTImageProcessor.This is because of the way we decide which class to loadhere. Namely, if one defines a tuple for theimage_processor_classattribute of the processor, then always the first class is used.Expected behaviorThe processor should reload theViTImageProcessorinstead ofCLIPImageProcessor.The current workaround is to do this:fromtransformers.processing_utilsimportProcessorMixinclassLlavaProcessor(ProcessorMixin):attributes=[\"image_processor\",\"tokenizer\"]image_processor_class=\"AutoImageProcessor\"tokenizer_class=(\"LlamaTokenizer\",\"LlamaTokenizerFast\")This correctly instantiates the image processor. However, in PR#29012,@amyerobertssuggestedthat the use of the Auto class is discouraged and it might be more appropriate to define the specific classes.I remember from the past that Sylvain had no problem regarding the use of the Auto class, but I'm up for discussion. It's definitely a bit inconsistent that we define explicit classes for the tokenizers, but not for the image processor.Looking at it, I  think the Auto class serve the exact purpose of what we're trying to achieve: loading the proper image processor class based on the preprocessor_config.json. Hence I'm wondering whether we shouldn't be leveraging the Auto classes by default for theimage_processor_classandtokenizer_classattributes of multimodal processors.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_265.txt:\n",
      "Title: Arrow files too large when training SegFormer\n",
      "URL: https://github.com/huggingface/transformers/issues/32010\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.4Platform: Windows-11-10.0.22631-SP0Python version: 3.12.4Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: Fine_tune_SegFormer_on_custom_dataset.ipynbGPU type: NVIDIA GeForce RTX 4090Who can help?@amyerobertsInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionMake a custon dataset to train SegFormerI'm using a dataset around 33000 images, 26000 for training 7000 for validationWhen training started, the transformer will continuously writing cache files (with speed around 50mb/s) to C:/Users//.cache/huggingface/metrics/mean_io_u/defaultWhich name is default_experiment-1-0.arrowWhen it reaches 50 epochs, the file size is about 707GB.I don't know why.Expected behaviorDisk space drainage, the training can't be continued.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_932.txt:\n",
      "Title: GotONNXRuntimeErrorwhen try to run BART in ONNX format\n",
      "URL: https://github.com/huggingface/transformers/issues/12851\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_926.txt:\n",
      "Title: Advice needed: Adding more FSMT models\n",
      "URL: https://github.com/huggingface/transformers/issues/13160\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_729.txt:\n",
      "Title: Request support for RWKV-4-World model.\n",
      "URL: https://github.com/huggingface/transformers/issues/24842\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_715.txt:\n",
      "Title: Support H100 training with FP8 in Trainer and Deepspeed\n",
      "URL: https://github.com/huggingface/transformers/issues/25333\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_714.txt:\n",
      "Title: Add Flax diverse group search\n",
      "URL: https://github.com/huggingface/transformers/issues/25355\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_927.txt:\n",
      "Title: Support OpenNMT models\n",
      "URL: https://github.com/huggingface/transformers/issues/13147\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_933.txt:\n",
      "Title: Model Request: Blenderbot 2.0\n",
      "URL: https://github.com/huggingface/transformers/issues/12807\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_502.txt:\n",
      "Title: The results of run_mae.py pre-training were abnormal\n",
      "URL: https://github.com/huggingface/transformers/issues/29416\n",
      "Body:\n",
      "System Infotransformersversion: 4.39.0.dev0Platform: Linux-5.10.0-60.18.0.50.oe2203.x86_64-x86_64-with-glibc2.34Python version: 3.9.18Huggingface_hub version: 0.21.3Safetensors version: 0.4.2Accelerate version: 0.27.2Accelerate config:    not foundPyTorch version (GPU?): 2.0.1+cu117 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@NielsRogge.InformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionViT_MAE_visualization_demo:https://github.com/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynbrun_mae.py:https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretrainingThis is the result of loading the pre-trained modelfacebook/vit-mae-hugedirectly from huggingface.This is the result of my continued pre_training usingfacebook/vit-mae-hugeI don't know why it's getting worse，The loss is decreasing from loss=1.7 at the beginning to loss=0.13.Expected behaviora\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_264.txt:\n",
      "Title: Embedding size 0 when using TrainingArguments & Deepspeed stage 3 withmodel.get_input_embedding()\n",
      "URL: https://github.com/huggingface/transformers/issues/32021\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.4Platform: Linux-5.4.0-181-generic-x86_64-with-glibc2.31Python version: 3.11.9Huggingface_hub version: 0.23.5Safetensors version: 0.4.3Accelerate version: 0.32.1PyTorch version (GPU): 2.3.0+cu121 (True)GPU type: Tesla V100-SXM2-16GBWho can help?@muellerzr@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionMy python scripttrain_temp.py:import torch\n",
      "import transformers\n",
      "from transformers import AutoModelForCausalLM\n",
      "from dataclasses import dataclass, field\n",
      "from typing import Optional\n",
      "\n",
      "@dataclass\n",
      "class TrainingArguments(transformers.TrainingArguments):\n",
      "    cache_dir: Optional[str] = field(default=None)\n",
      "\n",
      "parser = transformers.HfArgumentParser(TrainingArguments)\n",
      "training_args = parser.parse_args_into_dataclasses()\n",
      "\n",
      "# Load model and tokenizer\n",
      "config = transformers.AutoConfig.from_pretrained(\n",
      "    \"path/to/Qwen2\",\n",
      ")\n",
      "\n",
      "llm_model = AutoModelForCausalLM.from_pretrained(\n",
      "    \"path/to/Qwen2\",\n",
      "    config=config,\n",
      ")\n",
      "\n",
      "pretrained_embed = llm_model.get_input_embeddings()\n",
      "print(pretrained_embed)                # Embedding(152064, 3584)\n",
      "print(pretrained_embed.weight.shape)   # torch.Size([0])\n",
      "\n",
      "out = pretrained_embed(torch.ones((1, 1024), dtype=torch.int))\n",
      "print(out.shape)My running script:DISTRIBUTED_ARGS=\"\n",
      "    --nproc_per_node 4 \\\n",
      "    --nnodes 1 \\\n",
      "    --node_rank 0 \\\n",
      "    --master_addr localhost \\\n",
      "    --master_port 6001\n",
      "\"\n",
      "\n",
      "DS_CONFIG_PATH=\"ds_config_zero3.json\"\n",
      "\n",
      "torchrun $DISTRIBUTED_ARGS train_temp.py \\\n",
      "    --output_dir output_20240712_1 \\\n",
      "    --deepspeed ${DS_CONFIG_PATH}Myds_config_zero3.json:{\n",
      "    \"fp16\": {\n",
      "        \"enabled\": \"auto\",\n",
      "        \"loss_scale\": 0,\n",
      "        \"loss_scale_window\": 1000,\n",
      "        \"initial_scale_power\": 16,\n",
      "        \"hysteresis\": 2,\n",
      "        \"min_loss_scale\": 1\n",
      "    },\n",
      "    \"bf16\": {\n",
      "        \"enabled\": \"auto\"\n",
      "    },\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\",\n",
      "        \"params\": {\n",
      "            \"lr\": \"auto\",\n",
      "            \"betas\": \"auto\",\n",
      "            \"eps\": \"auto\",\n",
      "            \"weight_decay\": \"auto\"\n",
      "        }\n",
      "    },\n",
      "\n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\",\n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": \"auto\",\n",
      "            \"warmup_max_lr\": \"auto\",\n",
      "            \"warmup_num_steps\": \"auto\"\n",
      "        }\n",
      "    },\n",
      "\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3,\n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\",\n",
      "            \"pin_memory\": true\n",
      "        },\n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\",\n",
      "            \"pin_memory\": true\n",
      "        },\n",
      "        \"overlap_comm\": true,\n",
      "        \"contiguous_gradients\": true,\n",
      "        \"sub_group_size\": 1e9,\n",
      "        \"reduce_bucket_size\": \"auto\",\n",
      "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
      "        \"stage3_param_persistence_threshold\": \"auto\",\n",
      "        \"stage3_max_live_parameters\": 1e9,\n",
      "        \"stage3_max_reuse_distance\": 1e9,\n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    },\n",
      "\n",
      "    \"gradient_accumulation_steps\": \"auto\",\n",
      "    \"gradient_clipping\": \"auto\",\n",
      "    \"steps_per_print\": 100,\n",
      "    \"train_batch_size\": \"auto\",\n",
      "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
      "    \"wall_clock_breakdown\": false\n",
      "}Error:[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/mnt9/fangrui/qwen_bert/train_temp.py\", line 28, in <module>\n",
      "[rank3]:     out = pretrained_embed(torch.ones((1, 1024), dtype=torch.int))\n",
      "[rank3]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/home/fr450273/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "[rank3]:     return self._call_impl(*args, **kwargs)\n",
      "[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/home/fr450273/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "[rank3]:     return forward_call(*args, **kwargs)\n",
      "[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/home/fr450273/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/nn/modules/sparse.py\", line 163, in forward\n",
      "[rank3]:     return F.embedding(\n",
      "[rank3]:            ^^^^^^^^^^^^\n",
      "[rank3]:   File \"/home/fr450273/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/nn/functional.py\", line 2264, in embedding\n",
      "[rank3]:     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]: RuntimeError: 'weight' must be 2-DWhen deleteTrainingArgumentspart, the embedding size return normal.# Delete\n",
      "@dataclass\n",
      "class TrainingArguments(transformers.TrainingArguments):\n",
      "    cache_dir: Optional[str] = field(default=None)\n",
      "\n",
      "parser = transformers.HfArgumentParser(TrainingArguments)\n",
      "training_args = parser.parse_args_into_dataclasses()Expected behaviorprint(pretrained_embed.weight.shape)   # torch.Size([152064, 3584])\n",
      "print(out.shape)    # torch.Size([1, 1024, 3584])\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_516.txt:\n",
      "Title: DeepSpeed Support Stage 3\n",
      "URL: https://github.com/huggingface/transformers/issues/29254\n",
      "Body:\n",
      "System InfoDoes the trainer support stage 3?According tohttps://huggingface.co/transformers/v4.3.0/main_classes/trainer.html- it does not.Thanks,BrettWho can help?naInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionuse trainer with stage 3Expected behaviorParameter partitioning\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_258.txt:\n",
      "Title: BertForSequenceClassification.from_pretrained broken when using FSDP\n",
      "URL: https://github.com/huggingface/transformers/issues/32068\n",
      "Body:\n",
      "System Info-`Accelerate`version: 0.29.2\n",
      "- Platform: Linux-6.5.0-44-generic-x86_64-with-glibc2.35\n",
      "-`accelerate`bash location: /home/oskar/projects/robust-llm/venv/bin/accelerate\n",
      "- Python version: 3.10.12\n",
      "- Numpy version: 1.26.4\n",
      "- PyTorch version (GPU?): 2.2.2+cu121 (True)\n",
      "- PyTorch XPU available: False\n",
      "- PyTorch NPU available: False\n",
      "- PyTorch MLU available: False\n",
      "- System RAM: 14.85 GB\n",
      "- GPU type: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "-`Accelerate`default config:\n",
      "        Not foundInformationThe official example scriptsMy own modified scriptsTasksOne of the scripts in the examples/ folder of Accelerate or an officially supportedno_trainerscript in theexamplesfolder of thetransformersrepo (such asrun_no_trainer_glue.py)My own task or dataset (give details below)ReproductionThe following code successfully loads the model checkpoint if ran usingpython foo.pyoraccelerate launch foo.pybut not with FSDP enabledaccelerate launch --use_fsdp foo.py.This seems like a bug where we say inPreTrainedModel.from_pretrainedthatpretrained_model_name_or_pathcan be None \"if you are both providing the configuration and state dictionary\", which I do here. But then ifis_fsdp_enabled()is True, we setlow_cpu_mem_usage = Trueand thus in turnstate_dict = None, which causes the loading to fail.import torch\n",
      "from transformers import BertForSequenceClassification\n",
      "from accelerate import Accelerator\n",
      "\n",
      "checkpoint_path = \"https://github.com/unitaryai/detoxify/releases/download/v0.1-alpha/toxic_original-c1212f89.ckpt\"\n",
      "model_type = \"bert-base-uncased\"\n",
      "num_classes = 6\n",
      "\n",
      "accelerator = Accelerator()\n",
      "loaded = torch.hub.load_state_dict_from_url(\n",
      "    checkpoint_path, map_location=accelerator.device\n",
      ")\n",
      "state_dict = loaded[\"state_dict\"]\n",
      "config = BertForSequenceClassification.config_class.from_pretrained(\n",
      "    model_type, num_labels=num_classes\n",
      ")\n",
      "model = BertForSequenceClassification.from_pretrained(\n",
      "    pretrained_model_name_or_path=None,\n",
      "    config=config,\n",
      "    state_dict=state_dict,\n",
      "    local_files_only=False,\n",
      ")\n",
      "print(type(model))Error message:model = BertForSequenceClassification.from_pretrained(\n",
      "  File \"/home/oskar/projects/robust-llm/venv/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3754, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "  File \"/home/oskar/projects/robust-llm/venv/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4194, in _load_pretrained_model\n",
      "    state_dict = load_state_dict(shard_file, is_quantized=is_quantized)\n",
      "  File \"/home/oskar/projects/robust-llm/venv/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 506, in load_state_dict\n",
      "    if checkpoint_file.endswith(\".safetensors\") and is_safetensors_available():\n",
      "AttributeError: 'NoneType' object has no attribute 'endswith'Expected behaviorShould output<class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_476.txt:\n",
      "Title: [Mamba] Possible Issue in beam search with Mamba HF models\n",
      "URL: https://github.com/huggingface/transformers/issues/29730\n",
      "Body:\n",
      "System Infonightly build transformers versionWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "device = \"cuda:0\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
      "model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\").to(device)\n",
      "\n",
      "input_prompts = [\"Question: Who is the lead singer of Coldplay? Answer:\",]\n",
      "prompted_encoded = tokenizer(input_prompts[0:1], return_tensors=\"pt\", padding=True).to(device)['input_ids']\n",
      "\n",
      "with torch.inference_mode():\n",
      "        generated_tokens = model.generate(input_ids=prompted_encoded, max_length=60, use_cache=True, num_beams=5)\n",
      "        decoded_tokens = tokenizer.batch_decode(generated_tokens)\n",
      "        print(\"Generation finished.\")\n",
      "        print(decoded_tokens)Expected behaviorWe are observing a degradation of the mamba model generated output asnum_beamsparameter goes up.Example with num_beams=1Generation finished.\n",
      "['Question: Who is the lead singer of Coldplay? Answer: The lead singer of Coldplay is the lead singer of the band Coldplay.\\n\\nThe lead singer of Coldplay is the lead singer of the band Coldplay. The lead singer of Coldplay is the lead singer of the band']Example with num_beams=100:Generation finished.\n",
      "['Question: Who is the lead singer of Coldplay? Answer: Coldplay\\n\\n\\n Cold isfield\\n\\n\\nQuestion:\\nColdplayplayfield\\n\\n\\n:\\n\\n:\\n\\n:\\n\\n:\\n\\n:\\n\\n:\\n\\n:\\n\\n:\\n\\n:\\n']\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_310.txt:\n",
      "Title: meta-llama/Llama-2-7b-chat-hf tokenizermodel_max_lengthattribute needs to be fixed.\n",
      "URL: https://github.com/huggingface/transformers/issues/31705\n",
      "Body:\n",
      "System Infotransformers==4.42.3, python3.9.19Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionimporttransformerstokenizer=transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")print(tokenizer.model_max_length)This outputs1000000000000000019884624838656Expected behaviorExpected output should be4096since that is the max sequence length of this specific Llama model.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_304.txt:\n",
      "Title: PassHFQuantizertofrom_pretrainedkwargs\n",
      "URL: https://github.com/huggingface/transformers/issues/31738\n",
      "Body:\n",
      "Feature requestCurrently, when loading a model in quantized form, theHFQuantizeris created based on other kwargs passed into thefrom_pretrainedfunction. See current implementation below:# modeling_utils::from_pretrained()ifpre_quantizedorquantization_configisnotNone:ifpre_quantized:config.quantization_config=AutoHfQuantizer.merge_quantization_configs(config.quantization_config,quantization_config)else:config.quantization_config=quantization_confighf_quantizer=AutoHfQuantizer.from_config(config.quantization_config,pre_quantized=pre_quantized)else:hf_quantizer=NoneThis should be a straightforward addition, by adding the following lines:# modeling_utils::from_pretrained()hf_quantizer=kwargs.pop(\"hf_quantizer\",None)ifhf_quantizerisnotNone:passelifpre_quantizedorquantization_configisnotNone:\n",
      "        ...MotivationThis would give users more flexibility, and allow one to easily create and integrate custom implementations of theHFQuantizerclass. I am personally working on a project where this change is necessary to work with quantization methods that have not yet been added to the libraryYour contributionI can make a PR and contribution\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_462.txt:\n",
      "Title: Change Device Allocation and Improve Shape Handling in Trainer evaluation loop\n",
      "URL: https://github.com/huggingface/transformers/issues/29908\n",
      "Body:\n",
      "Feature requestContextCurrently when setting thecompute_metricsparameter as non-null value it sets theprediction_loss_onlyparam asFalse.Asprediction_loss_onlyisFalse,self.prediction_stepwill return non-null value for thelogitsvariable.This variable will be used here to bepadded, thenconcatenatedto previously stored logits.What bothers meDevice AllocationIf the logits are tensors, a new tensor will be created using the same device as logits' one (cf). Meaning that iflogitsis oncuda:0, the resulting tensor will also be oncuda:0. This unnecessarily consumes VRAM as after the tensor will be moved back to CPU (cf) without further operations.IMO the resulting tensor should be on CPU from the beginning to avoid consuming VRAM unnecessarily. But let me know if I miss something :)Handling of ShapesIt seems that the concatenation is only done acrosslogits(cf), not taking into account the shape of previously registered logits inpreds_host. Meaning that at each iteration, thelogitsshould have the same max length otherwise it will raise the following error :RuntimeError: Sizes of tensors must match except in dimension 0. Expected size [SHAPE] but got size [ANOTHER SHAPE] for tensor number 1 in the list.This causes issue forAnyToSeqmodels (i.eSeqToSeq) where the length of the outputs sequences can vary from one iteration to another. The current workaround would be to pad the output to a fix max length (i.e by padding the input itself to a fix max length) however this greatly increase the necessary compute, thus making the training process slower.A fix could be to directly cast the logits to numpy with a non-constant shapes across logits.@muellerzrI would be happy to have your input on that oneMotivationFor more context, I encountered those behaviors when working onthat issue.Fixing both behaviors would :Remove the spikes in VRAM usage generated when usingcompute_metricsAvoid having to pad the model's input unnecessarily thus reducing the compute required and making the training faster.Your contributionIt's kinda of a blocker on my side, so I would be happy to work on a PR :)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_338.txt:\n",
      "Title: Memory leak when using CLIPTextModel\n",
      "URL: https://github.com/huggingface/transformers/issues/31439\n",
      "Body:\n",
      "System Infotransformersversion: 4.26.1Platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.35Python version: 3.11.5Huggingface_hub version: 0.17.3PyTorch version (GPU?): 2.1.2 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: yesUsing distributed or parallel set-up in script?: yesWho can help?@ArthurZucker@younesbelkadaInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI can't free GPU memory after I use CLIPTextModelAlso, memory is allocated in another device for some reasonproblem should be reproduced by using the following code snippetfromtransformersimportCLIPTextModelimporttorchclip_text_model=CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(\"cuda:1\")delclip_text_modeltorch.cuda.empty_cache()Expected behaviorCLIPTextModel is placed in \"cuda:1\", but for some reason, memory gets allocated in \"cuda:0\" when I call torch.cuda.empty_cache()Also, memory is still not freed for \"cuda:1\"As a result, both \"cuda:0\" and \"cuda:1\" have some memory allocatedI've also tried using garbage collection and explicitly moving model to cpu, but they don't work.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_62.txt:\n",
      "Title: static cache: RuntimeError: cannot mutate tensors with frozen storage\n",
      "URL: https://github.com/huggingface/transformers/issues/33178\n",
      "Body:\n",
      "With:pytorch/pytorch@aa31e70(~2.5.0a0)huggingface/accelerate@3fcc9465c1027bOn:CPUNVidia A10Test that \"static cache works withtorch.export()\" fails with:# RUN_SLOW=1 python3 -m pytest --pspec -vv -k CacheTest tests/utils/test_cache_utils.py\n",
      "\n",
      "RuntimeError: cannot mutate tensors with frozen storage\n",
      "\n",
      "While executing %index_copy_ : [num_users=0] = call_method[target=index_copy_](args = (%k_out, 2, %l_input_pos_, %k_embed), kwargs = {})\n",
      "Original traceback:\n",
      "  File \"/home/dvrogozh/git/huggingface/transformers/tests/utils/test_cache_utils.py\", line 210, in forward\n",
      "    outs = self.model(\n",
      "  File \"/home/dvrogozh/git/huggingface/transformers/src/transformers/models/gemma/modeling_gemma.py\", line 1076, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/home/dvrogozh/git/huggingface/transformers/src/transformers/models/gemma/modeling_gemma.py\", line 889, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/home/dvrogozh/git/huggingface/transformers/src/transformers/models/gemma/modeling_gemma.py\", line 611, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/home/dvrogozh/git/huggingface/transformers/src/transformers/models/gemma/modeling_gemma.py\", line 521, in forward\n",
      "    key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "  File \"/home/dvrogozh/git/huggingface/transformers/src/transformers/cache_utils.py\", line 1101, in update\n",
      "    k_out.index_copy_(2, cache_position, key_states)I observe that adding a.clone()to the following 2 tensors does fix the issue. Such solution was suggested inpytorch/pytorch#127571 (comment). However I am not sure whether that's the correct fix. See#33178draft PR with this change.transformers/src/transformers/cache_utils.pyLines 1090 to 1091\n",
      "      in5c1027bk_out=self.key_cache[layer_idx]v_out=self.value_cache[layer_idx]CC:@gante@SunMarc\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_112.txt:\n",
      "Title: transformers - installation\n",
      "URL: https://github.com/huggingface/transformers/issues/32886\n",
      "Body:\n",
      "Hello,Going via the install and I see some issueshttps://huggingface.co/docs/transformers/installationA)PLATFORM WINpip install 'transformers[torch]'ERROR: Invalid requirement: \"'transformers[torch]'\": Expected package name at the start of dependency specifier'transformers[torch]'pip install 'transformers[tf-cpu]'ERROR: Invalid requirement: \"'transformers[tf-cpu]'\": Expected package name at the start of dependency specifier'transformers[tf-cpu]'pip install 'transformers[flax]'ERROR: Invalid requirement: \"'transformers[flax]'\": Expected package name at the start of dependency specifier'transformers[flax]'CORRECT SHOULD BEpip install \"transformers[torch]\"pip install \"transformers[tf-cpu]\"pip install \"transformers[flax]\"B)pip install \"transformers[tf-cpu]\" DID NOT WORKFailed to build tokenizersERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)https://huggingface.co/learn/nlp-course/en/chapter8/5transformers-cli envtransformersversion: 4.44.0Platform: Windows-10-10.0.22631-SP0Python version: 3.11.9Huggingface_hub version: 0.24.5Safetensors version: 0.4.4Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.4.0+cu118 (True)Tensorflow version (GPU?): 2.17.0 (False)Flax version (CPU?/GPU?/TPU?): 0.7.0 (cpu)Jax version: 0.4.13JaxLib version: 0.4.13Using distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_890.txt:\n",
      "Title: Add TUNet\n",
      "URL: https://github.com/huggingface/transformers/issues/15763\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_884.txt:\n",
      "Title: Gpt2 large for onnx exportation and int8 quantization\n",
      "URL: https://github.com/huggingface/transformers/issues/16195\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_853.txt:\n",
      "Title: Test summary with previous PyTorch/TensorFlow versions\n",
      "URL: https://github.com/huggingface/transformers/issues/18181\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_845.txt:\n",
      "Title: Add TF VideoMAE\n",
      "URL: https://github.com/huggingface/transformers/issues/18641\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_110.txt:\n",
      "Title: Optionalbiasfor qwen2 model\n",
      "URL: https://github.com/huggingface/transformers/issues/32892\n",
      "Body:\n",
      "Feature requestbiasof linear layers inqwen2model is hard coded as following:transformers/src/transformers/models/qwen2/modeling_qwen2.pyLines 217 to 219\n",
      "      in85345bbself.gate_proj=nn.Linear(self.hidden_size,self.intermediate_size,bias=False)self.up_proj=nn.Linear(self.hidden_size,self.intermediate_size,bias=False)self.down_proj=nn.Linear(self.intermediate_size,self.hidden_size,bias=False)transformers/src/transformers/models/qwen2/modeling_qwen2.pyLines 271 to 274\n",
      "      in85345bbself.q_proj=nn.Linear(self.hidden_size,self.num_heads*self.head_dim,bias=True)self.k_proj=nn.Linear(self.hidden_size,self.num_key_value_heads*self.head_dim,bias=True)self.v_proj=nn.Linear(self.hidden_size,self.num_key_value_heads*self.head_dim,bias=True)self.o_proj=nn.Linear(self.num_heads*self.head_dim,self.hidden_size,bias=False)It would be good to make bias optionally configurable through a config file to ensure compatibility with the latest models. (e.g. llama)Motivationbiasis optional in llama model as following:transformers/src/transformers/models/llama/modeling_llama.pyLines 286 to 288\n",
      "      in85345bbself.gate_proj=nn.Linear(self.hidden_size,self.intermediate_size,bias=config.mlp_bias)self.up_proj=nn.Linear(self.hidden_size,self.intermediate_size,bias=config.mlp_bias)self.down_proj=nn.Linear(self.intermediate_size,self.hidden_size,bias=config.mlp_bias)Your contributionI'll submit PR for this feature\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_104.txt:\n",
      "Title: AttributeError: 'T5Stack' object has no attribute 'layer' when using t5_model.prune_heads(heads_to_prune)\n",
      "URL: https://github.com/huggingface/transformers/issues/32917\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.4Platform: Linux-6.1.85+-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.23.5Safetensors version: 0.4.4Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionheads_to_prune={0: [3,4,5,6,7],1: [0,1,2,3,4,6,7],2: [3,4,5,6,7],3: [1,3,4,5],4: [4],5: [0,1,2,5]}model.prune_heads(heads_to_prune)[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in __getattr__(self, name)1707ifnameinmodules:\n",
      "   1708returnmodules[name]\n",
      "->1709         raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "   1710 \n",
      "   1711     def __setattr__(self, name: str, value: Union[Tensor,'Module']) ->None:\n",
      "\n",
      "AttributeError:'T5Stack'object has no attribute'layer'Expected behaviorExpected model to change with the given heads pruned\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_74.txt:\n",
      "Title: Potential RMSNorm precision issue\n",
      "URL: https://github.com/huggingface/transformers/issues/33133\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.1Platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.35Python version: 3.10.12Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.0a0+f70bd71a48.nv24.06 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: distributedUsing GPU in script?: YESWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionThere's a implementation difference between HF transformers'RMSNormand Nvidiatransformer_engine'sRMSNorm.Version:transformer-engine        1.7.0+4e7caa1First defineHFRMSNormcode, which is copied frommodeling_llamaimplementation fromtransformerslibrary.import torch\n",
      "from torch import nn\n",
      "\n",
      "class HFRMSNorm(nn.Module):\n",
      "    def __init__(self, hidden_size, eps=1e-6, config=None):\n",
      "        super().__init__()\n",
      "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
      "        self.variance_epsilon = eps\n",
      "\n",
      "    def forward(self, hidden_states):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return self.weight * hidden_states.to(input_dtype)Next, run the test code:import unittest\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from transformer_engine.pytorch.module.rmsnorm import RMSNorm as TELayerNorm\n",
      "from copy_from_hf import HFRMSNorm\n",
      "\n",
      "class TestLayerNormComparison(unittest.TestCase):\n",
      "    def setUp(self):\n",
      "        self.hidden_size = 4096\n",
      "        self.batch_size = 1\n",
      "        self.seq_length = 1024\n",
      "        self.eps = 1e-5\n",
      "\n",
      "        self.shared_weight = nn.Parameter(torch.randn(self.hidden_size, dtype=torch.bfloat16))\n",
      "\n",
      "        self.te_layernorm = TELayerNorm(self.hidden_size, eps=self.eps, zero_centered_gamma=False).to(torch.bfloat16)\n",
      "        self.hf_rmsnorm = HFRMSNorm(self.hidden_size, eps=self.eps).to(torch.bfloat16)\n",
      "\n",
      "        self.te_layernorm.weight = self.shared_weight\n",
      "        self.hf_rmsnorm.weight = self.shared_weight\n",
      "\n",
      "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "        self.te_layernorm.to(self.device)\n",
      "        self.hf_rmsnorm.to(self.device)\n",
      "\n",
      "    def test_layernorm_comparison(self):\n",
      "        input_tensor = torch.randn(self.batch_size, self.seq_length, self.hidden_size,\n",
      "                                   dtype=torch.bfloat16, device=self.device)\n",
      "\n",
      "        with torch.no_grad():\n",
      "            te_output = self.te_layernorm(input_tensor)\n",
      "            hf_output = self.hf_rmsnorm(input_tensor)\n",
      "\n",
      "        assert torch.allclose(te_output, hf_output, atol=1e-2)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()The assertion will fail.Expected behaviorIf we change the last line ofHFRMSNormfromreturn self.weight * hidden_states.to(input_dtype)to  return(self.weight.to(torch.float32) * hidden_states).to(input_dtype), the assertion should pass.We have a discussionhere, and I agree that we should  all internal computation in FP32. So what's your opinion on HF side?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_60.txt:\n",
      "Title: Doc bug: wrong token argument name for Tokenizer.from_pretrained()\n",
      "URL: https://github.com/huggingface/transformers/issues/33183\n",
      "Body:\n",
      "System InfoN/A for doc bug.Who can help?@stevhliuInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionhttps://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.from_pretrainedfrom_pretrained( identifierrevision = 'main'auth_token = None ) →TokenizerParametersidentifier (str) — The identifier of a Model on the Hugging Face Hub, that contains a tokenizer.json filerevision (str, defaults to main) — A branch or commit idauth_token (str, optional, defaults to None) — An optional auth token used to access private repositories on the Hugging Face HubExpected behavior'auth_token' is incorrect.'use_auth_token' works, but with the following warning:/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/modeling_utils.py:3220: FutureWarning: Theuse_auth_tokenargument is deprecated and will be removed in v5 of Transformers. Please usetokeninstead.'token' also works.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_48.txt:\n",
      "Title: Questions about supporting KV Cache quantization for models that do not support quantized cache now\n",
      "URL: https://github.com/huggingface/transformers/issues/33231\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Linux-3.10.0-1160.el7.x86_64-x86_64-with-glibc2.17Python version: 3.10.14Huggingface_hub version: 0.24.3Safetensors version: 0.4.3Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: NoUsing GPU in script?: YesGPU type: NVIDIA H800Who can help?@ArthurZucker@ganteInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers.cache_utils import QuantoQuantizedCache, QuantizedCacheConfig\n",
      "\n",
      "BS = 1024\n",
      "@torch.no_grad()\n",
      "def gen(model, input_ids, max_new_tokens, eos_token_id):\n",
      "    past_key_values = QuantoQuantizedCache(QuantizedCacheConfig(nbits=2, compute_dtype=torch.bfloat16))\n",
      "    for b in range(0, input_ids.shape[-1], BS):\n",
      "        e = min(input_ids.shape[-1], b + BS)\n",
      "        output = model(input_ids[:, b:e], past_key_values=past_key_values)\n",
      "        past_key_values = output.past_key_values\n",
      "\n",
      "    generated_tokens = []\n",
      "    input_id = output.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
      "    generated_tokens.append(input_id.item())\n",
      "\n",
      "    for _ in range(max_new_tokens-1):\n",
      "        output = model(input_id, past_key_values=past_key_values)\n",
      "        past_key_values = output.past_key_values\n",
      "        input_id = output.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
      "        if input_id.item() == eos_token_id:\n",
      "            break\n",
      "        generated_tokens.append(input_id.item())\n",
      "    generated_tokens = torch.tensor(generated_tokens, device=input_ids.device, dtype=input_ids.dtype).unsqueeze(0)\n",
      "    input_ids = torch.cat((input_ids, generated_tokens), dim=-1)\n",
      "    return input_idsExpected behaviorThe code snippet provided above generates random output for phi-3-mini-128K, which is a model that does not originally support KV Cache quantization.However, from my understanding of the quantized cache supported in Hugging Face Transformers, one can simple replace an instance ofDynamicCachetoQuantoQuantizedCacheto enable KV Cache quantization. This is also mentioned in#30483 (comment). Phi-3-mini-128K is a quite-standard decoder-only transformer-based model with only a few modifications on model structure compared with Llama, thus I believe that if the quantized cache can work correctly (which it does) on Llama, it can work correctly on Phi-3. The code snippet can generate high quality output on Llama, but it generates random tokens on Phi-3.Besides, could you provide a readme to teach model contributors to enable KV Cache quantization?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_448.txt:\n",
      "Title: Image Processor fails to process void segmentation maps\n",
      "URL: https://github.com/huggingface/transformers/issues/30064\n",
      "Body:\n",
      "System Infotransformersversion: 4.34.0Platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.31Python version: 3.10.13Huggingface_hub version: 0.21.4Safetensors version: 0.4.0PyTorch version (GPU?): 2.1.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import Mask2FormerImageProcessor\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "ignore_index = 255\n",
      "image_processor = Mask2FormerImageProcessor(ignore_index=ignore_index,\n",
      "                                                         do_resize=False,\n",
      "                                                         do_rescale=False,\n",
      "                                                         do_normalize=False)\n",
      "\n",
      "image_norm = np.random.rand(4, 4, 3)\n",
      "\n",
      "# Create void mask (all pixels have ignore_index)\n",
      "semantic_mask = np.ones(image_norm.shape[:2], dtype=np.uint8)*255\n",
      "\n",
      "semantic_mask = semantic_mask.astype(np.uint8)\n",
      "print(semantic_mask)\n",
      "\n",
      "inputs = image_processor(\n",
      "    image_norm,\n",
      "    segmentation_maps=semantic_mask,\n",
      "    return_tensors='pt',\n",
      ")\n",
      "\n",
      "print(inputs)===========================================================[[255 255 255 255][255 255 255 255][255 255 255 255][255 255 255 255]]Traceback (most recent call last):File \"/home/anba/catkin_ws/src/tas_dev/dev/anba/Mask2Former/test.py\", line 21, ininputs = image_processor(File \"/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py\", line 566, incallreturn self.preprocess(images, segmentation_maps=segmentation_maps, **kwargs)File \"/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py\", line 764, in preprocessencoded_inputs = self.encode_inputs(File \"/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py\", line 943, in encode_inputsmasks, classes = self.convert_segmentation_map_to_binary_masks(File \"/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py\", line 558, in convert_segmentation_map_to_binary_masksreturn convert_segmentation_map_to_binary_masks(File \"/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/transformers/models/mask2former/image_processing_mask2former.py\", line 284, in convert_segmentation_map_to_binary_masksbinary_masks = np.stack(binary_masks, axis=0)  # (num_labels, height, width)File \"<array_functioninternals>\", line 180, in stackFile \"/home/anba/anaconda3/envs/SAM/lib/python3.10/site-packages/numpy/core/shape_base.py\", line 422, in stackraise ValueError('need at least one array to stack')ValueError: need at least one array to stackProcess finished with exit code 1Expected behaviorIf this is intended that void masks should never be passed, then the result is fine.However, when training segmentation models, shouldn't it be possible to include images with only background/void class?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_312.txt:\n",
      "Title: Whisper - list index out of range with word level timestamps\n",
      "URL: https://github.com/huggingface/transformers/issues/31683\n",
      "Body:\n",
      "System Infotransformersversion: 4.42.2Platform: Windows-10-10.0.22621-SP0Python version: 3.10.14Huggingface_hub version: 0.23.4Safetensors version: 0.4.2Accelerate version: 0.31.0Accelerate config: \tnot foundPyTorch version (GPU?): 2.3.1 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?: NoUsing GPU in script?: YesGPU type: NVIDIA GeForce RTX 4070 Laptop GPUWho can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionLoad 'whisper-large-v3' AutoModelForSpeechSeq2Seq model and send it to GPUSetup model config with return_timestamps=\"word\", amongst other settings.Run audio through the pipe, which results in error---------------------------------------------------------------------------\n",
      "IndexError                                Traceback (most recent call last)\n",
      "Cell In[8], [line 1](vscode-notebook-cell:?execution_count=8&line=1)\n",
      "----> [1](vscode-notebook-cell:?execution_count=8&line=1) asr_out = transcribing_pipe(\n",
      "      [2](vscode-notebook-cell:?execution_count=8&line=2)     '../SampleData/Saba_interview_short.wav',\n",
      "      [3](vscode-notebook-cell:?execution_count=8&line=3)     return_timestamps=\"word\",\n",
      "      [4](vscode-notebook-cell:?execution_count=8&line=4)     generate_kwargs={\"language\": \"danish\"}\n",
      "      [5](vscode-notebook-cell:?execution_count=8&line=5)     )\n",
      "      [7](vscode-notebook-cell:?execution_count=8&line=7) asr_out\n",
      "\n",
      "File c:\\Users\\User\\miniconda3\\envs\\vva\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:284, in AutomaticSpeechRecognitionPipeline.__call__(self, inputs, **kwargs)\n",
      "    [221](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:221) def __call__(\n",
      "    [222](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:222)     self,\n",
      "    [223](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:223)     inputs: Union[np.ndarray, bytes, str],\n",
      "    [224](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:224)     **kwargs,\n",
      "    [225](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:225) ):\n",
      "    [226](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:226)     \"\"\"\n",
      "    [227](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:227)     Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\n",
      "    [228](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:228)     documentation for more information.\n",
      "   (...)\n",
      "    [282](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:282)                 `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\n",
      "    [283](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:283)     \"\"\"\n",
      "--> [284](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:284)     return super().__call__(inputs, **kwargs)\n",
      "\n",
      "File c:\\Users\\User\\miniconda3\\envs\\vva\\lib\\site-packages\\transformers\\pipelines\\base.py:1246, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n",
      "   [1244](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1244)     return self.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "   [1245](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1245) elif self.framework == \"pt\" and isinstance(self, ChunkPipeline):\n",
      "-> [1246](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1246)     return next(\n",
      "   [1247](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1247)         iter(\n",
      "   [1248](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1248)             self.get_iterator(\n",
      "   [1249](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1249)                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n",
      "   [1250](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1250)             )\n",
      "   [1251](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1251)         )\n",
      "   [1252](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1252)     )\n",
      "   [1253](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1253) else:\n",
      "   [1254](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/base.py:1254)     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\n",
      "File c:\\Users\\User\\miniconda3\\envs\\vva\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125, in PipelineIterator.__next__(self)\n",
      "    [123](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/pt_utils.py:123) # We're out of items within a batch\n",
      "    [124](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/pt_utils.py:124) item = next(self.iterator)\n",
      "--> [125](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/pt_utils.py:125) processed = self.infer(item, **self.params)\n",
      "    [126](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/pt_utils.py:126) # We now have a batch of \"inferred things\".\n",
      "    [127](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/pt_utils.py:127) if self.loader_batch_size is not None:\n",
      "    [128](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/pt_utils.py:128)     # Try to infer the size of the batch\n",
      "\n",
      "File c:\\Users\\User\\miniconda3\\envs\\vva\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:587, in AutomaticSpeechRecognitionPipeline.postprocess(self, model_outputs, decoder_kwargs, return_timestamps, return_language)\n",
      "    [584](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:584)             stride_right /= sampling_rate\n",
      "    [585](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:585)             output[\"stride\"] = chunk_len, stride_left, stride_right\n",
      "--> [587](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:587)     text, optional = self.tokenizer._decode_asr(\n",
      "    [588](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:588)         model_outputs,\n",
      "    [589](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:589)         return_timestamps=return_timestamps,\n",
      "    [590](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:590)         return_language=return_language,\n",
      "    [591](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:591)         time_precision=time_precision,\n",
      "    [592](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:592)     )\n",
      "    [593](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:593) else:\n",
      "    [594](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/pipelines/automatic_speech_recognition.py:594)     items = np.concatenate(final_items, axis=1)\n",
      "\n",
      "File c:\\Users\\User\\miniconda3\\envs\\vva\\lib\\site-packages\\transformers\\models\\whisper\\tokenization_whisper.py:832, in WhisperTokenizer._decode_asr(self, model_outputs, return_timestamps, return_language, time_precision)\n",
      "    [831](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:831) def _decode_asr(self, model_outputs, *, return_timestamps, return_language, time_precision):\n",
      "--> [832](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:832)     return _decode_asr(\n",
      "    [833](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:833)         self,\n",
      "    [834](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:834)         model_outputs,\n",
      "    [835](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:835)         return_timestamps=return_timestamps,\n",
      "    [836](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:836)         return_language=return_language,\n",
      "    [837](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:837)         time_precision=time_precision,\n",
      "    [838](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:838)     )\n",
      "\n",
      "File c:\\Users\\User\\miniconda3\\envs\\vva\\lib\\site-packages\\transformers\\models\\whisper\\tokenization_whisper.py:1032, in _decode_asr(tokenizer, model_outputs, return_timestamps, return_language, time_precision)\n",
      "   [1030](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:1030) current_tokens.append(token)\n",
      "   [1031](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:1031) if return_timestamps == \"word\":\n",
      "-> [1032](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:1032)     start_time = round(token_timestamps[i] + time_offset, 2)\n",
      "   [1033](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:1033)     if i + 1 < len(token_timestamps):\n",
      "   [1034](file:///C:/Users/User/miniconda3/envs/vva/lib/site-packages/transformers/models/whisper/tokenization_whisper.py:1034)         end_time = round(token_timestamps[i + 1] + time_offset, 2)\n",
      "\n",
      "IndexError: list index out of rangeI've uploaded the audio that I am trying to processhereI have a discussion on the whisper's hub page, where I have implemented a fix that seems to work.hereColab linkhereExpected behaviorLoad 'whisper-large-v3' AutoModelForSpeechSeq2Seq model and send it to GPUSetup model config with return_timestamps=\"word\", amongst other settings.Run audio through the pipe, which returns transcription together with the word level timestamps segment.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_474.txt:\n",
      "Title: Add option to not re-load the model when resuming from checkpoint.\n",
      "URL: https://github.com/huggingface/transformers/issues/29740\n",
      "Body:\n",
      "Feature requestI'd like to have the option to get the trainer to resume_from_checkpointwithoutreloading the model.I propose adding a boolean option parameter:without_checkpoint_model: bool = False.Starting from a checkpoint with this parameter set to True would require that the model was supplied to the Trainer constructor (pretty standard practice anyway) and would simply skip this line of code in the Trainer:self._load_from_checkpoint(resume_from_checkpoint)@muellerzr,@pacman100,@amyerobertsMotivationWhen Trainer.train(resume_from_checkpoint=my_checkpoint) is called, the Trainer will attempt to re-load the model in the my_checkpoint directory during the resumption process.This is kind of wasteful but no big tragedy if the model in question in fully supported by Trainer.Right now, Trainer needs to know how to reload a model before it can be used with resume_from_checkpoint.This was (and may be still) a big problem for QLoRA model users who had to spend quite some time waiting for Trainer to be modified to specifically support re-loading QLoRA models.  One could train these models from scratch just fine by loading them first and supplying them to the Trainer constructor, it was just impossible to resume_from_checkpoint with them.It looks like there is supposed support for resuming with QLoRA added very recently but then some folks complaining that it does not actually work in practice:How can we resume training from lora model?#29607Unable to resume LoRA training with PEFT#29383But even if QLoRA reloading does work today or will soon, there is a wider problem.In general, whenever there is a new model type that the Trainer does not explicitly know how to load, it will be impossible to resume_from_checkpoint for that kind of model until support is added to Trainer for loading that type of model.Lots of new kinds of quantization and LoRA initialization methods are being invented all the time (e.g LoftQ).  Do we really want to prevent their adoption because it takes time to adapt Trainer to intrinsically know how to load every kind of model under the sun?We might do well to just provide folks a way to work around that without having to fork transformers and then use their patched fork (like I've been doing).Really, since the Trainer can be instantiated with the fully restored checkpoint, there is no reason why the train() call should force re-loading the model from the checkpoint.Your contributionI've already got a POC implementation in my fork which I've pretty thoroughly integration tested, having resumed  QLoRA training from checkpoint with it dozens of times.stevemadere@5c2a1c8PR incoming.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_460.txt:\n",
      "Title: SigLIP tokenizer not enforcing use_fast=True\n",
      "URL: https://github.com/huggingface/transformers/issues/29925\n",
      "Body:\n",
      "System Infotransformersversion: 4.38.2Platform: Linux-4.18.0-477.27.1.el8_8.x86_64-x86_64-with-glibc2.28Python version: 3.10.13Huggingface_hub version: 0.21.4Safetensors version: 0.4.2Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.2.1+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoTokenizer\n",
      "t=AutoTokenizer.from_pretrained('google/siglip-so400m-patch14-384', use_fast=True)\n",
      "assert t.is_fast, 'tokenizer is not fast'\n",
      "print('Success')Expected behaviorprint 'Success' which indicatesuse_fast=True\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_306.txt:\n",
      "Title: Inconsistent special_token addition in EncoderDecoderModel forward pass\n",
      "URL: https://github.com/huggingface/transformers/issues/31729\n",
      "Body:\n",
      "System Infotransformersversion: 4.40.0Platform: Linux-4.18.0-513.11.1.el8_9.x86_64-x86_64-with-glibc2.28Python version: 3.11.9Huggingface_hub version: 0.22.2Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config:    not foundPyTorch version (GPU?): 2.2.2+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?: YesUsing distributed or parallel set-up in script?: NoWho can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionAdd a breakpoint at:transformers/src/transformers/models/encoder_decoder/modeling_encoder_decoder.pyLine 625\n",
      "      ine655029decoder_outputs=self.decoder(Run the example script of EncoderDecoderModeltransformers/src/transformers/models/encoder_decoder/modeling_encoder_decoder.pyLines 562 to 577\n",
      "      ine655029>>> from transformers import EncoderDecoderModel, BertTokenizer>>> import torch>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")>>> model = EncoderDecoderModel.from_encoder_decoder_pretrained(...     \"google-bert/bert-base-uncased\", \"google-bert/bert-base-uncased\"... )  # initialize Bert2Bert from pre-trained checkpoints>>> # training>>> model.config.decoder_start_token_id = tokenizer.cls_token_id>>> model.config.pad_token_id = tokenizer.pad_token_id>>> model.config.vocab_size = model.config.decoder.vocab_size>>> input_ids = tokenizer(\"This is a really long text\", return_tensors=\"pt\").input_ids>>> labels = tokenizer(\"This is the corresponding summary\", return_tensors=\"pt\").input_ids>>> outputs = model(input_ids=input_ids, labels=labels)Examine the content ofdecoder_input_ids. You will see that the bos_token has been added twice.Expected behaviorThe bos_token should only be prepended once to the input to the decoder.The reason for this bug occurring is thatBertTokenizeralready wraps the text in bos_token and eos_token. Then, during theEncoderDecoderModelforward pass (specifically during construction of thedecoder_input_idsfrom thelabels), another bos_token is added.This means that in order to use EncoderDecoderModel correctly, one has to tokenize the decoder inputs in a way such that an eos_token is added at the end of the text, but a bos_token is NOT added at the beginning. This is funky and clearly not wanted behavior.The decoder part of an EncoderDecoderModel is rarely initialized with an encoder-only Transformer. Since the tokenizer of decoder-only Transformers do not add bos_tokens nor eos_tokens, I think the correct way to change the behavior of the EncoderDecoderModel would be to add both the eos_token as well as the bos_token in the model forward pass. To clear up confusion, we should change the official example to constructing an EncoderDecoderModel from an encoder-only Transformer and a decoder-only Transformer.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_299.txt:\n",
      "Title: Saving Phi 3 vision fails due to tensor sharing\n",
      "URL: https://github.com/huggingface/transformers/issues/32354\n",
      "Body:\n",
      "Hello and thank you for the great work here!We are trying to save a Phi 3 vision mode, but are running into some issues saving it as safetensors.Due to a shared weight, saving unfortunately fails when using safetensors. I was wondering if there is a solution to de-tie the weight? I attempted to de-tie manually by copying the tensor, but that did not work (perhaps I did it incorrectly, as there is another reference?).Minimum reproducible examplefromtransformersimportAutoModelForCausalLMmodel_id=\"lamm-mit/Cephalo-Phi-3-vision-128k-4b-alpha\"model=AutoModelForCausalLM.from_pretrained(model_id,device_map=\"cuda\",trust_remote_code=True,torch_dtype=\"auto\")model.save_pretrained(\"out\",safe_serialization=True)\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_528.txt:\n",
      "Title: Swappingtqdmtorich\n",
      "URL: https://github.com/huggingface/transformers/issues/29064\n",
      "Body:\n",
      "Feature requestHi, forAutoTokenizer.train_new_from_iteratorthere's a hardcodedtqdmprogress bar I want to swap torichand I'm happy to PR it back.I can see on github it's attransformers/src/transformers/tokenization_utils_fast.pyand I can see in lines#790and#791that there's a further methodtrain_from_iteratorbut at this point I can't find where the actual code is? Can anyone point me to the right direction?Also, is there any reason to go against addingrichas a dependency?Where are thetqdmspecific bits of code, so I can go through them?Thanks!MotivationI'm not fond oftqdmit seems to create issues when used on AWS, SageMaker, etc. It's span is large and doesn't contain nearly enough information asrichcan. I wanna start by going overAutoTokenizerbecause it's where I first spotted it.Your contributionSlowly work through bits of code which rely ontqdmand add the option to swap forrichinstead.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_266.txt:\n",
      "Title: Add support for Apple's DCLM-Baseline-7B model\n",
      "URL: https://github.com/huggingface/transformers/issues/32000\n",
      "Body:\n",
      "Model descriptionThis is a new model released by Apple using a new framework called \"openlm\" so it doesn't work with Huggingface Transformers currently.Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationhttps://huggingface.co/apple/DCLM-Baseline-7BLink to the model weights.https://github.com/mlfoundations/open_lmLink to the model framework.https://github.com/mlfoundations/dclmhttps://arxiv.org/abs/2406.11794Link to the paper.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_925.txt:\n",
      "Title: \"Resource exhausted\" when loading Flax GPT-Neo 2.7B\n",
      "URL: https://github.com/huggingface/transformers/issues/13219\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_931.txt:\n",
      "Title: BatchFeature should cast tonp.float32by default\n",
      "URL: https://github.com/huggingface/transformers/issues/12862\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_515.txt:\n",
      "Title: AutoModel.from_config() should attempt to load model relative to where the config was loaded.\n",
      "URL: https://github.com/huggingface/transformers/issues/29259\n",
      "Body:\n",
      "System InfoCurrently if you do an AutoConfig.from_pretrained() and then use that config to try to AutoModel.from_config() it loses the context from where the config.json was loaded.  This wouldn't be a problem if AutoModel.from_config() also took a path_or_model_id style param but it doesn't.This becomes problematic mainly for situations where you want to write single code for both training and fine tuning models.  In this workflow, the notion of training and fine tuning is largely the same code so creating the model to then train looks something like this:try:\n",
      "    model = AutoModel.form_pretrained(..., config=conf, ...)\n",
      "except OSError, ValueError:\n",
      "    model = AutoModel.from_config(conf)If you're loading models from file like i am, not being able to easily AutoModel.from_config() with a specified path is problematic.  I would like all of my intermediate training steps to be done locally to potentially unrelated directory structures to the source.I think that we have two easy options.Add a field to the PretrainedConfig which is only populated when it's loaded from file which is the path to the config file the config was loaded from.  We can call it something like _loaded_from_file.  It would have to be removed prior to save()allow AutoModel.from_config() to take a path_or_model_id in addition to the config.  so the call would look something like:model = AutoModel.from_config(conf, path_or_model_id='/path/to/dir/containing/model')cc:@Rocketknight1Trying out recommended workflow of issuing bugs before PRs.Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionLoad a model config specifying custom code in the AutoMap from a path that is not also valid as$CWD/model-id/modeltry to instantiate a version of your model from AutoModel.from_config() using that config.Expected behaviorI'd like to have an instance of the model that's of the same type as the model that'd be returned if from_pretrained() were used.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_501.txt:\n",
      "Title: ReduceLROnPlateau version of get_constant_schedule_with_warmup\n",
      "URL: https://github.com/huggingface/transformers/issues/29417\n",
      "Body:\n",
      "Feature requestThere is a very useful scheduler for people too lazy to code up the sequential scheduler pieces from pytorch:https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#transformers.get_constant_schedule_with_warmupIt would be convenient to have a version of this where the long flat section is actually a ReduceLROnPlateau:https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.htmlMotivationI'm really lazy and would love to have this piece of work done for me, plus there would probably be other people who find it usefulYour contributionCould do, if it's a feature other people would like to see but no one is eager to pick up the gauntlet\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_529.txt:\n",
      "Title: Request for Flash Attention 2.0 Support in GPNRoFormerForMaskedLM\n",
      "URL: https://github.com/huggingface/transformers/issues/29060\n",
      "Body:\n",
      "Hello,I trust this message finds you well. I am currently attempting to run the GPN-MSA model, which utilizes AutoModelForMaskedLM, and I am keen on parallelizing the computation across multiple GPUs. To optimize the model's performance, I would like to request the integration of Flash Attention 2.0 support into GPNRoFormerForMaskedLM.As I explore this avenue for parallelization, I envision that many others within the community could benefit from this enhancement.Thank you for your time and consideration.Best regards.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_461.txt:\n",
      "Title: Support DBRX Model\n",
      "URL: https://github.com/huggingface/transformers/issues/29911\n",
      "Body:\n",
      "Feature requestSupport the DBRX model (only correct pronunciation: DB-Rex)blog post.Code is from the open sourcedatabricks/dbrxrepository.MotivationAcross a range of standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides the open community and enterprises building their own LLMs with capabilities that were previously limited to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.Your contribution#29910\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_307.txt:\n",
      "Title: how to generate router_logits in moe models using model.generate()?\n",
      "URL: https://github.com/huggingface/transformers/issues/31722\n",
      "Body:\n",
      "System Infotransformersversion: 4.41.2Platform: Linux-5.4.0-144-generic-x86_64-with-glibc2.31Python version: 3.10.0Huggingface_hub version: 0.23.4Safetensors version: 0.4.3Accelerate version: 0.31.0Accelerate config:    not foundPyTorch version (GPU?): 2.3.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing GPU in script?:Using distributed or parallel set-up in script?:Who can help?@ganteInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionfrom transformers import AutoModelForCausalLM, AutoTokenizerdevice = \"cuda\" # the device to load the model ontomodel = AutoModelForCausalLM.from_pretrained(\"/localssd/swlu/Qwen1.5-MoE-A2.7B-Chat\",torch_dtype=\"auto\",device_map=\"auto\")tokenizer = AutoTokenizer.from_pretrained(\"/localssd/swlu/Qwen1.5-MoE-A2.7B-Chat\")prompt = \"Give me a short introduction to large language model.\"messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": prompt}]text = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)generated_ids = model.generate(model_inputs.input_ids,max_new_tokens=512,return_dict_in_generate = True,output_router_logits = True)print(\"outputs:\", generated_ids.router_logits)Expected behaviorI want to get router_logits of moe models using model.generate() with the code above.But got:AttributeError: 'GenerateDecoderOnlyOutput' object has no attribute 'router_logits'\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_313.txt:\n",
      "Title: AttributeError: 'str' object has no attribute 'shape'\n",
      "URL: https://github.com/huggingface/transformers/issues/31678\n",
      "Body:\n",
      "System Infotransformers version 4.42.1Who can help?No responseInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionRun chatglm3 generationExpected behaviorraise Error like:def get_masks(self, input_ids, past_key_values, padding_mask=None):\n",
      "        batch_size, seq_length = input_ids.shape\n",
      "        full_attention_mask = torch.ones(batch_size, seq_length, seq_length, device=input_ids.device)\n",
      "        full_attention_mask.tril_()\n",
      "        past_length = 0\n",
      "        if past_key_values:\n",
      ">           past_length = past_key_values[0][0].shape[0]\n",
      "E           AttributeError: 'str' object has no attribute 'shape'\n",
      "\n",
      "../../../.cache/huggingface/modules/transformers_modules/chatglm3-6b/modeling_chatglm.py:683: AttributeError\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_61.txt:\n",
      "Title: Bug in Llama3.1 8B Instruct Tokenizer\n",
      "URL: https://github.com/huggingface/transformers/issues/33182\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.2Platform: Linux-5.15.0-1056-aws-x86_64-with-glibc2.31Python version: 3.10.14Huggingface_hub version: 0.23.3Safetensors version: 0.4.3Accelerate version: 0.33.0Accelerate config:    not foundPyTorch version (GPU?): 2.2.0+cu121 (True)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed or parallel set-up in script?:Using GPU in script?:GPU type: NVIDIA H100 80GB HBM3Who can help?@ArthurZuckerInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductionimport llama tokenizerget token id for 'Ð¾'test decodingtokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct')tokenizer.get_vocab()['Ð¾']above should give you 1482tokenizer.decode(1482)above should give you 'о' (Cyrillic)Expected behaviorSeems like a bug or inconsistency between the tokenizer's stated vocabulary and the actual encoding function. This causes confusion and errors in encoding/decoding.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_75.txt:\n",
      "Title: Extend Fx supported models with KV cache\n",
      "URL: https://github.com/huggingface/transformers/issues/33132\n",
      "Body:\n",
      "Feature requestI noticed only llama and opt models are supported for FX tracing with KV Cache right now, can I check what is the plan to extend it to more models? Thanks!MotivationI would like run fx traced graphmodules for generate(), which uses KV Cache. Right now it works for OPT and LLama, but I would like try on more models.Your contributionIf someone could point me to the general design pattern to make a model FX supported with KV cache or the lines of changes in modeling_opt.py or modeling_llama.py that made them work, I would be happy to submit PRs to make more models work.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_677.txt:\n",
      "Title: Helsinki-NLP/opus-mt-it-enisn't on HuggingFace Hub\n",
      "URL: https://github.com/huggingface/transformers/issues/26382\n",
      "Body:\n",
      "Model descriptionI have found lots of Opus translation model on HuggingFace Hub but couldn't find Portuguese to English model, however the model already exists in Helsinki repohttps://github.com/Helsinki-NLP/OPUS-MT-train/tree/master/models/pt-en#opus-2019-12-05zipis that something can be added quickly?Open source statusThe model implementation is availableThe model weights are availableProvide useful links for the implementationhttps://github.com/Helsinki-NLP/OPUS-MT-train/tree/master/models/pt-en#opus-2019-12-05zip\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_111.txt:\n",
      "Title: [Depth Anything V2] Incorrect model loading for metric depth estimation models\n",
      "URL: https://github.com/huggingface/transformers/issues/32890\n",
      "Body:\n",
      "System Infotransformersversion: 4.44.0Platform: macOS-14.6.1-arm64-arm-64bitPython version: 3.12.3Huggingface_hub version: 0.24.6Safetensors version: 0.4.4Accelerate version: not installedAccelerate config: not foundPyTorch version (GPU?): 2.4.0 (False)Tensorflow version (GPU?): not installed (NA)Flax version (CPU?/GPU?/TPU?): not installed (NA)Jax version: not installedJaxLib version: not installedUsing distributed set-upWho can help?@amyeroberts@NielsRoggeInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)ReproductionI just came back from vacation and noticed some issues raised with the Depth Anything V2 models (for metric depth estimation) on the hub.They seem to happen because when loading any of those models, the output activation function should be a Sigmoid but it is a ReLU instead:# code to replicate the issue\n",
      "from transformers import DepthAnythingForDepthEstimation\n",
      "\n",
      "model = DepthAnythingForDepthEstimation.from_pretrained(\n",
      "    \"depth-anything/depth-anything-V2-metric-indoor-small-hf\"\n",
      ").to(\"cpu\")\n",
      "\n",
      "modelThe model print is the following, and the output logits are incorrect because of the wrong activation function used (and it also looks like the output is not scaled bymax_depthfrom what I observed when playing with a toy example):DepthAnythingForDepthEstimation(\n",
      "  (backbone): Dinov2Backbone(\n",
      "    (embeddings): Dinov2Embeddings(\n",
      "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
      "        (projection): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): Dinov2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x Dinov2Layer(\n",
      "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (attention): Dinov2Attention(\n",
      "            (attention): Dinov2SelfAttention(\n",
      "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): Dinov2SelfOutput(\n",
      "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (layer_scale1): Dinov2LayerScale()\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Dinov2MLP(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          )\n",
      "          (layer_scale2): Dinov2LayerScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (neck): DepthAnythingNeck(\n",
      "    (reassemble_stage): DepthAnythingReassembleStage(\n",
      "      (layers): ModuleList(\n",
      "        (0): DepthAnythingReassembleLayer(\n",
      "          (projection): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resize): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))\n",
      "        )\n",
      "        (1): DepthAnythingReassembleLayer(\n",
      "          (projection): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resize): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (2): DepthAnythingReassembleLayer(\n",
      "          (projection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resize): Identity()\n",
      "        )\n",
      "        (3): DepthAnythingReassembleLayer(\n",
      "          (projection): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resize): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (convs): ModuleList(\n",
      "      (0): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (3): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (fusion_stage): DepthAnythingFeatureFusionStage(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x DepthAnythingFeatureFusionLayer(\n",
      "          (projection): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (residual_layer1): DepthAnythingPreActResidualLayer(\n",
      "            (activation1): ReLU()\n",
      "            (convolution1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation2): ReLU()\n",
      "            (convolution2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (residual_layer2): DepthAnythingPreActResidualLayer(\n",
      "            (activation1): ReLU()\n",
      "            (convolution1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation2): ReLU()\n",
      "            (convolution2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): DepthAnythingDepthEstimationHead(\n",
      "    (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation1): ReLU()\n",
      "    (conv3): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation2): ReLU()\n",
      "  )\n",
      ")Expected behaviorThe output activation function should be a Sigmoid at the end foractivation2and the logits should be scaled bymax_depthcorrectly:DepthAnythingForDepthEstimation(\n",
      "  (backbone): Dinov2Backbone(\n",
      "    (embeddings): Dinov2Embeddings(\n",
      "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
      "        (projection): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): Dinov2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x Dinov2Layer(\n",
      "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (attention): Dinov2Attention(\n",
      "            (attention): Dinov2SelfAttention(\n",
      "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): Dinov2SelfOutput(\n",
      "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (layer_scale1): Dinov2LayerScale()\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Dinov2MLP(\n",
      "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          )\n",
      "          (layer_scale2): Dinov2LayerScale()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (neck): DepthAnythingNeck(\n",
      "    (reassemble_stage): DepthAnythingReassembleStage(\n",
      "      (layers): ModuleList(\n",
      "        (0): DepthAnythingReassembleLayer(\n",
      "          (projection): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resize): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))\n",
      "        )\n",
      "        (1): DepthAnythingReassembleLayer(\n",
      "          (projection): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resize): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "        (2): DepthAnythingReassembleLayer(\n",
      "          (projection): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resize): Identity()\n",
      "        )\n",
      "        (3): DepthAnythingReassembleLayer(\n",
      "          (projection): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (resize): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (convs): ModuleList(\n",
      "      (0): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (3): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (fusion_stage): DepthAnythingFeatureFusionStage(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x DepthAnythingFeatureFusionLayer(\n",
      "          (projection): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (residual_layer1): DepthAnythingPreActResidualLayer(\n",
      "            (activation1): ReLU()\n",
      "            (convolution1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation2): ReLU()\n",
      "            (convolution2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "          (residual_layer2): DepthAnythingPreActResidualLayer(\n",
      "            (activation1): ReLU()\n",
      "            (convolution1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (activation2): ReLU()\n",
      "            (convolution2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): DepthAnythingDepthEstimationHead(\n",
      "    (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activation1): ReLU()\n",
      "    (conv3): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation2): Sigmoid()\n",
      "  )\n",
      ")It looks like the model is incorrectly initialized despite having the expected model config and the attributesmax_depthanddepth_estimation_typecorrectly set. The updatedDepthAnythingDepthEstimationHeadcode inmodeling_depth_anythingdoesn't seem applied for some reason during model initialization - or maybe the issue lies elsewhere.Any thoughts why this is happening@amyeroberts@NielsRogge? Please let me know your thoughts and happy to open a PR to make a fix!\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_139.txt:\n",
      "Title: Clarification on saving model checkpoints\n",
      "URL: https://github.com/huggingface/transformers/issues/32639\n",
      "Body:\n",
      "System Info- `transformers` version: 4.45.0.dev0\n",
      "- Platform: Linux-5.15.0-117-generic-x86_64-with-glibc2.35\n",
      "- Python version: 3.10.12\n",
      "- Huggingface_hub version: 0.24.5\n",
      "- Safetensors version: 0.4.4\n",
      "- Accelerate version: 0.33.0\n",
      "- Accelerate config:    not found\n",
      "- PyTorch version (GPU?): 2.4.0+cu121 (False)\n",
      "- Tensorflow version (GPU?): not installed (NA)\n",
      "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
      "- Jax version: not installed\n",
      "- JaxLib version: not installed\n",
      "- Using distributed or parallel set-up in script?: NoWho can help?@muellerzr@SunMarcInformationThe official example scriptsMy own modified scriptsTasksAn officially supported task in theexamplesfolder (such as GLUE/SQuAD, ...)My own task or dataset (give details below)Reproductioncd examples/pytorch/question-answering/python run_qa.py--model_name_or_path google-bert/bert-base-uncased--dataset_name squad--do_train--do_eval--per_device_train_batch_size 12--learning_rate 3e-5--num_train_epochs 2--max_seq_length 384--doc_stride 128--output_dir /tmp/debug_squad/--max_steps 50--save_steps 5000Expected behaviorNot sure what is expected, I see checkpoint saved twice(also see it onv4.43.3):[INFO|trainer.py:3510] 2024-08-13 00:03:33,083 >> Saving model checkpoint to /tmp/debug_squad/checkpoint-50\n",
      "<snip>\n",
      "<snip>\n",
      "...\n",
      "[INFO|trainer.py:3510] 2024-08-13 00:03:34,323 >> Saving model checkpoint to /tmp/debug_squad/When I go back totransformers v4.40.2I only see one save coming from thetrainer.save_model():[INFO|trainer.py:3305] 2024-08-13 00:21:02,182 >> Saving model checkpoint to /tmp/debug_squad/Suspect the first saving model checkpoint is fromhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py#L656and second is the trainer.save_model():https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py#L657Can you clarify if something changed in the train() to ensure model checkpoint is now saved as part of it?Why did the behavior change and was this intentional?cc:@jiminha, @emascare,@libinta,@regisss\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Contents of issue_844.txt:\n",
      "Title: Refactor Pytorchmodel.generatemethod to work on TPU\n",
      "URL: https://github.com/huggingface/transformers/issues/18661\n",
      "Body:\n",
      "Failed to retrieve issue body.\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ... existing code ...\n",
    "\n",
    "# Directory containing the text files\n",
    "directory = \"transformers/\"\n",
    "\n",
    "# Iterate over each filename in the 'issues' DataFrame\n",
    "for filename in issues['filename']:\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath, 'r') as file:\n",
    "            print(f\"Contents of {filename}:\")\n",
    "            print(file.read())\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    else:\n",
    "        print(f\"File {filename} not found in {directory}\")\n",
    "\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the text files\n",
    "directory = \"transformers/\"\n",
    "\n",
    "# Output file to write the contents\n",
    "output_file = \"output.txt\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, 'w') as out_file:\n",
    "    # Iterate over each filename in the 'issues' DataFrame\n",
    "    for filename in issues['filename']:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r') as file:\n",
    "                out_file.write(f\"Contents of {filename}:\\n\")\n",
    "                out_file.write(file.read())\n",
    "                out_file.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "        else:\n",
    "            out_file.write(f\"File {filename} not found in {directory}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory containing the text files\n",
    "directory = \"transformers/\"\n",
    "\n",
    "# Output file to write the contents\n",
    "output_file = \"output.txt\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, 'w') as out_file:\n",
    "    # Iterate over each filename in the 'issues' DataFrame\n",
    "    for filename in issues['filename']:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r') as file:\n",
    "                out_file.write(f\"Contents of {filename}:\\n\")\n",
    "                out_file.write(file.read())\n",
    "                out_file.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "        else:\n",
    "            out_file.write(f\"File {filename} not found in {directory}\\n\")\n",
    "\n",
    "# Read the contents of the output file\n",
    "with open(output_file, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(625, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1035, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>labels</th>\n",
       "      <th>comments</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weird text encoder NaNs specifically for FSDP ...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>0</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>change sequence_bias type of SequenceBiasLogit...</td>\n",
       "      <td># What does this PR do?\\r\\nIntroduce new seque...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Track progress for VLMs refactoring</td>\n",
       "      <td>\\r\\nThis issue tracks the progress on improvin...</td>\n",
       "      <td>[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Any plans on adding Flash Attention 3?</td>\n",
       "      <td>As title</td>\n",
       "      <td>[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNj...</td>\n",
       "      <td>0</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fix: use unidic-lite instead of ipadic as the ...</td>\n",
       "      <td># What does this PR do?\\r\\n\\r\\n&lt;!--\\r\\nCongrat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>open</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Weird text encoder NaNs specifically for FSDP ...   \n",
       "1  change sequence_bias type of SequenceBiasLogit...   \n",
       "2                Track progress for VLMs refactoring   \n",
       "3             Any plans on adding Flash Attention 3?   \n",
       "4  Fix: use unidic-lite instead of ipadic as the ...   \n",
       "\n",
       "                                                body  \\\n",
       "0  ### System Info\\n\\n- `transformers` version: 4...   \n",
       "1  # What does this PR do?\\r\\nIntroduce new seque...   \n",
       "2  \\r\\nThis issue tracks the progress on improvin...   \n",
       "3                                           As title   \n",
       "4  # What does this PR do?\\r\\n\\r\\n<!--\\r\\nCongrat...   \n",
       "\n",
       "                                              labels  comments state  \n",
       "0  [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         0  open  \n",
       "1                                                 []         1  open  \n",
       "2  [{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...         1  open  \n",
       "3  [{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNj...         0  open  \n",
       "4                                                 []         0  open  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"issues.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>labels</th>\n",
       "      <th>comments</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, body, labels, comments, state]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['state'] == 'closed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "open    1426\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1426, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"issues_classified.csv\")\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>labels</th>\n",
       "      <th>comments</th>\n",
       "      <th>state</th>\n",
       "      <th>is_version_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How to install transformers==4.45, two or thre...</td>\n",
       "      <td>### System Info\\n\\ntorch2.2\\n\\n### Who can hel...</td>\n",
       "      <td>[{'id': 1843765959, 'node_id': 'MDU6TGFiZWwxOD...</td>\n",
       "      <td>2</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>latest 44.4.2 doesn't support falcon_mamba</td>\n",
       "      <td>### System Info\\n\\npython 3.10\\r\\nrocky linux ...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>when l use the model generate method within @t...</td>\n",
       "      <td>### System Info\\n\\ntransformers version: 4.43....</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>5</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>When will Transformers version 4.45.0 be relea...</td>\n",
       "      <td>### System Info\\n\\nI installed this version 4....</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>when l use the model generate method within @t...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>5</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Persistent ImportError with TrainingArguments ...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>2</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Model saving (via `.save_pretrained` or `.push...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>4</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Generation: deprecate `PreTrainedModel` inheri...</td>\n",
       "      <td># What does this PR do?\\r\\n\\r\\nStep 2 of #3268...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Doc bug: wrong token argument name for Tokeniz...</td>\n",
       "      <td>### System Info\\n\\nN/A for doc bug.\\n\\n### Who...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>4</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Failed to import transformers.models.t5.modeli...</td>\n",
       "      <td>### System Info\\n\\nfrom transformers import T5...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>3</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>[Backbone] Remove out_features everywhere</td>\n",
       "      <td># What does this PR do?\\r\\n\\r\\nAs `out_feature...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>XLMRobertaTokenizer attribute has disappeared ...</td>\n",
       "      <td>### System Info\\n\\nOracle Linux 9.4 (Oracle-Li...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>5</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>transformers - installation</td>\n",
       "      <td>\\r\\nHello,\\r\\n\\r\\nGoing via the install and I ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>2</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>confusing deprecation msg for `DynamicCache.se...</td>\n",
       "      <td>The message says `\"The `seen_tokens` attribute...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>class Cache must not be a subclass of `torch.n...</td>\n",
       "      <td>### System Info\\r\\n\\r\\nI'm using `transformers...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>5</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>Docker container with development environment ...</td>\n",
       "      <td>### Feature request\\n\\nA docker container that...</td>\n",
       "      <td>[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNj...</td>\n",
       "      <td>6</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Compile compatibilty for decoder-only models</td>\n",
       "      <td># What does this PR do?\\r\\n\\r\\nRecently we mer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>FutureWarning: `torch._dynamo.external_utils.i...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>The cache for model files in Transformers v4.2...</td>\n",
       "      <td>### System Info\\r\\n\\r\\ngoogle colab with T4 ru...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>2</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>get error when running the chatglm3:  'Generat...</td>\n",
       "      <td>### System Info\\n\\nplatform == ubuntu 22.04\\r\\...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>2</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>Cannot build documentation on Mac OS</td>\n",
       "      <td>### System Info\\r\\n\\r\\n- `transformers` versio...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>3</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Using `numpy==2.0.0`</td>\n",
       "      <td>`transformers`  now remove the pin of `numpy&lt;2...</td>\n",
       "      <td>[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...</td>\n",
       "      <td>2</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>NotImplementedError: ggml_type 3 not implemented</td>\n",
       "      <td>### Description \\r\\nWhen trying to use the mod...</td>\n",
       "      <td>[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNj...</td>\n",
       "      <td>5</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>NumPy 2.0 support</td>\n",
       "      <td>### System Info\\n\\nmacOS 14.5\\ntransformers 4....</td>\n",
       "      <td>[]</td>\n",
       "      <td>10</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>HFQuantizer implementation for compressed-tens...</td>\n",
       "      <td>This PR adds an `HFQuantizer` for the [compres...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>Sink cache: fix generate from cache</td>\n",
       "      <td># What does this PR do?\\r\\n\\r\\nFixes #31381. T...</td>\n",
       "      <td>[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...</td>\n",
       "      <td>3</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>Minimum required accelerate library is not com...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[]</td>\n",
       "      <td>8</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>Galore finetuning #stopped</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMT...</td>\n",
       "      <td>6</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>sdpa for bert casues nan when using bfloat16 w...</td>\n",
       "      <td>### System Info\\r\\n\\r\\n* transformers: transfo...</td>\n",
       "      <td>[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMz...</td>\n",
       "      <td>6</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>RuntimeError: unable to open file when calling...</td>\n",
       "      <td>### System Info\\n\\n```\\r\\n- `transformers` ver...</td>\n",
       "      <td>[]</td>\n",
       "      <td>15</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>Significant performance degradation with multi...</td>\n",
       "      <td>### System Info\\r\\n\\r\\n```Shell\\r\\n# Env 1\\r\\n...</td>\n",
       "      <td>[]</td>\n",
       "      <td>23</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>torchrun breaks with load_model_at_end and wit...</td>\n",
       "      <td>### System Info\\r\\n\\r\\n- `transformers` versio...</td>\n",
       "      <td>[{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMT...</td>\n",
       "      <td>3</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>Image + text + audio uniform processors</td>\n",
       "      <td># What does this PR do?\\r\\n\\r\\nThis PR is a st...</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>Error on TPU: Invalid --2a886c8_slice_builder_...</td>\n",
       "      <td>### System Info\\n\\n```\\r\\ntransformers==4.39.3...</td>\n",
       "      <td>[{'id': 5160774128, 'node_id': 'LA_kwDOCUB6oc8...</td>\n",
       "      <td>3</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>add deepspeed grad ckpt</td>\n",
       "      <td>hi, @younesbelkada , sorry for the late PR.\\r\\...</td>\n",
       "      <td>[{'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNj...</td>\n",
       "      <td>7</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>EncoderDecoderModel with XLM-R</td>\n",
       "      <td>### System Info\\n\\nprivate setup:\\r\\n- `transf...</td>\n",
       "      <td>[]</td>\n",
       "      <td>9</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>SDPA gives nans/infs during sampling on ROCM w...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...</td>\n",
       "      <td>9</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Adherence to semantic versioning with APIs</td>\n",
       "      <td>### Feature request\\r\\n\\r\\nTransformers is an ...</td>\n",
       "      <td>[{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNj...</td>\n",
       "      <td>2</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Transformers Agents Collab Notebook - OpenAI R...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMD...</td>\n",
       "      <td>2</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>`get_imports` failing to respect conditionals ...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMz...</td>\n",
       "      <td>21</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Race condition when loading models from local ...</td>\n",
       "      <td>### System Info\\n\\n- `transformers` version: 4...</td>\n",
       "      <td>[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...</td>\n",
       "      <td>14</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>BERT: TensorFlow Model Garden Conversion scripts</td>\n",
       "      <td>### Feature request\\r\\n\\r\\nHi,\\r\\n\\r\\nafter wo...</td>\n",
       "      <td>[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...</td>\n",
       "      <td>5</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>Pipeline feature request for min_new_tokens</td>\n",
       "      <td>### Feature request\\n\\nPipeline already suppor...</td>\n",
       "      <td>[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...</td>\n",
       "      <td>15</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>Issue Loading 4-bit and 8-bit language models:...</td>\n",
       "      <td>### System Info\\n\\n### System Info\\r\\n\\r\\nI'm ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>29</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>NSP Support for Zero-shot Text Classification ...</td>\n",
       "      <td>### Feature request\\r\\n\\r\\nZero-shot classific...</td>\n",
       "      <td>[{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNz...</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>Multi-node training with Deepspeed hangs when ...</td>\n",
       "      <td>Hey, as I've described below, I think there ar...</td>\n",
       "      <td>[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...</td>\n",
       "      <td>5</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>Onnx Runtime Errors With LongT5</td>\n",
       "      <td>### System Info\\r\\n\\r\\n- `optimum` version: 1....</td>\n",
       "      <td>[{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMz...</td>\n",
       "      <td>8</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>[Community Event] Doc Tests Sprint</td>\n",
       "      <td>### This issue is part of our **Doc Test Sprin...</td>\n",
       "      <td>[{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOT...</td>\n",
       "      <td>98</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>[Kernel Fusion] training benchmarks of AOTAuto...</td>\n",
       "      <td>Note to maintainers: We are using this PR to c...</td>\n",
       "      <td>[{'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNj...</td>\n",
       "      <td>6</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>FLAX core dump error on CloudTPU when running ...</td>\n",
       "      <td>Hi, I'm having a weird problem trying to train...</td>\n",
       "      <td>[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...</td>\n",
       "      <td>7</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>Key Error: 'pre-processing' during conversion ...</td>\n",
       "      <td>## Environment info\\r\\n\\r\\n- `transformers` ve...</td>\n",
       "      <td>[{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...</td>\n",
       "      <td>3</td>\n",
       "      <td>open</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "27    How to install transformers==4.45, two or thre...   \n",
       "78           latest 44.4.2 doesn't support falcon_mamba   \n",
       "87    when l use the model generate method within @t...   \n",
       "91    When will Transformers version 4.45.0 be relea...   \n",
       "96    when l use the model generate method within @t...   \n",
       "102   Persistent ImportError with TrainingArguments ...   \n",
       "105   Model saving (via `.save_pretrained` or `.push...   \n",
       "109   Generation: deprecate `PreTrainedModel` inheri...   \n",
       "120   Doc bug: wrong token argument name for Tokeniz...   \n",
       "141   Failed to import transformers.models.t5.modeli...   \n",
       "158           [Backbone] Remove out_features everywhere   \n",
       "162   XLMRobertaTokenizer attribute has disappeared ...   \n",
       "272                         transformers - installation   \n",
       "291   confusing deprecation msg for `DynamicCache.se...   \n",
       "310   class Cache must not be a subclass of `torch.n...   \n",
       "329   Docker container with development environment ...   \n",
       "334        Compile compatibilty for decoder-only models   \n",
       "395   FutureWarning: `torch._dynamo.external_utils.i...   \n",
       "436   The cache for model files in Transformers v4.2...   \n",
       "470   get error when running the chatglm3:  'Generat...   \n",
       "473                Cannot build documentation on Mac OS   \n",
       "514                                Using `numpy==2.0.0`   \n",
       "550    NotImplementedError: ggml_type 3 not implemented   \n",
       "580                                   NumPy 2.0 support   \n",
       "593   HFQuantizer implementation for compressed-tens...   \n",
       "616                 Sink cache: fix generate from cache   \n",
       "637   Minimum required accelerate library is not com...   \n",
       "658                          Galore finetuning #stopped   \n",
       "692   sdpa for bert casues nan when using bfloat16 w...   \n",
       "694   RuntimeError: unable to open file when calling...   \n",
       "710   Significant performance degradation with multi...   \n",
       "712   torchrun breaks with load_model_at_end and wit...   \n",
       "741             Image + text + audio uniform processors   \n",
       "757   Error on TPU: Invalid --2a886c8_slice_builder_...   \n",
       "765                             add deepspeed grad ckpt   \n",
       "767                      EncoderDecoderModel with XLM-R   \n",
       "776   SDPA gives nans/infs during sampling on ROCM w...   \n",
       "822          Adherence to semantic versioning with APIs   \n",
       "840   Transformers Agents Collab Notebook - OpenAI R...   \n",
       "910   `get_imports` failing to respect conditionals ...   \n",
       "990   Race condition when loading models from local ...   \n",
       "1089   BERT: TensorFlow Model Garden Conversion scripts   \n",
       "1096        Pipeline feature request for min_new_tokens   \n",
       "1103  Issue Loading 4-bit and 8-bit language models:...   \n",
       "1131  NSP Support for Zero-shot Text Classification ...   \n",
       "1155  Multi-node training with Deepspeed hangs when ...   \n",
       "1225                    Onnx Runtime Errors With LongT5   \n",
       "1263                 [Community Event] Doc Tests Sprint   \n",
       "1286  [Kernel Fusion] training benchmarks of AOTAuto...   \n",
       "1295  FLAX core dump error on CloudTPU when running ...   \n",
       "1359  Key Error: 'pre-processing' during conversion ...   \n",
       "\n",
       "                                                   body  \\\n",
       "27    ### System Info\\n\\ntorch2.2\\n\\n### Who can hel...   \n",
       "78    ### System Info\\n\\npython 3.10\\r\\nrocky linux ...   \n",
       "87    ### System Info\\n\\ntransformers version: 4.43....   \n",
       "91    ### System Info\\n\\nI installed this version 4....   \n",
       "96    ### System Info\\n\\n- `transformers` version: 4...   \n",
       "102   ### System Info\\n\\n- `transformers` version: 4...   \n",
       "105   ### System Info\\n\\n- `transformers` version: 4...   \n",
       "109   # What does this PR do?\\r\\n\\r\\nStep 2 of #3268...   \n",
       "120   ### System Info\\n\\nN/A for doc bug.\\n\\n### Who...   \n",
       "141   ### System Info\\n\\nfrom transformers import T5...   \n",
       "158   # What does this PR do?\\r\\n\\r\\nAs `out_feature...   \n",
       "162   ### System Info\\n\\nOracle Linux 9.4 (Oracle-Li...   \n",
       "272   \\r\\nHello,\\r\\n\\r\\nGoing via the install and I ...   \n",
       "291   The message says `\"The `seen_tokens` attribute...   \n",
       "310   ### System Info\\r\\n\\r\\nI'm using `transformers...   \n",
       "329   ### Feature request\\n\\nA docker container that...   \n",
       "334   # What does this PR do?\\r\\n\\r\\nRecently we mer...   \n",
       "395   ### System Info\\n\\n- `transformers` version: 4...   \n",
       "436   ### System Info\\r\\n\\r\\ngoogle colab with T4 ru...   \n",
       "470   ### System Info\\n\\nplatform == ubuntu 22.04\\r\\...   \n",
       "473   ### System Info\\r\\n\\r\\n- `transformers` versio...   \n",
       "514   `transformers`  now remove the pin of `numpy<2...   \n",
       "550   ### Description \\r\\nWhen trying to use the mod...   \n",
       "580   ### System Info\\n\\nmacOS 14.5\\ntransformers 4....   \n",
       "593   This PR adds an `HFQuantizer` for the [compres...   \n",
       "616   # What does this PR do?\\r\\n\\r\\nFixes #31381. T...   \n",
       "637   ### System Info\\n\\n- `transformers` version: 4...   \n",
       "658   ### System Info\\n\\n- `transformers` version: 4...   \n",
       "692   ### System Info\\r\\n\\r\\n* transformers: transfo...   \n",
       "694   ### System Info\\n\\n```\\r\\n- `transformers` ver...   \n",
       "710   ### System Info\\r\\n\\r\\n```Shell\\r\\n# Env 1\\r\\n...   \n",
       "712   ### System Info\\r\\n\\r\\n- `transformers` versio...   \n",
       "741   # What does this PR do?\\r\\n\\r\\nThis PR is a st...   \n",
       "757   ### System Info\\n\\n```\\r\\ntransformers==4.39.3...   \n",
       "765   hi, @younesbelkada , sorry for the late PR.\\r\\...   \n",
       "767   ### System Info\\n\\nprivate setup:\\r\\n- `transf...   \n",
       "776   ### System Info\\n\\n- `transformers` version: 4...   \n",
       "822   ### Feature request\\r\\n\\r\\nTransformers is an ...   \n",
       "840   ### System Info\\n\\n- `transformers` version: 4...   \n",
       "910   ### System Info\\n\\n- `transformers` version: 4...   \n",
       "990   ### System Info\\n\\n- `transformers` version: 4...   \n",
       "1089  ### Feature request\\r\\n\\r\\nHi,\\r\\n\\r\\nafter wo...   \n",
       "1096  ### Feature request\\n\\nPipeline already suppor...   \n",
       "1103  ### System Info\\n\\n### System Info\\r\\n\\r\\nI'm ...   \n",
       "1131  ### Feature request\\r\\n\\r\\nZero-shot classific...   \n",
       "1155  Hey, as I've described below, I think there ar...   \n",
       "1225  ### System Info\\r\\n\\r\\n- `optimum` version: 1....   \n",
       "1263  ### This issue is part of our **Doc Test Sprin...   \n",
       "1286  Note to maintainers: We are using this PR to c...   \n",
       "1295  Hi, I'm having a weird problem trying to train...   \n",
       "1359  ## Environment info\\r\\n\\r\\n- `transformers` ve...   \n",
       "\n",
       "                                                 labels  comments state  \\\n",
       "27    [{'id': 1843765959, 'node_id': 'MDU6TGFiZWwxOD...         2  open   \n",
       "78    [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         1  open   \n",
       "87    [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         5  open   \n",
       "91    [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         1  open   \n",
       "96    [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         5  open   \n",
       "102   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         2  open   \n",
       "105   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         4  open   \n",
       "109                                                  []         5  open   \n",
       "120   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         4  open   \n",
       "141   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         3  open   \n",
       "158                                                  []         1  open   \n",
       "162   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         5  open   \n",
       "272                                                  []         2  open   \n",
       "291                                                  []         1  open   \n",
       "310   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         5  open   \n",
       "329   [{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNj...         6  open   \n",
       "334                                                  []         7  open   \n",
       "395   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         1  open   \n",
       "436   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         2  open   \n",
       "470   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         2  open   \n",
       "473   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         3  open   \n",
       "514   [{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...         2  open   \n",
       "550   [{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNj...         5  open   \n",
       "580                                                  []        10  open   \n",
       "593                                                  []         4  open   \n",
       "616   [{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...         3  open   \n",
       "637                                                  []         8  open   \n",
       "658   [{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMT...         6  open   \n",
       "692   [{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMz...         6  open   \n",
       "694                                                  []        15  open   \n",
       "710                                                  []        23  open   \n",
       "712   [{'id': 2155169140, 'node_id': 'MDU6TGFiZWwyMT...         3  open   \n",
       "741                                                  []         7  open   \n",
       "757   [{'id': 5160774128, 'node_id': 'LA_kwDOCUB6oc8...         3  open   \n",
       "765   [{'id': 2659267025, 'node_id': 'MDU6TGFiZWwyNj...         7  open   \n",
       "767                                                  []         9  open   \n",
       "776   [{'id': 3817266200, 'node_id': 'MDU6TGFiZWwzOD...         9  open   \n",
       "822   [{'id': 2648621985, 'node_id': 'MDU6TGFiZWwyNj...         2  open   \n",
       "840   [{'id': 3081136536, 'node_id': 'MDU6TGFiZWwzMD...         2  open   \n",
       "910   [{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMz...        21  open   \n",
       "990   [{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...        14  open   \n",
       "1089  [{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...         5  open   \n",
       "1096  [{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...        15  open   \n",
       "1103                                                 []        29  open   \n",
       "1131  [{'id': 1771187924, 'node_id': 'MDU6TGFiZWwxNz...         1  open   \n",
       "1155  [{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...         5  open   \n",
       "1225  [{'id': 2392046359, 'node_id': 'MDU6TGFiZWwyMz...         8  open   \n",
       "1263  [{'id': 1990918270, 'node_id': 'MDU6TGFiZWwxOT...        98  open   \n",
       "1286  [{'id': 2690307185, 'node_id': 'MDU6TGFiZWwyNj...         6  open   \n",
       "1295  [{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...         7  open   \n",
       "1359  [{'id': 2796628563, 'node_id': 'MDU6TGFiZWwyNz...         3  open   \n",
       "\n",
       "      is_version_issue  \n",
       "27                True  \n",
       "78                True  \n",
       "87                True  \n",
       "91                True  \n",
       "96                True  \n",
       "102               True  \n",
       "105               True  \n",
       "109               True  \n",
       "120               True  \n",
       "141               True  \n",
       "158               True  \n",
       "162               True  \n",
       "272               True  \n",
       "291               True  \n",
       "310               True  \n",
       "329               True  \n",
       "334               True  \n",
       "395               True  \n",
       "436               True  \n",
       "470               True  \n",
       "473               True  \n",
       "514               True  \n",
       "550               True  \n",
       "580               True  \n",
       "593               True  \n",
       "616               True  \n",
       "637               True  \n",
       "658               True  \n",
       "692               True  \n",
       "694               True  \n",
       "710               True  \n",
       "712               True  \n",
       "741               True  \n",
       "757               True  \n",
       "765               True  \n",
       "767               True  \n",
       "776               True  \n",
       "822               True  \n",
       "840               True  \n",
       "910               True  \n",
       "990               True  \n",
       "1089              True  \n",
       "1096              True  \n",
       "1103              True  \n",
       "1131              True  \n",
       "1155              True  \n",
       "1225              True  \n",
       "1263              True  \n",
       "1286              True  \n",
       "1295              True  \n",
       "1359              True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['is_version_issue'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"paper_repo_Info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"build_check_results.csv\")\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_url</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/bprabhakar/text-to-image</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://github.com/lorenzobrusco/ECGNeuralNetwork</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>https://github.com/hitvoice/DrQA</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>https://github.com/mdabbah/COOD_benchmarking</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>https://github.com/impredicative/irc-url-title...</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>https://github.com/nitarshan/robust-generaliza...</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>https://github.com/HolyBayes/VarDropPytorch</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              repo_url   status\n",
       "2          https://github.com/bprabhakar/text-to-image  Success\n",
       "38   https://github.com/lorenzobrusco/ECGNeuralNetwork  Success\n",
       "61                    https://github.com/hitvoice/DrQA  Success\n",
       "86        https://github.com/mdabbah/COOD_benchmarking  Success\n",
       "114  https://github.com/impredicative/irc-url-title...  Success\n",
       "145  https://github.com/nitarshan/robust-generaliza...  Success\n",
       "218        https://github.com/HolyBayes/VarDropPytorch  Success"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['status'] == 'Success']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
